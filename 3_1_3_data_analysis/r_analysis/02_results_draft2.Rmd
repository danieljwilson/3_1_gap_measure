---
title: "The BIG measure - Results"
# author: "Daniel J Wilson"
date: "`r Sys.Date()`"
# output: 
#   bookdown::html_document2:
#     toc: true
output:
  word_document: default
  bookdown::word_document2: default
always_allow_html: true

---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(psych)
library(kableExtra)
library(PerformanceAnalytics)
library(viridis)
library(corrr)
library(gridExtra)
library(caret) # or easy machine learning workflow
library(leaps) # for computing best subsets regression

# Import participants from cohort 1 and 2
cohort1 = read.csv('/Users/djw/Documents/pCloud_synced/Academics/Projects/2020_thesis/thesis_experiments/3_experiments/3_3_experience_sampling/3_3_1_raw_data/run_1/run1_subjects.csv')
cohort2 = read.csv('/Users/djw/Documents/pCloud_synced/Academics/Projects/2020_thesis/thesis_experiments/3_experiments/3_3_experience_sampling/3_3_1_raw_data/run_2/run2_subjects.csv')

# Import preprocessed dataframe
df = readRDS('/Users/djw/Documents/pCloud_synced/Academics/Projects/2020_thesis/thesis_experiments/3_experiments/3_1_gap_measure/3_1_3_data_analysis/r_analysis/data/df.RDS')
```

# Demographics

```{r demographics, echo=FALSE}
c1onB = length(df[df$cohort =='cohort 1' & df$onOff == 'onBoarding',]$age)
c1offB = length(df[df$cohort =='cohort 1' & df$onOff == 'offBoarding',]$age)

c2onB = length(df[df$cohort =='cohort 2' & df$onOff == 'onBoarding',]$age)
c2offB = length(df[df$cohort =='cohort 2' & df$onOff == 'offBoarding',]$age)

onB_num = length(df[df$onOff == 'onBoarding',]$age)

onB_m = table(df[df$onOff == 'onBoarding',]$gender)[1]
onB_f = table(df[df$onOff == 'onBoarding',]$gender)[2]
onB_o = table(df[df$onOff == 'onBoarding',]$gender)[3]

age_m = round(mean(as.numeric(df$age), na.rm = T), 2)
age_sd = round(sd(as.numeric(df$age), na.rm = T), 2)

# ethnicity
white = round(100 * table(df[df$onOff == 'onBoarding',]$ethnicity)[1]/onB_num, 2)
black = round(100 * table(df[df$onOff == 'onBoarding',]$ethnicity)[2]/onB_num, 2)
native = round(100 * table(df[df$onOff == 'onBoarding',]$ethnicity)[3]/onB_num, 2)
asian = round(100 * table(df[df$onOff == 'onBoarding',]$ethnicity)[4]/onB_num, 2)
other = round(100 * table(df[df$onOff == 'onBoarding',]$ethnicity)[5]/onB_num, 2)
```

We ran two separate cohorts through our study during the fall semester of 2022 and the winter semester of 2023. 

For Cohort 1 we had `r c1onB` subjects complete the Intention-Behavior gap measure at onboarding and `r c1offB` at offboarding. 

For Cohort 2 we had `r c2onB` subjects complete the Intention-Behavior gap measure at onboarding and `r c2offB` again at offboarding.

In total we had `r onB_num` subjects complete the Intention-Behavior gap measure at onboarding (`r onB_m` males, `r onB_f` females, `r onB_o` other) with a mean age of `r age_m` (SD = `r age_sd`). 

The majority of particpants were Asian (`r asian`%), followed by Other (`r other`%), Black or African American (`r black`%), White (`r white`%) and Indigenous or Native (`r native`%).

```{r description, echo=FALSE}
# # How many goals do people have?
# x = data.frame(table(df[df$onOff=='onBoarding',]$gender))
# names(x) = c('Gender', 'Count')
# x$Gender = c("Male", "Female", "Other")
# 
# ggplot(x, aes(x = Gender, y = Count, fill= Gender)) +
#   geom_bar(stat = "identity") +
#   scale_fill_viridis_d() +
#   labs(title="Gender")+
#   geom_text(aes(label = Count), vjust = -0.5) +
#   theme_void()
```

# Gap Characteristics
A simple domain-general gap measure was calculated by averaging the magnitude of the gap for all goal domains each subject selected. Later, we created a "weighted" version of the gap, leveraging the fact that we collect additional data from our subjects about each goal domain (e.g. domain importance, required effort, time, etc.). This is explained in detail in section 4.4 (Informing the Intention-Behavior Gap).

## Gap distribution in population
Ignoring this additional information for the moment, we can look at the "unweighted" measure of our gap and observe that the distribution of averaged gap magnitudes looks close to normal (Figure \@ref(fig:gap-distribution)), with a mean gap across subjects of 45.9% (SD = 15.8). We can also see that a single very productive subject indicated that they had a gap of zero. 

```{r gap-distribution, fig.cap="Distribution of Intention-Behavior gap magnitudes in sample.", echo=FALSE, warning=FALSE}

# Plot
ggplot(data.frame(x=df$domain_gap), aes(x='Subjects', y=x)) +
  geom_violin(fill="lightblue", color='black', alpha = 0.5, show.legend = FALSE) +
  geom_boxplot(width = 0.1, fill = "white", alpha = 0.9, outlier.shape = NA) +
  geom_jitter(width=0.2, show.legend = FALSE) + 
  labs(y="Gap (%)", x = '') +
  theme(legend.position = "none") +
  theme_minimal()

# describe(df$domain_gap)
```

## Goal Domain Count

Given that one unique aspect of our measure is that subjects can effectively choose the number of items (i.e. goal domains) based on self-relevance (up to a maximum of 34), we necessarily end up with variation in the total number of items/goals for each subject. Across subjects we can again see that this number approximates a normal distribution (Figure \@ref(fig:goal-domains-count)), with a mean of 18.3 goal domains selected (SD = 7.5).

```{r goal-domains-count, fig.cap="Number of goal domains for each subject.", warning=FALSE, message=FALSE, echo=FALSE}

x = dplyr::select(df, contains('ib_domain_success'))

# Calculate the number of non-NA domain goal values for the domains
non_na_counts <- data.frame(counts = rowSums(!is.na(x)))

non_na_counts = data.frame(counts = non_na_counts[non_na_counts>0,])

ggplot(data.frame(x=non_na_counts$counts), aes(x='Subjects', y=x)) +
  geom_violin(fill="lightblue", color='black', alpha = 0.5, show.legend = FALSE) +
  geom_boxplot(width = 0.1, fill = "white", alpha = 0.9, outlier.shape = NA) +
  geom_jitter(width=0.2, show.legend = FALSE) + 
  labs(y="Goal Domains (total)", x = '') +
  theme(legend.position = "none") +
  ylim(c(0,35)) +
  theme_minimal()
  
# describe(non_na_counts$counts)
```

## Goal Domain Characteristics

### Frequency

To provide an overview of some of the characteristics of our different goal domains we can look at domain selection frequency, importance, and motivation source (to what degree a goals is internally vs. externally motivated) for all of the domains. 

The frequency with which domains were selected as "goal" domains varied greatly across goal categories, with work/school being chosen most commonly, 89% of the time, and video games least, just 20% (detail in Figure \@ref(fig:goal-domains-frequency)).

```{r goal-domains-frequency, fig.cap="Goal domain frequency of selection.", warning=FALSE, message=FALSE, echo=FALSE}

x = dplyr::select(df, contains('ib_domain_success'))
# remove rows with ONLY NA values
x <- x[apply(x, 1, function(row) any(!is.na(row))), ]

n = nrow(x)

# Count the non NA values in each column
non_na_counts <- colSums(!is.na(x))
dom_count <- data.frame(Domain = names(non_na_counts), Frequency = (non_na_counts/n)*100)
# Remove "ib_domain_success_" from the "Name" column
dom_count$Domain <- gsub("ib_domain_success_", "", dom_count$Domain)
# sort
dom_count = dom_count[order(dom_count$Frequency),]
dom_count$Domain <- factor(dom_count$Domain, levels = dom_count$Domain)

ggplot(dom_count, aes(x = Domain, y = Frequency, fill = Frequency)) +
  geom_bar(stat = "identity", show.legend = F) +
  coord_flip() +
  scale_fill_gradient(limits = c(0, 100)) +
  xlab("Domain") +
  ylab("Frequency (%)") +
  scale_fill_viridis() +
  theme_minimal()
```

### Magnitude

Given that we had so many different goal domains (34) we were curious how the magnitude of the gap might differ by domain. As shown in Figure \@ref(fig:goal-domains-gap) we can see that there is good deal of heterogeneity in the gaps (where 100% would indicate not accomplishing a goal at all, and 0% would be complete accomplishment). The largest gap (volunteering, 59%) was in fact just over twice the magnitude of the smallest gap (alcohol and drug use, 28%).

```{r goal-domains-gap, fig.cap="Gap magnitude by goal domain.", warning=FALSE, message=FALSE, echo=FALSE}

x = dplyr::select(df, contains('ib_domain_success'))

x =x[apply(x, 1, function(x) !all(is.na(x))), ]

# Calculate the mean of columns omitting NA values
means <- colMeans(x, na.rm = TRUE)
# Calculate gap instead of success
means = 100 - means
means <- data.frame(Domain = names(means), Gap = means)
# Remove "ib_domain_success_" from the "Name" column
means$Domain <- gsub("ib_domain_success_", "", means$Domain)
# sort
gap_means = means[order(means$Gap),]
gap_means$Domain <- factor(gap_means$Domain, levels = gap_means$Domain)

ggplot(gap_means, aes(x = Domain, y = Gap, fill = Gap)) +
  geom_bar(stat = "identity", show.legend = F) +
  coord_flip() +
  scale_fill_gradient(limits = c(0, 100)) +
  xlab("Domain") +
  ylab("Gap (%)") +
  scale_fill_viridis() +
  theme_minimal()
```


### Additional Domain Information

As mentioned, we collected data on a number of dimensions that we thought could provide important additional information about each goal domain, including importance, time (i.e. how much time is required to accomplish the goal takes), and effort. We plot those distributions below (Figure \@ref(fig:intention-importance-distribution)). These specific dimensions were later used to create a "weighted" Intention-Behavior gap (See the section [Informing the Intention-Behavior Gap]).

```{r intention-importance-distribution, fig.cap= "Distributions of importance, time, and effort evaluations for subject goal domains as selected using a 7-point Likert scale.",echo=FALSE, warning=FALSE, message=FALSE}
data = df %>% 
  select(starts_with('ib_domain_import_')) %>%
  mutate(across(where(is.character), as.numeric))

# Convert the dataframe into a single vector
data <- unlist(data)

# Remove NA values
data <- na.omit(data)


# Convert vector to data frame and count occurrences of each value
data <- data.frame(value = data) %>%
  count(value)

# Create the bar plot
p1 = ggplot(data, aes(x = value, y = n)) +
  geom_bar(stat = "identity", fill = 'lightblue', alpha = 0.7) +
  labs(x = "Value", y = "Count", title = "Importance") +
  theme_minimal()

data = df %>%
  select(starts_with('ib_domain_time_')) %>%
  mutate(across(where(is.character), as.numeric))

# Convert the dataframe into a single vector
data <- unlist(data)

# Remove NA values
data <- na.omit(data)


# Convert vector to data frame and count occurrences of each value
data <- data.frame(value = data) %>%
  count(value)

# Create the bar plot
p2 = ggplot(data, aes(x = value, y = n)) +
  geom_bar(stat = "identity", fill = 'lightblue', alpha = 0.7) +
  labs(x = "Value", y = "Count", title = "Time") +
  theme_minimal()

data = df %>% 
  select(starts_with('ib_domain_effort_')) %>%
  mutate(across(where(is.character), as.numeric))

# Convert the dataframe into a single vector
data <- unlist(data)

# Remove NA values
data <- na.omit(data)


# Convert vector to data frame and count occurrences of each value
data <- data.frame(value = data) %>%
  count(value)

# Create the bar plot
p3 = ggplot(data, aes(x = value, y = n)) +
  geom_bar(stat = "identity", fill = 'lightblue', alpha = 0.7) +
  labs(x = "Value", y = "Count", title = "Effort") +
  theme_minimal()
  
grid.arrange(p1, p2, p3, ncol=3)
```


#### Goal Importance
When considering importance scores we can see that domain means only fall in the upper half of the possible range (Figure \@ref(fig:goal-domains-importance)), with the domain receiving the lowest score, video games, still received an average score slightly higher than the midpoint of the range (M = 3.55, SD = 2.06). This compression of the range we believe is likely due to the fact that subjects self-selected those domains for which they felt like they currently held a goal. The effect of this is that any domain with an importance of or close to zero would be eliminated. The domain rated most important (M = 6.26, SD = 0.98) was also the domain where subjects most commonly had a goal: Work/School.

```{r goal-domains-importance, fig.cap="Average goal importance by domain, measured using a 7-point Likert scale.", warning=FALSE, message=FALSE, echo=FALSE}

x = dplyr::select(df, contains('ib_domain_import'))
x <- x %>% mutate_all(as.numeric)

# Calculate the mean of columns omitting NA values
means <- colMeans(x, na.rm = TRUE)
sds <- sapply(x, function(column) sd(column, na.rm = TRUE))

means <- data.frame(Domain = names(means), Importance = means, SD = sds)
# Remove "ib_domain_success_" from the "Name" column
means$Domain <- gsub("ib_domain_import_", "", means$Domain)
# sort
import_means = means[order(means$Importance),]
import_means$Domain <- factor(import_means$Domain, levels = import_means$Domain)

ggplot(import_means, aes(x = Domain, y = Importance, fill = Importance)) +
  geom_bar(stat = "identity", show.legend = F) +
  coord_flip() +
  scale_fill_gradient(limits = c(1, 7)) +
  xlab("Domain") +
  ylab("Importance") +
  scale_fill_viridis() +
  theme_minimal()
```


#### Locus of Motivation
For each goal domain a subject selected as self-relevant we then asked them to indicate to what degree their goal in the chosen domain was internally and externally motivated. We can look at the difference in these scores (internal motivation - external motivation) as a measure of how much more internally motivated each domain is. We find that all domains, in fact, are more internally motivated than externally motivated, save for administrative work. We also note the interesting finding that while the Learning goal is highly internally motivated, the work/school goal is the second least internally motivated domain. For our sample we know that all subjects are students, but we are also aware that some subjects also work an additional job. This means that even though this category is mostly representing school, there is some contamination by people who are also working an outside job.

```{r goal-domains-motivation1, warning=FALSE, message=FALSE, echo=FALSE}

int = df %>% 
  dplyr::select(starts_with('ib_domain_internal')) %>%
  mutate(across(where(is.character), as.numeric))

int =int[apply(int, 1, function(x) !all(is.na(x))), ]

ext = df %>% 
  dplyr::select(starts_with('ib_domain_external')) %>%
  mutate(across(where(is.character), as.numeric))

ext =ext[apply(ext, 1, function(x) !all(is.na(x))), ]


# Calculate the mean of columns omitting NA values
means_int <- colMeans(int, na.rm = TRUE)
means_ext <- colMeans(ext, na.rm = TRUE)
# Calculate delta
motivation_int_delta = means_int - means_ext

motivation_df = data.frame(Domain = names(means_ext), internal = means_int, external = means_ext)
# Remove "ib_domain_external_" from the "Name" column
motivation_df$Domain <- gsub("ib_domain_external_", "", motivation_df$Domain)

ggplot(motivation_df, aes(y = reorder(Domain, internal))) +
  geom_segment(aes(x = external, xend = internal, yend = Domain)) +
  geom_point(aes(x = internal, color = "Internal"), size = 3) +
  geom_point(aes(x = external, color = "External"), size = 3) +
  scale_color_manual(name = "Motivation Source",
                     values = c("Internal" = "red2", "External" = "blue4")) +
  labs(y = "Domain", title = 'A') +
  theme_minimal() +
  theme(legend.position = "bottom", axis.title.x = element_blank())

# 
# 
# ggplot(motivation_df_long, aes(x = value, y = Domain)) +
#   geom_line() +
#   geom_point(aes(color = variable), size = 3) +
#   theme_minimal()
# 
# motivation_int_delta_df <- data.frame(Domain = names(means_ext), motivation_int_delta = motivation_int_delta)
# # Remove "ib_domain_internal_" from the "Name" column
# motivation_int_delta_df$Domain <- gsub("ib_domain_internal_", "", motivation_int_delta_df$Domain)
# # Remove "ib_domain_external_" from the "Name" column
# motivation_int_delta_df$Domain <- gsub("ib_domain_external_", "", motivation_int_delta_df$Domain)
# 
# # sort
# motivation_int_delta_df = motivation_int_delta_df[order(motivation_int_delta_df$motivation_int_delta),]
# motivation_int_delta_df$Domain <- factor(motivation_int_delta_df$Domain, levels = motivation_int_delta_df$Domain)
# 
# ggplot(motivation_int_delta_df, aes(x = Domain, y = motivation_int_delta, fill = motivation_int_delta)) +
#   geom_bar(stat = "identity", show.legend = F) +
#   coord_flip() +
#   xlab("Domain") +
#   ylab("Internal - External Motivation") +
#   scale_fill_viridis() +
#   theme_minimal()
```
```{r goal-domains-motivation2, fig.cap="Internal and external motivation levels for goal Domains. (A) Ordered by internal motivation. (B) Ordered by external motivation.", warning=FALSE, message=FALSE, echo=FALSE}

ggplot(motivation_df, aes(y = reorder(Domain, external))) +
  geom_segment(aes(x = external, xend = internal, yend = Domain)) +
  geom_point(aes(x = internal, color = "Internal"), size = 3) +
  geom_point(aes(x = external, color = "External"), size = 3) +
  scale_color_manual(name = "Motivation Source",
                     values = c("Internal" = "red2", "External" = "blue4")) +
  labs(x = "Motivation Level", y = "Domain", title = 'B') +
  theme_minimal() +
  theme(legend.position = "none")

```

### Relationships between gap, Domain frequency and Domain dimensions

We can also look at the relationships between the different goal domain attributes and Intention-Behavior gap magnitude. 

```{r domain-motivation-import-freq-gap-cor, fig.cap= "Correlations between goal domain attributes. Note: \\* p<0.05; \\** p<0.01; \\*** p<0.001",  warning=FALSE, message=FALSE, echo=FALSE}
library(GGally)

# domain time
time = df %>% 
  dplyr::select(starts_with('ib_domain_time')) %>%
  mutate(across(where(is.character), as.numeric))

time =time[apply(time, 1, function(x) !all(is.na(x))), ]

# domain effort
effort = df %>% 
  dplyr::select(starts_with('ib_domain_effort')) %>%
  mutate(across(where(is.character), as.numeric))

effort =effort[apply(effort, 1, function(x) !all(is.na(x))), ]

# Calculate the mean of columns omitting NA values
time <- colMeans(time, na.rm = TRUE)
effort <- colMeans(effort, na.rm = TRUE)

effort <- data.frame(Domain = names(time), effort = effort)
time <- data.frame(Domain = names(time), time = time)

# Remove "ib_domain_internal_" from the "Name" column
time$Domain <- gsub("ib_domain_time_", "", time$Domain)
# Remove "ib_domain_external_" from the "Name" column
effort$Domain <- gsub("ib_domain_time_", "", effort$Domain)


# List of dataframes to be merged
dfs <- list(dom_count, motivation_df, import_means[,c('Domain', 'Importance')], effort, time, gap_means)

# Use Reduce to merge the dataframes
merged_df <- Reduce(function(x, y) merge(x, y, by="Domain", all=TRUE), dfs)

chart.Correlation(merged_df[2:8], histogram=TRUE, pch=19)

# Cor numbers
cor_int = cor.test(merged_df$Gap, merged_df$internal)
r_int = round(cor_int$estimate, 2)
d_int = cor_int$parameter
p_int = round(cor_int$p.value, 3)

cor_imp = cor.test(merged_df$Gap, merged_df$Importance)
r_imp = round(cor_imp$estimate, 2)
d_imp = cor_imp$parameter
p_imp = round(cor_imp$p.value, 3)

cor_ext = cor.test(merged_df$Gap, merged_df$external)
r_ext = round(cor_ext$estimate, 2)
d_ext = cor_ext$parameter
p_ext = round(cor_ext$p.value, 3)

cor_int_time = cor.test(merged_df$time, merged_df$internal)
r_int_time = round(cor_int_time$estimate, 2)
d_int_time = cor_int_time$parameter
p_int_time = round(cor_int_time$p.value, 3)

cor_ext_time = cor.test(merged_df$time, merged_df$external)
r_ext_time = round(cor_ext_time$estimate, 2)
d_ext_time = cor_ext_time$parameter
p_ext_time = round(cor_ext_time$p.value, 3)

cor_int_effort = cor.test(merged_df$effort, merged_df$internal)
r_int_effort = round(cor_int_effort$estimate, 2)
d_int_effort = cor_int_effort$parameter
p_int_effort = round(cor_int_effort$p.value, 3)

cor_ext_effort = cor.test(merged_df$effort, merged_df$external)
r_ext_effort = round(cor_ext_effort$estimate, 2)
d_ext_effort = cor_ext_effort$parameter
p_ext_effort = round(cor_ext_effort$p.value, 3)

```

We observe that the variables with the strongest associations (all negative) with gap magnitude are internal motivation (r(`r d_int`) = `r r_int`, p = `r p_int`), goal importance (r(`r d_imp`) = `r r_imp`, p = `r p_imp`), and external motivation (r(`r d_ext`) = `r r_ext`, p = `r p_ext`). It is also interesting to note in passing that while more highly internally motivated goals tend to take more time (r(`r d_int_time`) = `r r_int_time`, p = `r p_int_time`) than externally motivated goals (r(`r d_ext_time`) = `r r_ext_time`, p = `r p_ext_time`), they are less correlated with effort (r(`r d_int_effort`) = `r r_int_effort`, p = `r p_int_effort`) than externally motivated goal levels (r(`r d_ext_effort`) = `r r_ext_effort`, p = `r p_ext_effort`).

## Gaps by Gender

```{r goal gender gap, warning=FALSE, message=FALSE, echo=FALSE}

x = dplyr::select(df[df$onOff == 'onBoarding',], contains('ib_domain_success'), gender)
x =x[apply(x, 1, function(x) !all(is.na(x))),]


# Calculate the mean of columns omitting NA values
means <- rowMeans(x[, !(names(x) %in% c("gender", "onOff"))], na.rm = TRUE)

# Calculate gap instead of success
means = 100 - means
means <- data.frame(Gap = means, Gender = x$gender)

# run t test
males = means[means$Gender==1,]$Gap
females = means[means$Gender==2,]$Gap
result = t.test(females, males)

# calculate means + sd
mean_f = mean(females, na.rm = T)
mean_m = mean(males, na.rm = T)
sd_f = round(sd(females, na.rm = T),2)
sd_m = round(sd(males, na.rm = T),2)
```
We found a significant difference in the magnitude of the reported Intention-Behavior gap for females (M = `r round(mean_f, 2)`%, SD = `r sd_f`) and males (M = `r round(mean_m, 1)`%, SD = `r sd_m`). A Welch two-samples t-test showed that the difference was statistically significant, t(`r round(result$parameter, 2)`) = `r round(result$statistic, 2)`, p = `r round(result$p.value, 3)`).

```{r plot-goal-gender-gap, warning=FALSE, message=FALSE, echo=FALSE}
# create dataframe
x <- data.frame(
  Value = c(females, males),
  Group = factor(rep(1:2, c(length(females), length(males))), labels=c("Females", "Males"))
)

# plot
ggplot(x, aes(x=Group, y=Value, fill=Group)) +
  geom_violin(alpha = 0.5, show.legend = FALSE) +
  geom_boxplot(width = 0.1, fill = "white", alpha = 0.9, outlier.shape = NA) +
  geom_jitter(width=0.2, aes(color=Group), show.legend = FALSE) + 
  labs(title="Gender gap difference", y="Gap") +
  geom_label(aes(x = 1.5, y = 80, label = paste("t =", round(result$statistic, 2), "\np-value =", round(result$p.value, 3))), color = "black", fill = "white") +
  theme(legend.position = "none") +
  theme_minimal()
```

# Reliability
Reliability in the context of a psychological measure refers to the consistency, stability, and repeatability of the scores produced by the measure. When an instrument is reliable, it yields consistent results under consistent conditions. This doesn't necessarily mean the instrument is measuring what it's supposed to measure (that's validity), but that it measures consistently (Nunnally, J. C., 1976).


## Cronbachs Alpha
Cronbach's Alpha ($\alpha$) is a statistic commonly used to measure the internal consistency of a scale or test, often in the fields of psychology, education, and related disciplines. In essence, it assesses how well a set of items (questions, tasks, etc.) measures a single construct. The value of $\alpha$ ranges between 0 and 1, with higher values indicating greater internal consistency (Tavakol, M. & Dennick, R., 2011). 

Mathematically, Cronbach's Alpha is given by:

\begin{equation} 
  \alpha = \frac{k}{k-1} \left(1 - \frac{\sum_{i=1}^{k} \sigma^2_{Yi}}{\sigma^2_X} \right)
  (\#eq:cronbachs-alpha)
\end{equation} 

Where $k$ represents the number of items, $\sigma^2_{Yi}$ is the variance of item $i$, and $\sigma^2_X$ is the variance of the observed total scores.

```{r chronbachs alpha, echo=FALSE, warning=FALSE, message=FALSE}
# note that this gives numerous warning messages...
chron = dplyr::select(df, contains('ib_domain_success'))

# remove rows that are all NA
chron <- chron[rowSums(is.na(chron)) != ncol(chron), ]

c_alpha = psych::alpha(chron, na.rm = T)

missingness = sum(colSums(is.na(chron))) / (dim(chron)[1] * dim(chron)[2])
```

We calculated Cronbach's alpha using a dataframe that included many cells with `NA` values, given that participants generally did not have goals for all domains. In total missing values were present in `r round(missingness, 3)*100`% of the dataframe's cells. Even with a sparse matrix the `psych` library (v2.3.6, Revelle) can still calculate a Cronbach's alpha metric by removing missing values and calculating pairwise correlations. Using this library internal consistency as measured by Cronbach's $\alpha$ (standardized) was `r round(c_alpha$total$std.alpha, 2)`.

While this supports the idea that we have an internally consistent uni-dimensional measure, we should also be aware that the value is inflated due to the high number of items (in our case 34) in the measure (Taber, K. S., 2018). One interpretation of a very high alpha value is that the items might be redundant. In our case, given the nature of our items, this is not a major concern as we have specifically chosen distinct domains. For example, exercise and diet may be closely related for many people, but they are not the same activity.

## Inter-Item Correlation

Inter-item correlation refers to the pairwise correlations between items on a measure as a means of evaluating the consistency of the measure. If items on a scale are supposed to tap into the same underlying construct, they should correlate positively with each other (Nunnally, J. C., & Bernstein, I. H., 1994).

This metric is calculated by computing a correlation matrix for all items and then averaging the correlations of the matrix (excluding the diagonal which will have a perfect correlation of 1 with itself). In other words, given a correlation matrix $C$ where $c_{ij}$ represents the correlation between item $i$ and item $j$, and $n$ is the number of items:

\begin{equation} 
  \text{Average Inter-item Correlation} = \frac{\sum_{i=1}^{n}\sum_{j=1, j\neq i}^{n} c_{ij}}{n(n-1)}
  (\#eq:inter-item-correlation)
\end{equation} 

```{r inter-item-correlation, fig.cap="Average pairwise correlation of each goal domain with all other goal domains.", echo=FALSE, warning=FALSE, message=FALSE}

chron = dplyr::select(df, contains('ib_domain_success'))

# remove rows with all na
chron <- chron[rowSums(is.na(chron)) != ncol(chron), ]

# create cor matrix
cor_matrix = cor(chron, use = 'pairwise.complete.obs')

# Set diagonal values to NA
diag(cor_matrix) <- NA

# Find the row and column index of the maximum value
max_location <- which(cor_matrix == max(cor_matrix, na.rm = TRUE), arr.ind = TRUE)
row_name <- rownames(cor_matrix)[max_location[1, "row"]]
col_name <- colnames(cor_matrix)[max_location[1, "col"]]
max_val = round(max(cor_matrix, na.rm = TRUE), 2)
# calculate pairwise correlation (note that this is hard coded - make sure it matches the row and col above)
cor_result <- cor.test(df$ib_domain_success_Alcohol_drug, df$`ib_domain_success_Video games`, use="pairwise.complete.obs")

# # Negative Correlations
# cor.test(df$`ib_domain_success_Video games`, df$ib_domain_success_Partner, use = 'pairwise.complete.obs')
# cor.test(df$`ib_domain_success_Video games`, df$`ib_domain_success_Community involvement`, use = 'pairwise.complete.obs')

# Calculate avg. correlation of each domain with the other domains
inter_item = cor_matrix %>% colMeans(na.rm = TRUE)
mean_inter_item = round(mean(inter_item), 2)

# Plot
inter_item <- data.frame(Domain = names(inter_item), Correlation = inter_item)
# Remove "ib_domain_success_" from the "Name" column
inter_item$Domain <- gsub("ib_domain_success_", "", inter_item$Domain)
# sort
inter_item = inter_item[order(inter_item$Correlation),]
inter_item$Domain <- factor(inter_item$Domain, levels = inter_item$Domain)

ggplot(inter_item, aes(x = Domain, y = Correlation, fill = Correlation)) +
  geom_bar(stat = "identity") +
  coord_flip() +
  xlab("Category") +
  ylab("Value") +
  scale_fill_viridis() +
  theme_minimal() +
  theme(legend.position = "none")

```

The mean inter-item correlation for our measure is `r mean_inter_item`, which is within the ideal range of .20 to .40, indicating that items are neither so different from one another that they are unlikely to be touching on the same measurement domain, nor so homogeneous that they are redundant (Piedmont, 2014).

The highest single bivariate correlation is between `r row_name` and `r col_name` of r(`r cor_result$parameter`) = `r round(cor_result$estimate, 2)`, p < 0.001. This underscores why for this measure a strong correlation does not suggest that items are equivalent.

There are also two negative correlations between items - both involving video gaming intentions. The first is with intentions toward the subject's relationship with a partner ($r(23) = -0.2, p = .34$), and the second with community involvement goals ($r(30) = -.02, p = .93$), however neither was significant.


## Item-Total Correlation
Item-total correlation, often referred to as the corrected item-total correlation, represents the correlation between a particular item and the sum of all the other items in a scale or test. The "corrected" aspect means that the total doesn't include the item itself. It is commonly used in psychometric analyses to gauge how well an item aligns with the overall scale or test.

We calculated corrected item-total correlations by testing how the score of an individual item $X_i$ correlates with the total score of all items, excluding the score of item $X_i$. It provides a measure of how much the item relates to the rest of the test when the influence of the item itself is removed from the total score. The full formula is:

\begin{equation} 
  r_{i} = \frac{\sum (X_i - \bar{X_i})(T' - \bar{T'})}{\sqrt{\sum (X_i - \bar{X_i})^2 \sum (T' - \bar{T'})^2}}
  (\#eq:item-total-correlation)
\end{equation} 

Where $\bar{X_i}$ is the mean score of item $X_i$ and $\bar{T'}$ is the mean of the corrected scores $T'$ ($T' = T - X_i$). 

```{r item-total-correlation, fig.cap="Corrected item-total correlation of each domain with sum all other domains, excluding itself.", echo=FALSE, warning=FALSE, message=FALSE}

chron = dplyr::select(df, contains('ib_domain_success'))

# remove rows with all na
chron <- chron[rowSums(is.na(chron)) != ncol(chron), ]

item_total_cors = rep(NaN, dim(chron)[2])

# loop through all items
for (i in 1:ncol(chron)){
  # calculate total score
  totals = rowSums(chron[, -i], na.rm = T)
  item_total_cors[i] = abs(cor(chron[,i], totals, use = 'pairwise.complete.obs')[1])
}

item_total_cor = round(mean(item_total_cors), 2)

# Plot
item_total <- data.frame(Domain = colnames(chron), Correlation = item_total_cors)
# Remove "ib_domain_success_" from the "Name" column
item_total$Domain <- gsub("ib_domain_success_", "", item_total$Domain)
# sort
item_total = item_total[order(item_total$Correlation),]
item_total$Domain <- factor(item_total$Domain, levels = item_total$Domain)

ggplot(item_total, aes(x = Domain, y = Correlation, fill = Correlation)) +
  geom_bar(stat = "identity") +
  coord_flip() +
  xlab("Category") +
  ylab("Value") +
  scale_fill_viridis() +
  theme_minimal() +
  theme(legend.position = "none")

```

The average corrected item-total correlation was `r item_total_cor`. This represents 13 items that had item-total correlations ≥.4, which indicates excellent discrimination, 15 items that were between .3 and .4, which indicates good discrimination, four items that were between .2 and .3 which indicates marginal discrimination (though all were above .29) and two items ≤0.19 which means poor discrimination (Qin, 2006, Streiner & Norman, 2008).


## Test-Retest Reliability
Test-retest reliability refers to the extent to which scores on a particular measure are stable over a specified period. In other words, if the same participants complete the same measure on two different occasions (with no intervention or change occurring between the two test times), their scores should be similar if the measure is reliable. This is particularly true for constructs that are expected to be stable over time, such as intelligence or personality. However, when measuring constructs that might be expected to change over fairly short time periods (e.g. mood), the test-retest method can be less appropriate as a measure of reliability (Nunnally, J. C., & Bernstein, I. H., 1994, Streiner, D. L., & Norman, G. R., 2008).

We use the Intraclass Correlation Coefficient (ICC) to quantify test-retest reliability. This has been found to be superior than simply calculating the Pearson correlation between scores at two different times as it assesses not just the correlation, but also the agreement between measurements. There are a number of versions of the ICC, we use ICC(3,1) which is used for single measures and based on a two-way mixed-effects model. It is calculated by:

\begin{equation} 
  ICC(3,1) = \frac{\sigma^2_{\text{subject}}}{\sigma^2_{\text{subject}} + \sigma^2_{\text{error}}}
  (\#eq:ICC-31)
\end{equation} 

Where $\sigma^2_{\text{subject}}$ is the between-subjects variance, and $\sigma^2_{\text{error}}$ is the within-subjects variance (McGraw, K. O., & Wong, S. P., 1996).

### Test-Retest Evaluation
```{r test-retest, echo=FALSE, message=FALSE}

# COHORT 1
# create df with time 1 and time 2 columns
test1 = df[df$onOff=='onBoarding' & df$cohort == 'cohort 1',]
test1 = test1[c('domain_gap', 'consent_sonaID', 'EndDate')]
names(test1) = c('gap_t1', 'StudentID', 'date_t1')
test2 = df[df$onOff=='offBoarding' & df$cohort == 'cohort 1',]
test2 = test2[c('domain_gap', 'consent_sonaID', 'EndDate')]
names(test2) = c('gap_t2', 'StudentID', 'date_t2')

# merge cohort 1
test_retest = merge(test1, test2, by = 'StudentID')

# add participant ID
cohort1$StudentID = as.character(cohort1$Student.Number)
test_retest = merge(test_retest, cohort1[,c('ParticipantIdentifier', 'StudentID')], by = 'StudentID')

# remove rows missing values
test_retest1 <- test_retest[complete.cases(test_retest), ]

# COHORT 2
# create df with time 1 and time 2 columns
test1 = df[df$onOff=='onBoarding' & df$cohort == 'cohort 2',]
test1 = test1[c('domain_gap', 'consent_sonaID', 'EndDate')]
names(test1) = c('gap_t1', 'StudentID', 'date_t1')
test2 = df[df$onOff=='offBoarding' & df$cohort == 'cohort 2',]
test2 = test2[c('domain_gap', 'consent_sonaID', 'EndDate')]
names(test2) = c('gap_t2', 'StudentID', 'date_t2')

# merge cohort 2
test_retest = merge(test1, test2, by = 'StudentID')

# add participant ID
cohort2$StudentID = as.character(cohort2$Student.Number)
test_retest = merge(test_retest, cohort2[,c('ParticipantIdentifier', 'StudentID')], by = 'StudentID')

# remove rows missing values
test_retest2 <- test_retest[complete.cases(test_retest), ]

# combine
test_retest_all_gap = bind_rows(test_retest1, test_retest2)
test_retest_ibgap = test_retest_all_gap

# calculate ICC
ICC_gap = ICC(test_retest_all_gap[,c('gap_t1', 'gap_t2')], missing=TRUE, alpha = .05, lmer = TRUE, check.keys = FALSE)

lower_95 = round(ICC_gap$results$`lower bound`[1], 2)
upper_95 = round(ICC_gap$results$`upper bound`[1], 2)

# calculate time gaps
week_dif = (as.Date(test_retest_all_gap$date_t2) - as.Date(test_retest_all_gap$date_t1))/7
week_dif_mean = describe(as.numeric(week_dif))$mean
week_dif_sd = describe(as.numeric(week_dif))$sd
week_dif_n = describe(as.numeric(week_dif))$n
```

We had `r week_dif_n` subjects complete the measure twice - once near the beginning of semester (t1), and a second time (t2) about 3 months later at the end of the semester (weeks = `r round(week_dif_mean, 2)`, SD = `r round(week_dif_sd, 2)`).

The ICC for the two measurement timepoints was found to be ICC(3,1) = `r round(ICC_gap$results$ICC[1], 2)`, 95% CI [`r lower_95`, `r upper_95`], indicating moderate reliability (Koo and Li 2016). For reference the Short Grit Scale had a 1 year test-retest stability of $r = .68$ and conscientiousness scores based on the NEO Five-Factor Inventory (Costa & McCrae, 1992) correlated at $r = .59$ over 4 years in a test by Robins, Fraley, Roberts and Trzesniewski (2001). Notably, within our own data we found that most comparison trait measures had a higher ICC value than our gap measure at t1 and t2 (see Table \@ref(tab:test-retest-other-measures)).

```{r test-retest-plot, fig.cap="Each dot represents one subject's Intention-Behavior gap score at timepoint one (t1) and timepoint two (t2).", echo=FALSE, message=FALSE, warning=FALSE}
# Plot
ggplot(data=test_retest_all_gap, aes(x=gap_t1, y=gap_t2))+
  geom_point()+
  geom_abline(slope=1, intercept = 0) +
  labs(x = "Gap at T1", y = "Gap at T2") +
  theme_minimal()
```

We believe this relatively low correlation for the gap measure could be partially due to influence on responses from current state levels of the gap at the time of completing the measure, which are expected to fluctuate. Additionally, we are studying a population (almost all first year undergraduate students) that we would expect to have higher state-level variance in their Intention-Behavior gap due to the unique moment in their lives starting a university education represents. As with any major life change, novel situations are ripe ground for Intention-Behavior gaps as we are attempting to set goals we have not previously attempted. One might expect someone in their 30s or 40s with a regular job to have a more stable test-retest value for this measure.


```{r test-retest-other-measures, echo=FALSE, message=FALSE}
#--------------#
# Self Control #
#--------------#

# COHORT 1
# create df with time 1 and time 2 columns
test1 = df[df$onOff=='onBoarding' & df$cohort == 'cohort 1',]
test1 = test1[c('brief_self_control_score', 'consent_sonaID', 'EndDate')]
names(test1) = c('gap_t1', 'StudentID', 'date_t1')
test2 = df[df$onOff=='offBoarding' & df$cohort == 'cohort 1',]
test2 = test2[c('brief_self_control_score', 'consent_sonaID', 'EndDate')]
names(test2) = c('gap_t2', 'StudentID', 'date_t2')

# merge cohort 1
test_retest = merge(test1, test2, by = 'StudentID')

# add participant ID
cohort1$StudentID = as.character(cohort1$Student.Number)
test_retest = merge(test_retest, cohort1[,c('ParticipantIdentifier', 'StudentID')], by = 'StudentID')

# remove rows missing values
test_retest1 <- test_retest[complete.cases(test_retest), ]

# COHORT 2
# create df with time 1 and time 2 columns
test1 = df[df$onOff=='onBoarding' & df$cohort == 'cohort 2',]
test1 = test1[c('brief_self_control_score', 'consent_sonaID', 'EndDate')]
names(test1) = c('gap_t1', 'StudentID', 'date_t1')
test2 = df[df$onOff=='offBoarding' & df$cohort == 'cohort 2',]
test2 = test2[c('brief_self_control_score', 'consent_sonaID', 'EndDate')]
names(test2) = c('gap_t2', 'StudentID', 'date_t2')

# merge cohort 2
test_retest = merge(test1, test2, by = 'StudentID')

# add participant ID
cohort2$StudentID = as.character(cohort2$Student.Number)
test_retest = merge(test_retest, cohort2[,c('ParticipantIdentifier', 'StudentID')], by = 'StudentID')

# remove rows missing values
test_retest2 <- test_retest[complete.cases(test_retest), ]

# combine
test_retest_all = bind_rows(test_retest1, test_retest2)

# calculate ICC
ICC1_self_control = ICC(test_retest_all[,c('gap_t1', 'gap_t2')], alpha = 0.5, lmer = F)

#-------------------#
# Conscientiousness #
#-------------------#

# COHORT 1
# create df with time 1 and time 2 columns
test1 = df[df$onOff=='onBoarding' & df$cohort == 'cohort 1',]
test1 = test1[c('con_hex_score', 'consent_sonaID', 'EndDate')]
names(test1) = c('gap_t1', 'StudentID', 'date_t1')
test2 = df[df$onOff=='offBoarding' & df$cohort == 'cohort 1',]
test2 = test2[c('con_hex_score', 'consent_sonaID', 'EndDate')]
names(test2) = c('gap_t2', 'StudentID', 'date_t2')

# merge cohort 1
test_retest = merge(test1, test2, by = 'StudentID')

# add participant ID
cohort1$StudentID = as.character(cohort1$Student.Number)
test_retest = merge(test_retest, cohort1[,c('ParticipantIdentifier', 'StudentID')], by = 'StudentID')

# remove rows missing values
test_retest1 <- test_retest[complete.cases(test_retest), ]

# COHORT 2
# create df with time 1 and time 2 columns
test1 = df[df$onOff=='onBoarding' & df$cohort == 'cohort 2',]
test1 = test1[c('con_hex_score', 'consent_sonaID', 'EndDate')]
names(test1) = c('gap_t1', 'StudentID', 'date_t1')
test2 = df[df$onOff=='offBoarding' & df$cohort == 'cohort 2',]
test2 = test2[c('con_hex_score', 'consent_sonaID', 'EndDate')]
names(test2) = c('gap_t2', 'StudentID', 'date_t2')

# merge cohort 2
test_retest = merge(test1, test2, by = 'StudentID')

# add participant ID
cohort2$StudentID = as.character(cohort2$Student.Number)
test_retest = merge(test_retest, cohort2[,c('ParticipantIdentifier', 'StudentID')], by = 'StudentID')

# remove rows missing values
test_retest2 <- test_retest[complete.cases(test_retest), ]

# combine
test_retest_all = bind_rows(test_retest1, test_retest2)

# calculate ICC
ICC1_conscientiousness = ICC(test_retest_all[,c('gap_t1', 'gap_t2')], alpha = 0.5, lmer = F)

#------#
# Grit #
#------#

# COHORT 1
# create df with time 1 and time 2 columns
test1 = df[df$onOff=='onBoarding' & df$cohort == 'cohort 1',]
test1 = test1[c('grit_scale_score', 'consent_sonaID', 'EndDate')]
names(test1) = c('gap_t1', 'StudentID', 'date_t1')
test2 = df[df$onOff=='offBoarding' & df$cohort == 'cohort 1',]
test2 = test2[c('grit_scale_score', 'consent_sonaID', 'EndDate')]
names(test2) = c('gap_t2', 'StudentID', 'date_t2')

# merge cohort 1
test_retest = merge(test1, test2, by = 'StudentID')

# add participant ID
cohort1$StudentID = as.character(cohort1$Student.Number)
test_retest = merge(test_retest, cohort1[,c('ParticipantIdentifier', 'StudentID')], by = 'StudentID')

# remove rows missing values
test_retest1 <- test_retest[complete.cases(test_retest), ]

# COHORT 2
# create df with time 1 and time 2 columns
test1 = df[df$onOff=='onBoarding' & df$cohort == 'cohort 2',]
test1 = test1[c('grit_scale_score', 'consent_sonaID', 'EndDate')]
names(test1) = c('gap_t1', 'StudentID', 'date_t1')
test2 = df[df$onOff=='offBoarding' & df$cohort == 'cohort 2',]
test2 = test2[c('grit_scale_score', 'consent_sonaID', 'EndDate')]
names(test2) = c('gap_t2', 'StudentID', 'date_t2')

# merge cohort 2
test_retest = merge(test1, test2, by = 'StudentID')

# add participant ID
cohort2$StudentID = as.character(cohort2$Student.Number)
test_retest = merge(test_retest, cohort2[,c('ParticipantIdentifier', 'StudentID')], by = 'StudentID')

# remove rows missing values
test_retest2 <- test_retest[complete.cases(test_retest), ]

# combine
test_retest_all = bind_rows(test_retest1, test_retest2)

ICC1_grit = ICC(test_retest_all[,c('gap_t1', 'gap_t2')], alpha = 0.5, lmer = F)

#------#
# DASS #
#------#

# COHORT 1
# create df with time 1 and time 2 columns
test1 = df[df$onOff=='onBoarding' & df$cohort == 'cohort 1',]
test1 = test1[c('DASS_overall', 'consent_sonaID', 'EndDate')]
names(test1) = c('gap_t1', 'StudentID', 'date_t1')
test2 = df[df$onOff=='offBoarding' & df$cohort == 'cohort 1',]
test2 = test2[c('DASS_overall', 'consent_sonaID', 'EndDate')]
names(test2) = c('gap_t2', 'StudentID', 'date_t2')

# merge cohort 1
test_retest = merge(test1, test2, by = 'StudentID')

# add participant ID
cohort1$StudentID = as.character(cohort1$Student.Number)
test_retest = merge(test_retest, cohort1[,c('ParticipantIdentifier', 'StudentID')], by = 'StudentID')

# remove rows missing values
test_retest1 <- test_retest[complete.cases(test_retest), ]

# COHORT 2
# create df with time 1 and time 2 columns
test1 = df[df$onOff=='onBoarding' & df$cohort == 'cohort 2',]
test1 = test1[c('DASS_overall', 'consent_sonaID', 'EndDate')]
names(test1) = c('gap_t1', 'StudentID', 'date_t1')
test2 = df[df$onOff=='offBoarding' & df$cohort == 'cohort 2',]
test2 = test2[c('DASS_overall', 'consent_sonaID', 'EndDate')]
names(test2) = c('gap_t2', 'StudentID', 'date_t2')

# merge cohort 2
test_retest = merge(test1, test2, by = 'StudentID')

# add participant ID
cohort2$StudentID = as.character(cohort2$Student.Number)
test_retest = merge(test_retest, cohort2[,c('ParticipantIdentifier', 'StudentID')], by = 'StudentID')

# remove rows missing values
test_retest2 <- test_retest[complete.cases(test_retest), ]

# combine
test_retest_all = bind_rows(test_retest1, test_retest2)

# calculate ICC
ICC1_DASS = ICC(test_retest_all[,c('gap_t1', 'gap_t2')], alpha = 0.5, lmer = F)

#------------#
# Sub. Happy #
#------------#

# COHORT 1
# create df with time 1 and time 2 columns
test1 = df[df$onOff=='onBoarding' & df$cohort == 'cohort 1',]
test1 = test1[c('sub_happy_score', 'consent_sonaID', 'EndDate')]
names(test1) = c('gap_t1', 'StudentID', 'date_t1')
test2 = df[df$onOff=='offBoarding' & df$cohort == 'cohort 1',]
test2 = test2[c('sub_happy_score', 'consent_sonaID', 'EndDate')]
names(test2) = c('gap_t2', 'StudentID', 'date_t2')

# merge cohort 1
test_retest = merge(test1, test2, by = 'StudentID')

# add participant ID
cohort1$StudentID = as.character(cohort1$Student.Number)
test_retest = merge(test_retest, cohort1[,c('ParticipantIdentifier', 'StudentID')], by = 'StudentID')

# remove rows missing values
test_retest1 <- test_retest[complete.cases(test_retest), ]

# COHORT 2
# create df with time 1 and time 2 columns
test1 = df[df$onOff=='onBoarding' & df$cohort == 'cohort 2',]
test1 = test1[c('sub_happy_score', 'consent_sonaID', 'EndDate')]
names(test1) = c('gap_t1', 'StudentID', 'date_t1')
test2 = df[df$onOff=='offBoarding' & df$cohort == 'cohort 2',]
test2 = test2[c('sub_happy_score', 'consent_sonaID', 'EndDate')]
names(test2) = c('gap_t2', 'StudentID', 'date_t2')

# merge cohort 2
test_retest = merge(test1, test2, by = 'StudentID')

# add participant ID
cohort2$StudentID = as.character(cohort2$Student.Number)
test_retest = merge(test_retest, cohort2[,c('ParticipantIdentifier', 'StudentID')], by = 'StudentID')

# remove rows missing values
test_retest2 <- test_retest[complete.cases(test_retest), ]

# combine
test_retest_all = bind_rows(test_retest1, test_retest2)

# calculate ICC
ICC1_sub_happy = ICC(test_retest_all[,c('gap_t1', 'gap_t2')], alpha = 0.5, lmer = F)

#-----------------#
# Quality of Life #
#-----------------#

# COHORT 1
# create df with time 1 and time 2 columns
test1 = df[df$onOff=='onBoarding' & df$cohort == 'cohort 1',]
test1 = test1[c('qol_score', 'consent_sonaID', 'EndDate')]
names(test1) = c('gap_t1', 'StudentID', 'date_t1')
test2 = df[df$onOff=='offBoarding' & df$cohort == 'cohort 1',]
test2 = test2[c('qol_score', 'consent_sonaID', 'EndDate')]
names(test2) = c('gap_t2', 'StudentID', 'date_t2')

# merge cohort 1
test_retest = merge(test1, test2, by = 'StudentID')

# add participant ID
cohort1$StudentID = as.character(cohort1$Student.Number)
test_retest = merge(test_retest, cohort1[,c('ParticipantIdentifier', 'StudentID')], by = 'StudentID')

# remove rows missing values
test_retest1 <- test_retest[complete.cases(test_retest), ]

# COHORT 2
# create df with time 1 and time 2 columns
test1 = df[df$onOff=='onBoarding' & df$cohort == 'cohort 2',]
test1 = test1[c('qol_score', 'consent_sonaID', 'EndDate')]
names(test1) = c('gap_t1', 'StudentID', 'date_t1')
test2 = df[df$onOff=='offBoarding' & df$cohort == 'cohort 2',]
test2 = test2[c('qol_score', 'consent_sonaID', 'EndDate')]
names(test2) = c('gap_t2', 'StudentID', 'date_t2')

# merge cohort 2
test_retest = merge(test1, test2, by = 'StudentID')

# add participant ID
cohort2$StudentID = as.character(cohort2$Student.Number)
test_retest = merge(test_retest, cohort2[,c('ParticipantIdentifier', 'StudentID')], by = 'StudentID')

# remove rows missing values
test_retest2 <- test_retest[complete.cases(test_retest), ]

# combine
test_retest_all = bind_rows(test_retest1, test_retest2)

# calculate ICC
ICC1_qol = ICC(test_retest_all[,c('gap_t1', 'gap_t2')], alpha = 0.5, lmer = F)

#-------------#
# Flourishing #
#-------------#

# COHORT 1
# create df with time 1 and time 2 columns
test1 = df[df$onOff=='onBoarding' & df$cohort == 'cohort 1',]
test1 = test1[c('flourishing_score', 'consent_sonaID', 'EndDate')]
names(test1) = c('gap_t1', 'StudentID', 'date_t1')
test2 = df[df$onOff=='offBoarding' & df$cohort == 'cohort 1',]
test2 = test2[c('flourishing_score', 'consent_sonaID', 'EndDate')]
names(test2) = c('gap_t2', 'StudentID', 'date_t2')

# merge cohort 1
test_retest = merge(test1, test2, by = 'StudentID')

# add participant ID
cohort1$StudentID = as.character(cohort1$Student.Number)
test_retest = merge(test_retest, cohort1[,c('ParticipantIdentifier', 'StudentID')], by = 'StudentID')

# remove rows missing values
test_retest1 <- test_retest[complete.cases(test_retest), ]

# COHORT 2
# create df with time 1 and time 2 columns
test1 = df[df$onOff=='onBoarding' & df$cohort == 'cohort 2',]
test1 = test1[c('flourishing_score', 'consent_sonaID', 'EndDate')]
names(test1) = c('gap_t1', 'StudentID', 'date_t1')
test2 = df[df$onOff=='offBoarding' & df$cohort == 'cohort 2',]
test2 = test2[c('flourishing_score', 'consent_sonaID', 'EndDate')]
names(test2) = c('gap_t2', 'StudentID', 'date_t2')

# merge cohort 2
test_retest = merge(test1, test2, by = 'StudentID')

# add participant ID
cohort2$StudentID = as.character(cohort2$Student.Number)
test_retest = merge(test_retest, cohort2[,c('ParticipantIdentifier', 'StudentID')], by = 'StudentID')

# remove rows missing values
test_retest2 <- test_retest[complete.cases(test_retest), ]

# combine
test_retest_all = bind_rows(test_retest1, test_retest2)

# calculate ICC
ICC1_flourish = ICC(test_retest_all[,c('gap_t1', 'gap_t2')], alpha = 0.5, lmer = F)



xdata <- data.frame(
  Measure = c("Intention-Behavior Gap", "Self-Control", "Conscientiousness", "Grit", "DASS", "Subjective Happiness", "Quality of Life", "Flourishing"),
  ICC = c(round(ICC_gap$results$ICC[1], 2), 
          round(ICC1_self_control$results$ICC[1], 2),
          round(ICC1_conscientiousness$results$ICC[1], 2),
          round(ICC1_grit$results$ICC[1], 2),
          round(ICC1_DASS$results$ICC[1], 2),
          round(ICC1_sub_happy$results$ICC[1], 2),
          round(ICC1_qol$results$ICC[1], 2),
          round(ICC1_flourish$results$ICC[1], 2)),
  LowerBound = c(round(ICC_gap$results$`lower bound`[1], 2),
                 round(ICC1_self_control$results$`lower bound`[1], 2),
                 round(ICC1_conscientiousness$results$`lower bound`[1], 2),
                 round(ICC1_grit$results$`lower bound`[1], 2),
                 round(ICC1_DASS$results$`lower bound`[1], 2),
                 round(ICC1_sub_happy$results$`lower bound`[1], 2),
                 round(ICC1_qol$results$`lower bound`[1], 2),
                 round(ICC1_flourish$results$`lower bound`[1], 2)),
  UpperBound = c(round(ICC_gap$results$`upper bound`[1], 2),
                 round(ICC1_self_control$results$`upper bound`[1], 2),
                 round(ICC1_conscientiousness$results$`upper bound`[1], 2),
                 round(ICC1_grit$results$`upper bound`[1], 2),
                 round(ICC1_DASS$results$`upper bound`[1], 2),
                 round(ICC1_sub_happy$results$`upper bound`[1], 2),
                 round(ICC1_qol$results$`upper bound`[1], 2),
                 round(ICC1_flourish$results$`upper bound`[1], 2))
)

kable(xdata, format = "html", table.attr = "class=nofluid",
      caption = "Comparison of the test-retest reliability of the Intention-Behavior gap measure with other moderator and outcome variables."
      ) %>%
  kable_styling(bootstrap_options = c("striped", "hover"))
```

### Test-Retest Variability
We decided to test the potential influence of both regression to the mean and current state-level gaps on the trait level measure. These two approaches are related in that we would expect the mechanism behind a regression to the mean to be largely driven by the fact that individuals happened to be in a particularly high or low-gap state at the time of completing the trait measure.

#### Regression to the mean
We first tested if regression to the mean did, in fact, seem to be occurring in our sample. Regression to the mean describes the general phenomenon that within a given subject particularly high or low measurements tend to be followed by measurements that are closer to the population mean (Barnett, Pols & Dobson, 2005). The explanation for this phenomenon is that sampling variability itself tends to be normally distributed so that within subject if you take a measurement that is at the far end of their distribution, due to noise, the next time you sample that same person it is unlikely that you will get such an extreme value. To see if this was the case with our own data we took the top an bottom quintiles of our Intention-Behavior gap measure scores at t1 and looked at how the scores for those same subjects changed when moving to t2.

```{r test-retest-regression-to-mean, echo=FALSE, message=FALSE}

# Note that this is actually looking at the top/bottom 20%, not 10...
filter_top10 = test_retest_ibgap %>%
  filter(gap_t1 > quantile(gap_t1, 0.8))

top10_t1_mean = round(mean(filter_top10$gap_t1, na.rm=T), 2)
top10_t1_sd = round(sd(filter_top10$gap_t1, na.rm=T), 2)
top10_t2_mean = round(mean(filter_top10$gap_t2, na.rm=T), 2)
top10_t2_sd = round(sd(filter_top10$gap_t2, na.rm=T), 2)
top10_ttest = t.test(filter_top10$gap_t1, filter_top10$gap_t2, alternative = "greater")
ttest_top20_p = round(top10_ttest$p.value, 3)

filter_bottom10 = test_retest_ibgap %>%
  filter(gap_t1 < quantile(gap_t1, 0.2))

bottom10_t1_mean = round(mean(filter_bottom10$gap_t1, na.rm=T), 2)
bottom10_t1_sd = round(sd(filter_bottom10$gap_t1, na.rm=T), 2)
bottom10_t2_mean = round(mean(filter_bottom10$gap_t2, na.rm=T), 2)
bottom10_t2_sd = round(sd(filter_bottom10$gap_t2, na.rm=T), 2)
bottom10_ttest = t.test(filter_bottom10$gap_t1, filter_bottom10$gap_t2, alternative = "less", na.rm=T)
ttest_bottom20_p = round(bottom10_ttest$p.value, 3)
```
We did find support for our intuition that regression to the mean was occurring (see Figure \@ref(fig:test-retest-regression-to-mean-plot)). Taking the highest 20% of gaps from t1 the average was a `r top10_t1_mean`% gap (SD = `r top10_t1_sd`). At t2 this number had decreased on average to `r top10_t2_mean`% (SD = `r top10_t2_sd`). The same pattern, but in the opposite direction, was found when looking at the lowest 20% of gaps, where the t1 average was `r bottom10_t1_mean`% (SD = `r bottom10_t1_sd`) and the t2 average `r bottom10_t2_mean` (SD = `r bottom10_t2_sd`). The difference in means was significant in both cases, based on a one-tailed Welch two sample t-test (p = `r ttest_top20_p` in the first case and p = `r ttest_bottom20_p`).

```{r test-retest-regression-to-mean-plot, fig.cap="Change in average Intention-Behavior gap scores for the top and bottom quintiles of subjects as measured at t1.", echo=FALSE, message=FALSE}

df_plot <- tibble(
  Time = c("Time 1", "Time 1", "Time 2", "Time 2", "Time 1", "Time 2"),
  Category = c("Top 20%", "Bottom 20%", "Top 20%", "Bottom 20%", "Mean", "Mean"),
  Value = c(top10_t1_mean, bottom10_t1_mean, top10_t2_mean, bottom10_t2_mean, mean(test_retest_ibgap$gap_t1), mean(test_retest_ibgap$gap_t2)),
  SE = c( top10_t1_sd/sqrt(length(filter_top10)),
          top10_t2_sd/sqrt(length(filter_top10)),
          bottom10_t1_sd/sqrt(length(filter_bottom10)),
          bottom10_t2_sd/sqrt(length(filter_bottom10)),
          sd(test_retest_ibgap$gap_t1)/sqrt(length(test_retest_ibgap)),
          sd(test_retest_ibgap$gap_t2)/sqrt(length(test_retest_ibgap))
          )
)

ggplot(df_plot, aes(x = Time, y = Value, color = Category, group = Category)) +
  geom_point(size = 3) +
  geom_line() +
  geom_errorbar(aes(ymin = Value - SE, ymax = Value + SE), width = 0.2) +
  labs(y = 'Intention-Behavior Gap', x = '') +
  theme_minimal()
```

#### Recency bias
Given that there does seem to be regression to the mean in our samples, we can question what might be causing this measurement error. One possible source, as pointed out before, is the biasing of responses that are intended to capture trait-level gap magnitude by a subject's current state-gap level gap. Given that for most subjects we have daily measurements of their state-level Intention-Behavior gaps we can look to see if there seems to be an influence from temporally adjacent state-level gap reports and the trait-level measure.

To test our hypothesis we took the completion date of each subject's trait-level measure at t1 and t2 and compared it to an average of state-level gap reports for the week leading up to the measurement (including the day of the measurement). Note that in some cases we did not have data for all seven days, and so simply averaged across the days that were accounted for in that week-long window. Our expectation was that we would find a higher correlation between the trait-level measure and the temporally adjacent state-level seven-day average, than between the trait-level measure and a more temporally distant state-level seven-day average (i.e., $r_{matchedTime} > r_{unmatchedTime}$).

```{r test-retest-state-effects, echo=FALSE, message=FALSE}
library(lubridate)

# load daily data
daily_gaps_1 = read_csv('/Users/djw/Documents/pCloud_synced/Academics/Projects/2020_thesis/thesis_experiments/3_experiments/3_3_experience_sampling/3_3_2_processed_data/run_1/run1_selfReport.csv')
daily_gaps_1 = daily_gaps_1[c('ParticipantIdentifier', 'DAILY_past24_gap', 'DAILY_goal1_report', 'DAILY_goal2_report', 'trial_date')]

daily_gaps_2 = read_csv('/Users/djw/Documents/pCloud_synced/Academics/Projects/2020_thesis/thesis_experiments/3_experiments/3_3_experience_sampling/3_3_2_processed_data/run_2/run2_selfReport.csv')
daily_gaps_2 = daily_gaps_2[c('ParticipantIdentifier', 'DAILY_past24_gap', 'DAILY_goal1_report', 'DAILY_goal2_report', 'trial_date')]

# bind dfs
daily_gaps = bind_rows(daily_gaps_1, daily_gaps_2)

# Convert date columns in to date type
test_retest_all_gap$date_t1 <- ymd_hms(test_retest_all_gap$date_t1)
test_retest_all_gap$date_t2 <- ymd_hms(test_retest_all_gap$date_t2)
daily_gaps$trial_date = ymd(daily_gaps$trial_date)

# Specify range of dates
#date_range = 7

# Function to retrieve dates and compute composite
compute_composite <- function(participant, date, is_t2=FALSE) {
  dates_range <- seq(date - days(6), date, by = "days")
  
  # Extract relevant rows
  relevant_data <- daily_gaps %>%
    filter(ParticipantIdentifier == participant,
           trial_date %in% date(dates_range))
  
  # Check if less than 7 days and adjust
  if (nrow(relevant_data) < 7) {
    num_missing <- 7 - nrow(relevant_data)
    
    # If the date is "date_t2" and falls after the latest date in df2
    if (is_t2 && all(daily_gaps[daily_gaps$ParticipantIdentifier == participant,]$trial_date < date(date))) {
      
      # Last day of collected data for participant
      max_date = max(daily_gaps[daily_gaps$ParticipantIdentifier == participant,]$trial_date)
      # Is there more than 3 days between the last collected data day and the trait measure completion?
      if(date(date) - max_date  > 3){
        # If so, then we don't use that data
        extra_dates = NA
        # relevant_data$DAILY_past24_gap = NA
        # relevant_data$DAILY_goal1_report = NA
        # relevant_data$DAILY_goal2_report = NA
      }
      else{
        # If there is less than 3 or fewer days gap then we take the previous 7 days data (whether completed or not)
        extra_dates <- seq(max_date - days(6), max_date, by = "days")
      }
    } 
    else {
      # this is for t1 errors
      extra_dates <- seq(date + days(1), date + days(num_missing), by = "days")
    }
    
    relevant_data <- rbind(relevant_data, 
                           daily_gaps %>% filter(ParticipantIdentifier == participant,
                                          trial_date %in% date(extra_dates)))
  }
  
  # Compute composite value
  composite_value <- (mean(relevant_data$DAILY_past24_gap, na.rm=T) * (3/4) +
                      (100 - mean(relevant_data$DAILY_goal1_report, na.rm=T)) * (1/8) +
                      (100 - mean(relevant_data$DAILY_goal2_report, na.rm=T)) * (1/8))
  return(composite_value)
}

# Apply function to each row in df1
test_retest_all_gap$daily_means_t1 <- mapply(compute_composite, test_retest_all_gap$ParticipantIdentifier, date(test_retest_all_gap$date_t1), FALSE)
test_retest_all_gap$daily_means_t2 <- mapply(compute_composite, test_retest_all_gap$ParticipantIdentifier, date(test_retest_all_gap$date_t2), TRUE)

# Calculate correlations and SE
r_c1_match = cor.test(test_retest_all_gap$gap_t1, test_retest_all_gap$daily_means_t1)$estimate
r_c1_nonMatch = cor.test(test_retest_all_gap$gap_t1, test_retest_all_gap$daily_means_t2)$estimate
n_c1_match = 104
n_c1_nonMatch = 56

r_c2_match = cor.test(test_retest_all_gap$gap_t2, test_retest_all_gap$daily_means_t2)$estimate
r_c2_nonMatch = cor.test(test_retest_all_gap$gap_t2, test_retest_all_gap$daily_means_t1)$estimate
n_c2_match = 56
n_c2_nonMatch = 104

# Test for significance in correlation differences
# Fisher's z-transformation.
z_c1_match <- 0.5 * log((1 + r_c1_match) / (1 - r_c1_match))
z_c1_nonMatch <- 0.5 * log((1 + r_c1_nonMatch) / (1 - r_c1_nonMatch))

z_c2_match <- 0.5 * log((1 + r_c2_match) / (1 - r_c2_match))
z_c2_nonMatch <- 0.5 * log((1 + r_c2_nonMatch) / (1 - r_c2_nonMatch))

# Standard error and z-value for the difference
SE_diff_c1 <- sqrt(1/(n_c1_match - 3) + 1/(n_c1_nonMatch - 3))
z_diff_c1 <- (z_c1_match - z_c1_nonMatch) / SE_diff_c1

SE_diff_c2 <- sqrt(1/(n_c2_match - 3) + 1/(n_c2_nonMatch - 3))
z_diff_c2 <- (z_c2_match - z_c2_nonMatch) / SE_diff_c2

# One-tailed p-value
p_value_c1 <- 1 - pnorm(abs(z_diff_c1))
p_value_c2 <- 1 - pnorm(abs(z_diff_c2))
```

Note that when there was missing data (i.e. not all measurements for the seven day period were present) we conducted the calculation on the reduced number of days rather than keep going back in time, as the state-level correlation would be expected to decay. We also did not create state-level scores for people who completed their off-boarding trait-level measures more than 3 days after the completion of data collection. Again, we wanted to avoid having an extended gap between the trait-level measure and the state-level data.

We used Fisher's z-transformation to test whether the two correlations were significantly different, where:

\begin{equation} 
  z_{\text{diff}} = \frac{z_1 - z_2}{SE}
  (\#eq:z-transform)
\end{equation} 

And standard error ($SE$) is calculated as follows:

\begin{equation} 
  SE = \sqrt{\frac{1}{n_1 - 3} + \frac{1}{n_2 - 3}}
  (\#eq:standard-error)
\end{equation} 

Where $n_1$ and $n_2$ are the sample sizes for the two correlations. Fisher's z-transformation is described as:

\begin{equation} 
  z = \frac{1}{2} \ln \left( \frac{1 + r}{1 - r} \right)
  (\#eq:fishers-z-transform)
\end{equation} 

We then conducted a one-tailed test for significance:

\begin{equation} 
  p = 1 - \Phi(\lvert z_{\text{diff}} \rvert)
  (\#eq:one-tailed-phi)
\end{equation} 

We found, as anticipated, that the daily-gap measures that were proximate in time to the onboarding Intention-Behavior gap measure correlated slightly more strongly with the initial gap measures than with the offboarding Intention-Behavior gap measures, but the difference was not significant ($r_{matchedTime} = .45, r_{unmatchedTime} = .42, p = .41$). The daily-gap measures proximate in time to the offboarding measure were also more strongly correlated with the offboarding Intention-Behavior gap measure than with the onboarding measure, as predicted, but, again, those differences were not significant ($r_{matchedTime} = .47, r_{unmatchedTime} = .37, p = .26$). So, while the hypothesized effect is strictly present, the magnitude of that effect is not large enough to allow us to say that our hypothesis has been supported by the data (Figure \@ref(fig:test-retest-state-effects-plot)).

```{r test-retest-state-effects-plot, fig.cap="Comparison of correlation values of temporally matched state and trait measurments, versus temporally distant state and trait measures.", echo=FALSE, message=FALSE}

df_plot <- tibble(
  Time = c("Matched", "Matched", "Unmatched", "Unmatched"),
  Category = c("Onboarding", "Offboarding", "Onboarding", "Offboarding"),
  Value = c(r_c1_match, r_c2_match, r_c1_nonMatch, r_c2_nonMatch),
)


ggplot(df_plot, aes(x = Time, y = Value, color = Category, group = Category)) +
  geom_point(size = 3) +
  geom_label(aes(label = round(Value,2)), hjust = 1.3, color = 'black', size=3) +  # Adjust vertical position with vjust
  geom_line() +
  labs(y = 'Correlation (r)', x = 'Comparison') +
  theme_minimal()
```

## Measure vs. Daily Mean

```{r measure-vs-daily-gap-ICCs, echo=FALSE, message=FALSE}
# import daily gaps
daily_gaps = read_csv('/Users/djw/Documents/pCloud_synced/Academics/Projects/2020_thesis/thesis_experiments/3_experiments/3_3_experience_sampling/3_3_2_processed_data/run_2/run2_selfReport.csv')

daily_gaps = daily_gaps[c('ParticipantIdentifier', 'DAILY_past24_gap', 'DAILY_goal1_report', 'DAILY_goal2_report')]
# convert success to gap
daily_gaps$DAILY_goal1_report = 100 - daily_gaps$DAILY_goal1_report
daily_gaps$DAILY_goal2_report = 100 - daily_gaps$DAILY_goal2_report

daily_gaps$combined_gap = ((3/4) * daily_gaps$DAILY_past24_gap) + ((1/8) * daily_gaps$DAILY_goal1_report) + ((1/8) * daily_gaps$DAILY_goal2_report)

daily_gaps_agg = daily_gaps %>% 
    group_by(ParticipantIdentifier) %>% 
    summarise(mean_daily_gap = mean(DAILY_past24_gap, na.rm = T),
              mean_daily_gap_combo = mean(combined_gap, na.rm = T),
              n = n()
              )

# summary stats of daily data observations
daily_n_m = describe(daily_gaps_agg$n)$mean
daily_n_sd = describe(daily_gaps_agg$n)$sd

# merge daily gaps with test-retest df
trt_daily = merge(test_retest_all_gap, daily_gaps_agg, by = 'ParticipantIdentifier')
# create mean of onboarding offboarding measures
trt_daily$mean_measure = (trt_daily$gap_t1 + trt_daily$gap_t2) / 2

# calculate ICC: onboarding, daily
ICC2_on_daily = round(ICC(trt_daily[,c('gap_t1', 'mean_daily_gap')], alpha = 0.5, lmer = F)$results$ICC[2], 2)
# calculate ICC: onboarding, daily combo
ICC2_on_dailyCombo = round(ICC(trt_daily[,c('gap_t1', 'mean_daily_gap_combo')], alpha = 0.5, lmer = F)$results$ICC[2], 2)
# calculate ICC: offboarding, daily
ICC2_off_daily = round(ICC(trt_daily[,c('gap_t2', 'mean_daily_gap')], alpha = 0.5, lmer = F)$results$ICC[2], 2)
# calculate ICC: offboarding, daily combo
ICC2_off_dailyCombo = round(ICC(trt_daily[,c('gap_t2', 'mean_daily_gap_combo')], alpha = 0.5, lmer = F)$results$ICC[2], 2)
# calculate ICC: mean off/on, daily
ICC2_onOff_daily = round(ICC(trt_daily[,c('mean_measure', 'mean_daily_gap')], alpha = 0.5, lmer = F)$results$ICC[2], 2)
# calculate ICC: mean off/on, daily combo
ICC2_onOff_dailyCombo = round(ICC(trt_daily[,c('mean_measure', 'mean_daily_gap_combo')], alpha = 0.5, lmer = F)$results$ICC[2], 2)

```

Given that we collected 12 weeks of daily measures of the Intention-Behavior gap, another opportunity to assess the reliability of the Intention-Behavior gap measure would be to see how an average of the onboarding and offboarding Intention-Behavior gap trait measures correlates with the daily reported gaps over the course of the 12-week study. This test of reliability is motivated by the idea that you would expect many measurements of the state-level Intention-Behavior gap taken over an extended period of time to approximate the trait level measure. We tested to see if this was the case in our data, with the caveat that since we do not have a clear idea of the timescale of fluctuations in the state-level measure, it is not guaranteed that 12 weeks is a long enough time period to reliably approach each subjects true trait-level mean. To conduct our test we used an intraclass correlation statistic to test how similar the mean of the daily self-reports were with the Intention Behavior Gap measure value for each participant. As a reminder, the daily self-report measure was provided by participants each evening. The number of self reports varied, up to a max of 84 ($M = `r round(daily_n_m, 2)`, SD = `r round(daily_n_sd, 2)`$). 

In the table below (Table \@ref(tab:measure-dailyGap-table)) we looked at both the onboarding and offboarding measurements separately as well as a combination of the two measurements (averaged together). We also looked at the daily gap as calculated based on a single measure ("Over the past 24 hours the level of my intention gap was:", on a scale of 0-100%), as well as a composite measure which included the gaps on two specific goals the participant had set for themselves. We used a simple weighting scheme of 75%/25% for the overall measure and the two specific goal measures. In all cases we found that this composite daily gap measure was more highly correlated with the Intention-Behavior Gap measure, and also found that combining both the onboarding and offboarding measurements of the instrument provided the highest correlation.


```{r measure-dailyGap-table, echo=FALSE, message=FALSE}
# create table
tab_01 = data.frame(
  measure_source = c("onboarding", "onboarding", "offboarding", "offboarding", "combined", "combined"),
  daily_source = c("single measure", "composite", "single measure", "composite", "single measure", "composite"),
  ICC = c(ICC2_on_daily, ICC2_on_dailyCombo, ICC2_off_daily, ICC2_off_dailyCombo, ICC2_onOff_daily, ICC2_onOff_dailyCombo)
)

tab_01_table = knitr::kable(
  tab_01,
  format = "html",
  table.attr = "class='table'",
  booktabs = TRUE,
  escape = FALSE,
  col.names = c("Measure Source", "Daily Gap", "ICC"),
  align = c("l", "l", "l"),
  caption = "ICC of Intention Behavior Gap measure and daily self-report of gap"
  )

kable_styling(tab_01_table, 
              full_width = TRUE, 
              bootstrap_options = c("striped", "condensed"))

my_table <- t(apply(cars, 2, function(x) # Create data
  round(c(Mean = mean(x), SD = sd(x), Min = min(x), Max = max(x)), 2)
))
```

# Validation
In order to assess how well our measure is accurately capturing a domain-general Intention-Behavior gap we employed both traditional and measure-specific methods. We will start with examining convergent validity, where we test associations between our measure and other measures that we believe should, theoretically, be associated with the Intention-Behavior gap. We then conduct a test to check that our measure predicts out of sample domains for a given subject better than the population mean. We then test whether we can gain increased explanatory power by weighting our Intention-Behavior gap using additional information we have for each domain (importance, time and effort). We identify models of best-fit for our outcome measures from a set of predictors including all moderators and our weighted Intention-Behavior gap measure. Finally, we confirm that the Intention-Behavior gap is indeed related to behavior, and not simply ambitious goal-setting.

## Content Validity
We attempted to establish content validity of our measure, the idea that our items captures the full range of the construct/characteristic, by first building off canonical instruments. Specifically we used the Canadian Time Use Survey (2015 - 2016, Cycle 29) developed by Statistics Canada and the American Time Use Survey (2011-2022) developed by the U.S. Bureau of Labor Statistics. We then used the Delphi method to refine and add to this list.

``` {r other count, echo=FALSE, message=FALSE, warning=FALSE}
other_percentage = 100 * (1 - sum(is.na(chron$ib_domain_success_Other)) / length(chron$ib_domain_success_Other))
```
Beyond these steps, given the phenomenon of "unknown-unknowns" it is tricky to confirm that a measure is sampling from the entirety of the domain the concept in question covers. However, in our case we included an optional "other" goal category in the measure, which could be seen as an indicator of the degree to which our items were incomplete. A minority of subjects, `r round(other_percentage, 1)`%, indicated that they had a goal that did not fit within our listed categories. We did not require participants to specify what their "other" goal was, so while we may assume that their responses would have been idiosyncratic enough not to merit a single new category, we are unable to confirm this, which is a shortcoming of the study design. That said, for the majority of subjects the provided items appeared to capture the full range of their goal categories.

## Convergent Validity

### Moderators
Given that there are not established measures of the Intention-Behavior gap that we are aware of, we were not able to compare the performance of our instrument to others that are designed to measure the same construct. That said, we did select a number of measures that we hypothesized would moderate the translation of intentions to behavior, specifically self-control, conscientiousness, grit, sensation-seeking, future time perspective, ambition, social desirability bias, and work ethic.

```{r concurrent-validity-moderator-correlations, fig.cap="Correlations between the Intention-Behavior gap and constructs believed to moderate the relationship between intentions and behavior. Note: \\* p<0.05; \\** p<0.01; \\*** p<0.001", echo=FALSE, message=FALSE, warning=FALSE}
# predicting gap cols
cor_cols_predict_gap = c('con_hex_score',
             'social_des_score',
             'ambition_score',
             'brief_self_control_score',
             'bsss_overall',
             'future_time_perspective_score',
             'grit_scale_score',
             'secular_measure_work_ethic_score',
             
             'domain_gap'
)


# Calculate correlations and p-values for var1 with other variables
var_names <- names(df[cor_cols_predict_gap])
# Excluding domain gap
var_names = subset(var_names, var_names!= 'domain_gap')
cor_table <- data.frame(Variable = character(), Correlation = numeric(), P_Value = numeric(), stringsAsFactors = FALSE)

for (var_name in var_names) {
  cor_result <- cor.test(df$domain_gap, df[[var_name]])
  cor_table <- rbind(cor_table, data.frame(Variable = var_name, Correlation = round(cor_result$estimate, 2), P_Value = round(cor_result$p.value, 4)))
}

# Rename variables
variable_newNames = c(
  'Conscientiousness',
  'Social desirability',
  'Ambition',
  'Self-control',
  'Sensation-seeking',
  'Future perspective',
  'Grit',
  'Work ethic'
)

cor_table$Variable = variable_newNames

# Remove row names
rownames(cor_table) <- NULL

# Order by correlation
cor_table <- cor_table %>%
  arrange(Correlation) %>%
  mutate(Variable = factor(Variable, levels = Variable))

# Creating a significance column
cor_table <- cor_table %>%
  mutate(significance = case_when(
    P_Value < .001 ~ "***",
    P_Value < .01  ~ "**",
    P_Value < .05  ~ "*",
    TRUE           ~ ""
  ))

# Plot
ggplot(cor_table, aes(x = Variable, y = Correlation, fill = Correlation)) +
  geom_col() + coord_flip() +
  scale_fill_gradient2(low = "blue", mid = "white", high = "red", midpoint = 0,
                       limits = c(-1, 1), guide = "colorbar") +
  geom_label(aes(label = paste0(round(Correlation, 2), significance)), 
             vjust = 0.5, hjust = 0.5, fill='white', color = "black") +
  labs(x = 'Moderator construct', y = 'Correlation (r)') +
  ylim(min(cor_table$Correlation)-.03, max(cor_table$Correlation)+.03) +  # Use ylim instead of scale_x_continuous
  theme_minimal() +
  theme(legend.position = "none")

# chart.Correlation(df[cor_cols_predict_gap], histogram=TRUE, pch=19)

# # create table
# tab_01 = data.frame(
#   measure_source = c("onboarding", "onboarding", "offboarding", "offboarding", "combined", "combined"),
#   daily_source = c("single measure", "composite", "single measure", "composite", "single measure", "composite"),
#   ICC = c(ICC2_on_daily, ICC2_on_dailyCombo, ICC2_off_daily, ICC2_off_dailyCombo, ICC2_onOff_daily, ICC2_onOff_dailyCombo)
# )
# 
# tab_01_table = knitr::kable(
#   tab_01,
#   format = "html",
#   table.attr = "class='table'",
#   booktabs = TRUE,
#   escape = FALSE,
#   col.names = c("Measure Source", "Daily Gap", "ICC"),
#   align = c("l", "c", "c"),
#   caption = "ICC of Intention Behavior Gap measure and daily self-report of gap"
#   )
# 
# kable_styling(tab_01_table, 
#               full_width = TRUE, 
#               bootstrap_options = c("striped", "condensed"))
```


We found that all hypothesized moderators, except for sensation seeking and work ethic, were significantly correlated with the Intention-Behavior gap (see Table above). Those measures that were significantly associated had small to moderate correlations. This is in line with our expectations as we do not see these constructs as equivalent to the gap, and we also believe there will be considerable effects of individual differences in terms of the strength of any particular moderator. This level of correlation also reduces the risk of issues arising due to multicollinearity when conducting model comparison.

### Outcomes: Well-being
In addition to moderators of the gap, we hypothesized that the Intention-Behavior gap would be negatively associated with well-being (i.e. a higher level of gap would correlate with lower levels of well-being). To test this hypothesis and provide further validation for our measure we looked at measures for flourishing, harmony, quality of life, satisfaction with life, subjective happiness, self-esteem, stress, and a composite depression/anxiety/stress scale. It is worth noting that given the estimate that intentional activity accounts for approximately 40% of the variance in happiness (Lyubomirsky, Sheldon, & Schkade, 2005), theoretically we would expect an upper bound of any given correlation with well-being to be $r = .63$.

We did find that, as hypothesized, all of our measures of well-being were significantly correlated ($ps < .001$) with the Intention-Behavior gap, most at a moderate level (see Figure \@ref(fig:predictive-validity-plot)).

```{r predictive-validity-plot, fig.cap="Correlations between the Intention-Behavior gap and well-being measures. DASS refers to the Depression, Anxiety and Stress Scale. Note: \\* p<0.05; \\** p<0.01; \\*** p<0.001", echo=FALSE, message=FALSE, warning=FALSE}
# gap predicts cols
cor_cols_gap_predicts = c(
                         'flourishing_score',
                         'harmony_score',
                         'qol_score',
                         'sat_life_score',
                         'sub_happy_score',
                         'DASS_overall',
                         'perceived_stress_score',
                         'rosenberg_SES_score',
                         'domain_gap'
)


# Calculate correlations and p-values for var1 with other variables
var_names <- names(df[cor_cols_gap_predicts])
# Excluding domain gap
var_names = subset(var_names, var_names!= 'domain_gap')
cor_table <- data.frame(Variable = character(), Correlation = numeric(), P_Value = numeric(), stringsAsFactors = FALSE)

for (var_name in var_names) {
  cor_result <- cor.test(df$domain_gap, df[[var_name]])
  cor_table <- rbind(cor_table, data.frame(Variable = var_name, Correlation = round(cor_result$estimate, 2), P_Value = cor_result$p.value))
}

# Rename variables
variable_newNames = c(
  'Flourishing',
  'Harmony',
  'Quality of life',
  'Life satisfaction',
  'Subjective Happiness',
  'DASS',
  'Perceived stress',
  'Self-esteem'
)

cor_table$Variable = variable_newNames

# Remove row names
rownames(cor_table) <- NULL

# Order by correlation
cor_table <- cor_table %>%
  arrange(Correlation) %>%
  mutate(Variable = factor(Variable, levels = Variable))

# Creating a significance column
cor_table <- cor_table %>%
  mutate(significance = case_when(
    P_Value < .001 ~ "***",
    P_Value < .01  ~ "**",
    P_Value < .05  ~ "*",
    TRUE           ~ ""
  ))

# Plot
ggplot(cor_table, aes(x = Variable, y = Correlation, fill = Correlation)) +
  geom_col() + coord_flip() +
  scale_fill_gradient2(low = "blue", mid = "white", high = "red", midpoint = 0,
                       limits = c(-1, 1), guide = "colorbar") +
  geom_label(aes(label = paste0(round(Correlation, 2), significance)), 
             vjust = 0.5, hjust = 0.5, fill='white', color = "black") +
  labs(x = 'Outcome measure', y = 'Correlation (r)') +
  ylim(-0.5, 0.35) +  # Use ylim instead of scale_x_continuous
  theme_minimal() +
  theme(legend.position = "none")

```


```{r predictive-validity-table, echo=FALSE, message=FALSE, warning=FALSE}
# Calculate correlations and p-values for var1 with other variables
# var_names <- names(df[cor_cols_gap_predicts])
# # Excluding domain gap
# var_names = subset(var_names, var_names!= 'domain_gap')
# cor_table <- data.frame(Variable = character(), Correlation = numeric(), P_Value = numeric(), stringsAsFactors = FALSE)
# 
# for (var_name in var_names) {
#   cor_result <- cor.test(df$domain_gap, df[[var_name]])
#   cor_table <- rbind(cor_table, data.frame(Variable = var_name, Correlation = round(cor_result$estimate, 2), P_Value = cor_result$p.value))
# }
# # order by correlation
# cor_table = cor_table[order(cor_table$Correlation),]
# rownames(cor_table) <- NULL
# 
# # # create table
# # tab_01 = data.frame(
# #   measure_source = c("onboarding", "onboarding", "offboarding", "offboarding", "combined", "combined"),
# #   daily_source = c("single measure", "composite", "single measure", "composite", "single measure", "composite"),
# #   ICC = c(ICC2_on_daily, ICC2_on_dailyCombo, ICC2_off_daily, ICC2_off_dailyCombo, ICC2_onOff_daily, ICC2_onOff_dailyCombo)
# # )
# # 
# # tab_01_table = knitr::kable(
# #   tab_01,
# #   format = "html",
# #   table.attr = "class='table'",
# #   booktabs = TRUE,
# #   escape = FALSE,
# #   col.names = c("Measure Source", "Daily Gap", "ICC"),
# #   align = c("l", "c", "c"),
# #   caption = "ICC of Intention Behavior Gap measure and daily self-report of gap"
# #   )
# knitr::kable(
#   cor_table,
#   format = "html",
#   table.attr = "class='table'",
#   booktabs = TRUE,
#   caption = "ICC of Intention Behavior Gap measure and daily self-report of gap"
# ) %>%
# kable_styling(full_width = TRUE,
#               bootstrap_options = c("striped", "condensed"))
```

### Outcomes: Empirical
In addition to non-observable psychological constructs (such as well-being), we looked at two empirical outcomes, body mass index (BMI, self-reported) and grades (provided by the University of Toronto).

#### BMI
We looked at associations between BMI and a number of moderator and outcome variables along with the Intention-Behavior gap.

```{r bmi-measure-correlations, fig.cap="Correlations between various measures and BMI. Note: \\* p<0.05; \\** p<0.01; \\*** p<0.001", echo=FALSE, message=FALSE, warning=FALSE}

# gap predicts cols
cor_cols_bmi = c(
  'con_hex_score',
  'brief_self_control_score',
  'bsss_overall',
  'grit_scale_score',
  'sat_life_score',
  'sub_happy_score',
  'DASS_overall',
  'rosenberg_SES_score'
)


#--------------------------#
# mean bmi by mean measure #
#--------------------------#
# adding some extra measures to this dataframe or additional analyses
df$grade_predict_gap = df$gradePredictAvg - df$grades_avg

# COHORT 1
# create df with time 1 and time 2 columns
test1 = df[df$onOff=='onBoarding' & df$cohort == 'cohort 1',]
test1 = test1[c('domain_gap', 'consent_sonaID', 'EndDate', 'bmi', 'grades_avg', 'grade_predict_gap', cor_cols_bmi)]
names(test1) = c('gap_t1', 'StudentID', 'date_t1', 'bmi_t1', 'grades_avg', 'grade_predict_gap',
                 "con_hex_score_t1", "brief_self_control_score_t1", "bsss_overall_t1", "grit_scale_score_t1",
                 "sat_life_score_t1", "sub_happy_score_t1", "DASS_overall_t1", "rosenberg_SES_score_t1"
                 )
test2 = df[df$onOff=='offBoarding' & df$cohort == 'cohort 1',]
test2 = test2[c('domain_gap', 'consent_sonaID', 'EndDate', 'bmi', 'grades_avg', 'grade_predict_gap', cor_cols_bmi)]
names(test2) = c('gap_t2', 'StudentID', 'date_t2', 'bmi_t2', 'grades_avg', 'grade_predict_gap',
                 "con_hex_score_t2", "brief_self_control_score_t2", "bsss_overall_t2", "grit_scale_score_t2",
                 "sat_life_score_t2", "sub_happy_score_t2", "DASS_overall_t2", "rosenberg_SES_score_t2"
                 )

# merge cohort 1
test_retest = merge(test1, test2, by = 'StudentID')

# add participant ID
cohort1$StudentID = as.character(cohort1$Student.Number)
test_retest = merge(test_retest, cohort1[,c('ParticipantIdentifier', 'StudentID')], by = 'StudentID')

# remove rows where bmi_t1 and bmi_t2 are both na
test_retest1 <- test_retest[!(is.na(test_retest$bmi_t1) & is.na(test_retest$bmi_t2)), ]

# add cohort name
test_retest1$cohort = 'cohort 1'

# COHORT 2
# create df with time 1 and time 2 columns
test1 = df[df$onOff=='onBoarding' & df$cohort == 'cohort 2',]
test1 = test1[c('domain_gap', 'consent_sonaID', 'EndDate', 'bmi', 'grades_avg', 'grade_predict_gap', cor_cols_bmi)]
names(test1) = c('gap_t1', 'StudentID', 'date_t1', 'bmi_t1', 'grades_avg', 'grade_predict_gap',
                 "con_hex_score_t1", "brief_self_control_score_t1", "bsss_overall_t1", "grit_scale_score_t1",
                 "sat_life_score_t1", "sub_happy_score_t1", "DASS_overall_t1", "rosenberg_SES_score_t1"
                 )
test2 = df[df$onOff=='offBoarding' & df$cohort == 'cohort 2',]
test2 = test2[c('domain_gap', 'consent_sonaID', 'EndDate', 'bmi', 'grades_avg', 'grade_predict_gap', cor_cols_bmi)]
names(test2) = c('gap_t2', 'StudentID', 'date_t2', 'bmi_t2', 'grades_avg', 'grade_predict_gap',
                 "con_hex_score_t2", "brief_self_control_score_t2", "bsss_overall_t2", "grit_scale_score_t2",
                 "sat_life_score_t2", "sub_happy_score_t2", "DASS_overall_t2", "rosenberg_SES_score_t2"
                 )

# merge cohort 2
test_retest = merge(test1, test2, by = 'StudentID')

# add participant ID
cohort2$StudentID = as.character(cohort2$Student.Number)
test_retest = merge(test_retest, cohort2[,c('ParticipantIdentifier', 'StudentID')], by = 'StudentID')

# remove rows where bmi_t1 and bmi_t2 are both na
test_retest2 <- test_retest[!(is.na(test_retest$bmi_t1) & is.na(test_retest$bmi_t2)), ]

# add cohort name
test_retest2$cohort = 'cohort 2'

# combine
t1t2_bmi = bind_rows(test_retest1, test_retest2)

# means
t1t2_bmi$bmi_mean = rowMeans(t1t2_bmi[, c("bmi_t1", "bmi_t2")], na.rm = TRUE)
t1t2_bmi$gap_mean = rowMeans(t1t2_bmi[, c("gap_t1", "gap_t2")], na.rm = TRUE)
t1t2_bmi$consientiousness = rowMeans(t1t2_bmi[, c("con_hex_score_t1", "con_hex_score_t2")], na.rm = TRUE)
t1t2_bmi$self_control = rowMeans(t1t2_bmi[, c("brief_self_control_score_t1", "brief_self_control_score_t2")], na.rm = TRUE)
t1t2_bmi$sensation_seeking = rowMeans(t1t2_bmi[, c("bsss_overall_t1", "bsss_overall_t2")], na.rm = TRUE)
t1t2_bmi$grit = rowMeans(t1t2_bmi[, c("grit_scale_score_t1", "grit_scale_score_t2")], na.rm = TRUE)
t1t2_bmi$sat_life = rowMeans(t1t2_bmi[, c("sat_life_score_t1", "sat_life_score_t2")], na.rm = TRUE)
t1t2_bmi$sub_happy = rowMeans(t1t2_bmi[, c("sub_happy_score_t1", "sub_happy_score_t2")], na.rm = TRUE)
t1t2_bmi$DASS = rowMeans(t1t2_bmi[, c("DASS_overall_t1", "DASS_overall_t2")], na.rm = TRUE)
t1t2_bmi$self_esteem = rowMeans(t1t2_bmi[, c("rosenberg_SES_score_t1", "rosenberg_SES_score_t2")], na.rm = TRUE)

# remove bmi outlier at 57
t1t2_bmi <- t1t2_bmi[!(t1t2_bmi$bmi_mean > 50), ]
# remove gap outlier at 80
t1t2_bmi <- t1t2_bmi[!(t1t2_bmi$gap_mean > 80), ]

# # linear model
# summary(lm(bmi_mean ~ gap_mean, data = t1t2_bmi))



# Plot correlations
cor_cols_bmi = c(
  'bmi_mean',
  'consientiousness',
  'self_control',
  'sensation_seeking',
  'grit',
  'sat_life',
  'sub_happy',
  'DASS',
  'self_esteem',
  'gap_mean'
)

# Calculate correlations and p-values for var1 with other variables
var_names <- names(t1t2_bmi[cor_cols_bmi])
# Excluding bmi
var_names = subset(var_names, var_names!= 'bmi_mean')
cor_table <- data.frame(Variable = character(), Correlation = numeric(), P_Value = numeric(), stringsAsFactors = FALSE)

for (var_name in var_names) {
  cor_result <- cor.test(t1t2_bmi$bmi_mean, t1t2_bmi[[var_name]])
  cor_table <- rbind(cor_table, data.frame(Variable = var_name, Correlation = round(cor_result$estimate, 2), P_Value = round(cor_result$p.value, 2)))
}

# Rename variables
variable_newNames = c(
  'Conscientiousness',
  'Self-control',
  'Sensation-seeking',
  'Grit',
  'Life satisfaction',
  'Subjective happiness',
  'DASS',
  'Self-esteem',
  'I-B gap'
)

cor_table$Variable = variable_newNames

# Remove row names
rownames(cor_table) <- NULL

# Order by correlation
cor_table <- cor_table %>%
  arrange(Correlation) %>%
  mutate(Variable = factor(Variable, levels = Variable))

# Creating a significance column
cor_table <- cor_table %>%
  mutate(significance = case_when(
    P_Value < .001 ~ "***",
    P_Value < .01  ~ "**",
    P_Value < .05  ~ "*",
    TRUE           ~ ""
  ))

# Plot
ggplot(cor_table, aes(x = Variable, y = Correlation, fill = Correlation)) +
  geom_col() + coord_flip() +
  scale_fill_gradient2(low = "blue", mid = "white", high = "red", midpoint = 0,
                       limits = c(-1, 1), guide = "colorbar") +
  geom_label(aes(label = paste0(round(Correlation, 2), significance)), 
             vjust = 0.5, hjust = 0.5, fill='white', color = "black") +
  labs(x = 'Measure', y = 'Correlation (r)') +
  ylim(-0.15, 0.21) +  # Use ylim instead of scale_x_continuous
  theme_minimal() +
  theme(legend.position = "none")

# chart.Correlation(t1t2_bmi[cor_cols_bmi], histogram=TRUE, pch=19)

```

We found that only the Intention-Behavior gap was significantly associated with BMI. The association was small and positive (i.e. a larger gap correlates with a higher BMI). While most of the remaining measures tended to at least correlate in the anticipated direction (e.g. DASS: positive, self-control: negative), none of those correlations reached significance ($ps > .19$). We can also view the scatterplot of individual subjects which illustrates this relationship more clearly (Figure \@ref(fig:bmi-measure-scatterplot)).

```{r bmi-measure-scatterplot, fig.cap="Scatterplot of subjects BMI versus their Intention-Behavior gap measure. Note that both BMI and the gap measure are calculated from an average of the t1 and t2 values, for those subjects that responded at both timepoints.", echo=FALSE, message=FALSE, warning=FALSE}

# calculate correlation
cor_result <- cor.test(t1t2_bmi$gap_mean, t1t2_bmi$bmi_mean, use="pairwise.complete.obs")

# Plot scatter plot
ggplot(t1t2_bmi, aes(x = bmi_mean, y = gap_mean)) +
  geom_point() +
  geom_smooth(method = "lm", se = FALSE, color = "lightblue") +
  geom_label(aes(x = 35, y = 75, label = paste("r =", round(cor_result$estimate, 2), ", p =", format(round(cor_result$p.value, 2), scientific = FALSE))), color = "black") +
  labs(x = "BMI", y ="IB-Gap") +
  theme_minimal()
```



#### Grades

We collected end of semester grade data from all participants and then tested for relationships between the same set of variables as chosen for BMI. In this case if we plot our subjects as before (Figure \@ref(fig:grades-gap-scatterplot)) we can see that there is no relationship between the Intention-Behavior gap measure and grade point average (GPA).

```{r grades-gap-scatterplot, fig.cap="Scatterplot of all subjects, where I-B gap values are averages of t1 and t2 values when data is available. GPA is calculated out of 100.", echo=FALSE, warning=FALSE, message=FALSE}
# calculate correlation
cor_result <- cor.test(t1t2_bmi$gap_mean, t1t2_bmi$grades_avg.x, use="pairwise.complete.obs")

# Plot scatter plot
ggplot(t1t2_bmi, aes(x = grades_avg.x, y = gap_mean)) +
  geom_point() +
  geom_smooth(method = "lm", se = FALSE, color = "lightblue") +
  geom_label(aes(x = 55, y = 75, label = paste("r =", round(cor_result$estimate, 2), ", p =", format(round(cor_result$p.value, 2), scientific = FALSE))), color = "black") +
  labs(x = "GPA", y ="I-B Gap") +
  theme_minimal()
```

However, this was not the case for all measures, as illustrated in Figure \@ref(fig:grades-correlation-measures).

```{r grades-correlation-measures, fig.cap="Correlations between GPA and various measures. Note: \\* p<0.05; \\** p<0.01; \\*** p<0.001", echo=FALSE, warning=FALSE, message=FALSE}
# Plot correlations
# rename for clarity
t1t2_bmi$grades = t1t2_bmi$grades_avg.x

cor_cols = c(
  'grades',
  'consientiousness',
  'self_control',
  'sensation_seeking',
  'grit',
  'sat_life',
  'sub_happy',
  'DASS',
  'self_esteem',
  'gap_mean'
)

# Calculate correlations and p-values for var1 with other variables
var_names <- names(t1t2_bmi[cor_cols])
# Excluding bmi
var_names = subset(var_names, var_names!= 'grades')
cor_table <- data.frame(Variable = character(), Correlation = numeric(), P_Value = numeric(), stringsAsFactors = FALSE)

for (var_name in var_names) {
  cor_result <- cor.test(t1t2_bmi$grades, t1t2_bmi[[var_name]])
  cor_table <- rbind(cor_table, data.frame(Variable = var_name, Correlation = round(cor_result$estimate, 2), P_Value = round(cor_result$p.value, 2)))
}

# Rename variables
variable_newNames = c(
  'Conscientiousness',
  'Self-control',
  'Sensation-seeking',
  'Grit',
  'Life satisfaction',
  'Subjective happiness',
  'DASS',
  'Self-esteem',
  'I-B gap'
)

cor_table$Variable = variable_newNames

# Remove row names
rownames(cor_table) <- NULL

# Order by correlation
cor_table <- cor_table %>%
  arrange(Correlation) %>%
  mutate(Variable = factor(Variable, levels = Variable))

# Creating a significance column
cor_table <- cor_table %>%
  mutate(significance = case_when(
    P_Value < .001 ~ "***",
    P_Value < .01  ~ "**",
    P_Value < .05  ~ "*",
    TRUE           ~ ""
  ))

# Plot
ggplot(cor_table, aes(x = Variable, y = Correlation, fill = Correlation)) +
  geom_col() + coord_flip() +
  scale_fill_gradient2(low = "blue", mid = "white", high = "red", midpoint = 0,
                       limits = c(-1, 1), guide = "colorbar") +
  geom_label(aes(label = paste0(round(Correlation, 2), significance)), 
             vjust = 0.5, hjust = 0.5, fill='white', color = "black") +
  labs(x = 'Measure', y = 'Correlation (r)') +
  ylim(min(cor_table$Correlation)-.05, max(cor_table$Correlation)+.05) +  # Use ylim instead of scale_x_continuous
  theme_minimal() +
  theme(legend.position = "none")
# chart.Correlation(t1t2_bmi[cor_cols], histogram=TRUE, pch=19)
```

## Out of Sample Prediction
If there exists is a domain-general Intention-Behavior gap, as we propose, we would expect a random sample of the Intention-Behavior gap magnitudes a person had over a number of goal domains to be predictive of an out of sample goal domain gap for that same person. In this case "predictive" means that we believe the expected value of their gap in a held out domain, as calculated based on the average of their gaps in the sampled domains, to contain information about their true gap value in that held out domain. Mathematically this could be expressed as:

\begin{equation} 
  \mathbb{E}(D_{x}) = \frac{1}{|S|} \sum_{\substack{y \in S \\ y \neq x}} D_y
  (\#eq:expected-value-held-out-domain)
\end{equation} 

Where:
- $\mathbb{E}(D_{x})$ represents the expected value of held out domain $x$.
- $D_y$ is the value of the $y^{th}$ domain.
- $|S|$ represents the number of domains in the sampled subset $S$.

We should be clear that while this equation represents the logic of our theory, we do not, in practice, expect to find a precise equivalence, due to individual variance across domains. However, even with this variance we do expect a significantly smaller prediction error if $D_x$ and the sampled $D_y$s are from the same subject, rather than from a different subject, or taken from the sample population mean. We take three approaches to validating that this is indeed the case. 

In our first approach, we use a permutation test, where, for a given subject (e.g. $subject_i$) and specific domain (e.g. $D_{ix}$), instead of trying to predict the specific domain gap from the mean of that same subject's other domain gaps ($D_{iy}$s), we use a randomly chosen subject's (e.g. $subject_j$) average domain gap (excluding the selected domain). We use this methodology to create a set of "predicted" gaps for all of the domains of $subject_i$. We create estimates for all other subjects using this same methodology. We then repeated this process 1000 times and each time calculate the error in each estimate (for a given subject in a given domain ($D_{ix}$)) as follows:

\begin{equation} 
  \text{Error} = \left| \mathbb{E}(D_{ix}) - D_{ix} \right|
  (\#eq:error-expected-value)
\end{equation}   
  
$\text{Error}$ is the absolute difference between the expected value of $D_x$ (represented by $\mathbb{E}(D_x)$) and the actual value of $D_x$. This is then averaged using the following formula:

\begin{equation} 
  \text{AvgError} = \frac{1}{\sum_{i=1}^{n} m_i} \sum_{i=1}^{n} \sum_{j=1}^{m_i} \text{Error}_{ij}
  (\#eq:average-error)
\end{equation} 

Where:
- $n$ represents the total number of subjects.
- $m_i$ represents the total number of domains for the $i^{th}$ subject.
- $\text{Error}_{ij}$ represents the error for the $j^{th}$ domain of the $i^{th}$ subject.

The denominator in the fraction to average all of the error scores is $\sum_{i=1}^{n} m_i$ instead of simply $n \times m$ since each subject can have a different number of goal domains. We then compare the $\text{AvgError}$ of a matched subject with the distribution of $\text{AvgError}$s we get from our permuted subjects to test if a subject's own goal domain gaps do a better job of predicting a held out domain than a random other subject's domain gaps. Figure \@ref(fig:out-of-sample-gap-prediction) below suggests that this is indeed the case.

To note as well is the fact that we normalized all gap scores before conducting this calculation, given that there are systematic differences in goal domain gap magnitudes (e.g. Volunteering average gap is close to 60% while Work average gap is just below 35%). This means we are trying to predict how many standard deviations above or below the average a subject is in each domain, rather than their actual score.

```{r out-of-sample-gap-prediction, fig.cap="Plot of distribution of average errors. The red dotted line represents the average error when calculated with the actual (non-permuted) subjects data.", echo=FALSE, message=FALSE, warning=FALSE}
# calculate 

# Select success columns
data = df %>%
  select(contains('domain_success_'))

# Convert to numeric
data <- data.frame(lapply(data, as.numeric))

# Normalize
data_norm = data.frame(scale(data))

# create a function to compute the mean of all non-NA values in a row, excluding the current column value
compute_mean <- function(row, current_column) {
  values <- row[!is.na(row)]
  values <- values[-current_column]
  mean(values)
}

# create the second dataframe
data_norm2 <- data_norm

# for each row in the original dataframe, compute the values for the second dataframe
for (i in 1:nrow(data_norm)) {
  for (j in 1:ncol(data_norm)) {
    if (is.na(data_norm[i, j])) {
      data_norm2[i, j] <- NA
    } else {
      data_norm2[i, j] <- compute_mean(data_norm[i, ], j)
    }
  }
}


#------#
# Test #
#------#

data_norm_complete <- data_norm[!apply(is.na(data_norm), 1, all), ]
data_norm2_complete <- data_norm2[!apply(is.na(data_norm2), 1, all), ]

## T Test of errors
# Original - Predicted vs. Original - Avg (0)
pred_error = abs(data_norm_complete - data_norm2_complete)

vec_pred <- as.vector(as.matrix(pred_error))
vec_mean <- as.vector(as.matrix(abs(data_norm_complete)))
vec_pred = na.omit(vec_pred)
vec_mean = na.omit(vec_mean)

# Test between predicted and average gap values
gap_mean_t = t.test(vec_pred, vec_mean)

# Permutation Test
## Takes a long time to run so can just load the data from previous trial with n = 1000
permutation_avg_error = read_rds('data/permutation_avg_error.rds')
## mix the order of the rows before converting to vector and measuring average error

# # set the number of resamples
# n <- 1000
# 
# # initialize a vector to store the permutation errors
# permutation_avg_error <- numeric(n)
# 
# # for each permutation resample
# for (rep in 1:n) {
#   print(rep)
#   # create prediction df
#   # create the second dataframe
#   pred_df <- data_norm_complete
#   
#   # shuffle indices
#   random_indices <- sample(nrow(data_norm_complete))
#   # for each row in the original dataframe, compute the values for the second dataframe
#   for (i in 1:nrow(data_norm_complete)) {
#     for (j in 1:ncol(data_norm_complete)) {
#       if (is.na(data_norm_complete[i, j])) {
#         pred_df[i, j] <- NA
#       } else {
#         pred_df[i, j] <- compute_mean(data_norm_complete[random_indices[i], ], j) # function ignores the current col when calculating mean
#       }
#     }
#   }
#   
#   # calculate error with new pred df
#   pred_error = abs(data_norm_complete - pred_df)
#   average_error = mean(unlist(pred_error), na.rm = T)
#   
#   # store the permuted error
#   permutation_avg_error[rep] <- average_error
# }
# 
# write_rds(permutation_avg_error, "permutation_avg_error.rds")

# # compute the 95% confidence interval
# ci <- quantile(permutation_avg_error, 0.05)
# 
# ci

# TTest
pred_error_mean = mean(vec_pred)
result = t.test(permutation_avg_error, mu = pred_error_mean)

# create a ggplot object
ggplot(data = data.frame(permutation_avg_error), aes(x=permutation_avg_error))+
  geom_density(fill="#619CFF", alpha = 0.5) +
  geom_vline(xintercept = mean(vec_pred), color = "#F8766D", linetype = "dashed") +
  geom_label(aes(x = mean(vec_pred), y = 10, label = "0.63"), color = "#F8766D", fill = "white") +
  labs(x = "Average Error (SD)", y = 'Density') +
  xlim(0.6, 1.04) +
  theme_minimal()



```

A one-sample t-test was computed to determine whether the permuted subject error was different to the within-subject error (`r round(pred_error_mean, 2)`). The permuted average prediction error across 1000 repetitions was `r round(result$estimate, 2)` which was significantly higher than the non-permuted within-subject error (95% CI[`r round(result$conf.int[1], 3)`, `r round(result$conf.int[2], 3)`, t(`r result$parameter`) = `r round(result$statistic, 1)`, p < .001).

The second, alternative approach we take to testing whether there is meaningful signal in our measure used a different prediction approach. In this case we looked to predict a subject's unmeasured domain gap ($d_{xi}$) by a random sample of another Intention-Behavior gap for that same subject, but in a different domain (e.g. $d_{yi}$). This is essentially asserting that for an unknown Intention-Behavior gap ($d_{xi}$) we expect to find a higher correlation between *any* other Intention-Behavior gap domain of that *same subject* and the unknown domain ($d_{xi}$), than between the unknown domain ($d_{xi}$) and a *different subject's* gap for that *same* domain ($d_{xj}$). To represent this mathematically, for a given subject $i$ and a randomly selected subject $j$:

\begin{equation} 
  \mathbb{E}[\rho(D_{ix}, D_{iy})] > \mathbb{E}[\rho(D_{ix}, D_{jx})]
  (\#eq:within-subject-versus-within-domain-inequality)
\end{equation} 

Where:
- $D_i = \{ D_{i1}, D_{i2}, ... , D_{in} \}$ represents a set of domain ($D$) values for subject $i$.
- $D_{ix}$ is a randomly chosen domain value from $D_i$.
- $D_{iy}$ is a second randomly chosen domain value from $D_i$ (where $x \neq y$).
- subject $j$ is randomly chosen where $i \neq j$).

To test this we replaced all values for all subjects in all domains (e.g. $D_{ix}$) with a randomly chosen other domain from the same subject (e.g. $D_{iy}$, where $x \neq y$) as well as with a random other subject's value from the same domain (e.g. $D_jx$, where $i \neq j$). We then tested the average correlation between these predicted values and the actual values. We repeated this process 100 times to come up with a distribution of correlations for both methods, as shown in Figure \@ref(fig:out-of-sample-predict-Cendri) below. 

``` {r out-of-sample-predict-Cendri, fig.cap = "Density plots of the correlation of a given domain with either a different domain from the same subject (within subject), or a different randomly selected subject but using the same domain (within domain). The two plots have zero overlap.", warning=FALSE, message=FALSE, echo=FALSE}
## CENDRI IDEA
# don't normalize
# Select success columns
data = df %>%
  select(contains('domain_success_'))

# Convert to numeric
data <- data.frame(lapply(data, as.numeric))
# vector version
data_vec <- as.vector(as.matrix(data))

# # Repeats
n <- 100
# 
# # initialize vectors to store the correlations
# subject_cor <- numeric(n)
# domain_cor <- numeric(n)
# 
# 
# # for each permutation resample
# for (rep in 1:n) {
#   print(rep)
#   # create prediction df
#   # create the second dataframe
#   pred_df <- data
#   rand_df <- data
#   
#   # for each row in the original dataframe, compute the values for the second dataframe
#   for (i in 1:nrow(data)) {
#     for (j in 1:ncol(data)) {
#       if (is.na(data[i, j])) {
#         pred_df[i, j] <- NA
#         rand_df[i, j] <- NA
#       } else {
#         # pred df
#         row_values = data[i, -j]
#         row_values = row_values[!is.na(row_values)]
#         pred_df[i, j] <- sample(row_values, 1) # rand select in row (subject)
#         # rand df
#         col_values = data[-i, j]
#         col_values = col_values[!is.na(col_values)]
#         rand_df[i, j] <- sample(col_values, 1) # rand select in column (domain)
#       }
#     }
#   }
# 
#   # calculate correlation with pred df
#   # convert the dataframes to vectors
#   pred_vec <- as.vector(as.matrix(pred_df))
#   rand_vec <- as.vector(as.matrix(rand_df))
#   
#   # compute the correlation between the two vectors
#   pred_cor <- cor(data_vec, pred_vec, use = 'complete.obs')
#   rand_cor = cor(data_vec, rand_vec, use = 'complete.obs')
# 
#   # store the correlations
#   subject_cor[rep] <- pred_cor
#   domain_cor[rep] <- rand_cor
# }
# 
# write_rds(subject_cor, "data/subject_cor43.rds")
# write_rds(domain_cor, "data/domain_cor43.rds")


## Takes a long time to run so can just load the data from previous trial with n = 100
subject_cor = read_rds('data/subject_cor43.rds')
domain_cor = read_rds('data/domain_cor43.rds')


# Create a combined data frame
plot_df <- data.frame(
  value = c(subject_cor, domain_cor),
  group = factor(rep(c("within subject", "within domain"), each=n))
)

# Calculate overlap percentage (this is a simplistic approximation)
min_max <- range(c(subject_cor, domain_cor))
x_vals <- seq(min_max[1], min_max[2], length.out = n)
dens1 <- density(subject_cor, from=min_max[1], to=min_max[2], n=n)$y
dens2 <- density(domain_cor, from=min_max[1], to=min_max[2], n=n)$y
overlap_area <- sum(pmin(dens1, dens2) * diff(x_vals)[1])
percentage_overlap <- overlap_area / sum(dens1 * diff(x_vals)[1]) * 100  # relative to the first distribution

# mean and 95% confidence interval
result = t.test(plot_df[plot_df$group=='within subject',]$value, plot_df[plot_df$group=='within domain',]$value)
within_s_mean = mean(plot_df[plot_df$group=='within subject',]$value)
within_d_mean = mean(plot_df[plot_df$group=='within domain',]$value)
within_s_sd = sd(plot_df[plot_df$group=='within subject',]$value)
within_d_sd = sd(plot_df[plot_df$group=='within domain',]$value)

# Plot
ggplot(plot_df, aes(x = value, fill = group)) +
  geom_density(alpha = 0.5) +
  xlab("Correlation (r)") + ylab("Density") +
  xlim(0, .35) +
  theme_minimal()
```

The mean correlation in the within-subject calculation was `r round(within_s_mean, 2)` (SD = `r round(within_s_sd, 2)`), whereas the mean for the within-domain calculation was `r round(within_d_mean, 2)` (SD = `r round(within_d_sd, 2)`). A Welch two-samples t-test showed that the correlation for predictions made using a subjects own domains was significantly greater than the correlation between a subject's gap in a given domain and a randomly chosen other subject's gap in that same domain (t(`r round(result$parameter, 1)`) = `r result$statistic`, p < .001).

``` {r mean vs. prediction test, echo=FALSE, message=FALSE, warning=FALSE}
# Original - Predicted vs. Original - Avg (0)
pred_errors_w_sub = abs(data_norm_complete - data_norm2_complete)
# create array of 0s
zero_array = array(0, dim = dim(data_norm_complete))
pred_errors_means = abs(data_norm_complete - zero_array)

# Calculate SDs
sd_w_sub = sd(unlist(pred_errors_w_sub), na.rm = T)
sd_means = sd(unlist(pred_errors_means), na.rm = T)

# T-test
result = t.test(pred_errors_w_sub, pred_errors_means)
```

Finally, the third approach was to test whether the within-subject method used in the first test of estimating the held out domain ($D_ix$) performs better than just taking the domain average for all domains (${ D_1, D_2, ... , D_n }$). We conducted this test with the normalized data for simplicity since in this case all predictions would simply be $0$. We then calculated the average error for both methods. The average error of the within subject method was `r round(result$estimate[1], 2)` (SD = `r round(sd_w_sub, 2)`) while the average error of the method that just used domain mean values was `r round(result$estimate[1], 2)` (SD = `r round(sd_means, 2)`). A Welch two-samples t-test showed that the within subject prediction significantly outperformed a sample mean by domain strategy (t(`r round(result$parameter, 0)`) = `r round(result$statistic, 2)`, p < .001). Combined, these tests give us confidence that our measure is picking up meaningful signal, and that Intention-Behavior gaps *do* generalize within subject across disparate domains.

## Informing the Intention-Behavior Gap
As an additional validation we wanted to check whether our Intention-Behavior gap measure had a significant association with our outcome of interest, well-being, above and beyond the hypothesized moderators of the translation of intentions into behavior which we previously found to be correlated with the gap measure itself (e.g. self-control, conscientiousness, etc.). However, before trying to determine models of best fit, we wanted to create a weighted version of the Intention-Behavior gap. This weighed measure would use the additional information we collected about each goal from our subjects.

So far our Intention-Behavior gap measure has ignored the fact that we know more than simply the Intention-Behavior gap in each selected goal domain. Specifically, we collected information about the importance, effort, and time requirements for the goals that our subjects had in each goal domain. Our reasoning is that it seems likely that the degree to which someone's success in a given goal domain affected their well-being would be related to how important that domain was to them, as well as how much time and effort were required for that goal domain. For example, you might expect those domains that are more important to contribute more to their well-being. To incorporate these additional dimensions to predict our outcome variable we employed a 10-fold cross-validation procedure (to avoid overfitting) using the following equation:

\begin{equation}
  Y \sim \beta_0 + \beta_1x_g + \beta_2x_i + \beta_3x_e + \beta_4x_t + \epsilon
  (\#eq:weightedGap-v1)
\end{equation}

In this equation (Equation \@ref(eq:weightedGap-v1)) $x_g$ is the Intention-Behavior gap, $x_i$ is the domain importance, $x_e$ is the effort to accomplish one's goals in that domain and $x_t$ is the amount of time required to complete one's goals in that domain. Fitting this model we find that we increase our correlation with our eight well-being outcome variables on average by 6.7%, going from an average correlation of .359 to .383.

The model did not seem to be overfitting, as training and testing error were very similar for the eight outcome variables, with a mean absolute difference between the errors of 0.03 (SD = 0.04), keeping in mind that we are working with normalized data so this difference is in units of standard deviation. It is also interesting to look at the coefficient values for the model. Domain success is clearly the main predictor of our well-being measures as you can observe in \@ref(fig:weighted-measure1) below.

```{r weighted-measure1, fig.cap = "Coefficient violin plots are labeled with mean values across all goal domain dimensions.", echo=FALSE, message=FALSE, warning=FALSE}

coefs = read_csv('data/model1_coefs.csv')

# convert to long
df_long <- coefs %>% 
  select(c('outcome', 'success', 'import', 'effort', 'time')) %>% 
  pivot_longer(cols = c(success, import, effort, time), names_to = "Variable", values_to = "Value")

# Calculate the mean for each variable
var_means <- df_long %>% 
  group_by(Variable) %>% 
  summarise(mean_value = mean(Value)) %>%
  arrange(desc(mean_value))

# Order the factor levels based on mean
df_long$Variable <- factor(df_long$Variable, levels = var_means$Variable)

# Plot
ggplot(df_long, aes(x=Variable, y=Value, fill=Variable)) +
  geom_violin(alpha = 0.5, show.legend = FALSE) +
  geom_boxplot(width = 0.1, fill = "white", alpha = 0.9, outlier.shape = NA) +
  geom_jitter(width=0.2, aes(color=Variable), show.legend = FALSE) + 
  geom_label(data = var_means, aes(y = mean_value, label = round(mean_value, 3)), vjust = 3.5, alpha = 0.9, show.legend = FALSE) +

  labs(y="Coefficient", x="") +
  # geom_label(aes(x = 1.5, y = 80, label = paste("t =", round(result$statistic, 2), "\np-value =", round(result$p.value, 3))), color = "black", fill = "white") +
  theme(legend.position = "none") +
  theme_minimal()
```

However, we could also imagine that the effect of success on well-being isn't simply a linear, but interacts with our other goal dimensions. For example, you might imagine that the a given level of success on a very important goal would have a greater impact on well-being than the same level of success on a goal of only modest importance. We have therefore constructed a second weighted-gap model that includes interactions between the Intention-Behavior gap and importance, effort, and time, as shown by the three terms added to the previous equation. We again used 10-fold cross-validation to fit the model.

\begin{equation}
  G_w \sim \beta_0 + \beta_1x_g + \beta_2x_i + \beta_3x_e + \beta_4x_t + \beta_5x_gx_i + \beta_6x_gx_e + \beta_7x_gx_t  + \epsilon
  (\#eq:weightedGap-v2)
\end{equation}

Here (Equation \@ref(eq:weightedGap-v2)), for example, $x_gx_i$ is the interaction, for a given domain, between a subject's gap and the importance of that gap ($e$ represents effort and $t$, time). This model correlated even more strongly with our well-being outcome variables, with a 19.8% improvement in average correlation values (from $r =.359$ to $r = .43$) over the "gap only" model, and a 12.3% increase over the weighted model without interaction terms (from $r =.383$ to $r = .43$).

Again, the model did not seem to be overfitting as training and testing error were very similar for the eight outcome variables, with a mean absolute difference of 0.03 (SD = 0.04, units are standard deviation). 

```{r weighted-measure2, fig.cap = "Coefficient violin plots are labeled with mean values across all goal domain dimensions, and interactions.", echo=FALSE, message=FALSE, warning=FALSE}

coefs = read_csv('data/model2_coefs.csv')

# convert to long
df_long <- coefs %>% 
  dplyr::select(c('outcome', 'success', 'import', 'effort', 'time', 'successXimport', 'successXeffort', 'successXtime')) %>% 
  pivot_longer(cols = c(success, import, effort, time, successXimport, successXeffort, successXtime), names_to = "Variable", values_to = "Value")

# Calculate the mean for each variable
var_means <- df_long %>% 
  group_by(Variable) %>% 
  summarise(mean_value = mean(Value)) %>%
  arrange(desc(mean_value))

# Order the factor levels based on mean
df_long$Variable <- factor(df_long$Variable, levels = var_means$Variable)

# Plot
ggplot(df_long, aes(x=Variable, y=Value, fill=Variable)) +
  geom_violin(alpha = 0.5, show.legend = FALSE) +
  geom_boxplot(width = 0.1, fill = "white", alpha = 0.9, outlier.shape = NA) +
  geom_jitter(width=0.2, aes(color=Variable), show.legend = FALSE) + 
  geom_label(data = var_means, aes(y = mean_value, label = round(mean_value, 3)), vjust = 3.5, alpha = 0.9, show.legend = FALSE) +

  labs(x = "", y="Coefficient") +
  # geom_label(aes(x = 1.5, y = 80, label = paste("t =", round(result$statistic, 2), "\np-value =", round(result$p.value, 3))), color = "black", fill = "white") +
  theme(legend.position = "none") +
  theme_minimal()
```

Looking at the coefficient values for the model a few interesting observations can be immediately made. First, domain success is no longer the main predictor of our well-being measures - and not only that, but the sign of the success coefficient has flipped, from positive ($\beta = .22$, p<??) to negative ($\beta = -.11, p<??$). This raises the interesting idea that greater than average success in accomplishing a goal that has only average importance, takes an average amount of time, and requires an average effort, actually has a negative impact on well-being (recall that this data is normalized and so the sample average maps to zero). Less surprisingly the model posits that if you achieve only average success on a goal that expending above average effort in achieving that goal lowers well-being ($\beta=-.23, p<??$). The other most predictive variables in the model were the interactions between the gap and effort ($\beta=.32, p<??$), as well as the gap and importance ($\beta=.13, p<??$). The fact that the interaction between success and effort was the single best predictor surprised us as we had expected the interaction between success and importance to be more important.

## Model Selection

We wanted to test whether our Intention-Behavior gap measure explained unique variance in our well-being outcome measures when other predictors were included in the same model.

### Individual Outcome Variable Prediction
Specifically, we added all moderator variables and the weighted Intention-Behavior gap then iterated through all possible models using the `leaps` package (v3.1; Lumley, 2020). This selects the best fitting model for a given dependent variable (in this case our outcome measures) for each possible number of predictor variables. In our case given that we had nine predictors (our eight moderator variables plus our weighted Intention-Behavior gap measure), we ended up with a series of models, starting with one predictor and going all the way up to nine predictor variables.

We then evaluated each of these models individually using nested (100 repetitions) five-fold cross-validation, to avoid overfitting, with the `caret` package (version 6.0.94; Kuhn, Max, 2008). Each repetition of the cross-validation process selected a best fitting model for each outcome measure, based on minimizing RMSE on the held out measure across folds. Across iterations of this process the model with the lowest RMSE varied, usually fluctuating between two or three models that best predicted a given outcome (Figure \@ref(fig:measure-fit-comparisons)). The fact that the best-fitting model varied in repeated iterations of the cross-fold validation indicates that there were multiple models that had very similar fits, and the random splitting of the data led to one being selected over another in a given iteration. We selected the modal model - that is to say the model that was most often selected as best fitting over the 100 iterations - as the final best fitting model.

```{r measure-fit-comparisons, fig.cap="Bars show frequency that a model with a given number of parameters was selected in the cross-validation process. In total 100 iterations of the cross-fold validation process were run.", echo=FALSE, message=FALSE, warning=FALSE}

# Nested cross validation code...
# Commented out as we are loading a saved df since running the nested cross validation takes quite a long time

# # load data
# df_weighted = read_csv('/Users/djw/Documents/pCloud_synced/Academics/Projects/2020_thesis/thesis_experiments/3_experiments/3_1_gap_measure/3_1_2_processed_data/weighted_gap.csv')
# 
# # select prediction and outcome columns 
# predict_cols = c(
#   'con_hex_score',
#   'social_des_score',
#   'ambition_score',
#   'brief_self_control_score',
#   'bsss_overall',
#   'future_time_perspective_score',
#   'grit_scale_score',
#   'secular_measure_work_ethic_score',
#   'model2_gap' # weighted gap
# )
# 
# outcome_cols = c(
#   'DASS_overall_Reversed',
#   'flourishing_score',
#   'harmony_score',
#   'perceived_stress_Reversed',
#   'rosenberg_SES_score',
#   'qol_score',
#   'sat_life_score',
#   'sub_happy_score'
# )
# 
# # Select relevant columns
# df_weighted = df_weighted[ ,c(predict_cols, outcome_cols)]
# 
# # Store all results
# results <- data.frame(
#   Outcome = character(0),
#   RMSE = numeric(0),
#   DF = numeric(0),
#   R_2 = numeric(0),
#   setNames(lapply(predict_cols, function(x) numeric(0)), predict_cols),  # Columns for each predictor
#   var_num = numeric(0),
#   iteration = numeric(0)
# )
# 
# ## Repeated Cross-Validation to achieve predictor stability
# # Set number of repeats
iterations = 100
# 
# # Repeat CV process
# for (i in 1:iterations) {
#   # print(paste('Cross Validation Repeat #', i))
#   
#   # # Set seed
#   # set.seed(42)
#   
#   # Iterate over outcomes
#   for (outcome in outcome_cols){
#     
#     # select data with relevent predictor
#     data = df_weighted[,c(predict_cols, outcome)]
#     # Remove rows with NA values
#     data = na.omit(data)
#     data = data.frame(scale(data))
#     # Weighted gap is actually "success", not gap, so reverse it
#     data$model2_gap = data$model2_gap * -1
#     
#     # for each outcome, use regsubsets to find the best models
#     best_subsets <- regsubsets(as.formula(paste(outcome, "~ .")), data = data, nvmax = ncol(data) - 1)
#     
#     # Extract the best models based on coefficients
#     best_models <- lapply(1:(ncol(data) - 1), function(num_predictors) {
#       coefs <- coef(best_subsets, id = num_predictors)
#       predictors <- names(coefs)[-1]  # exclude intercept
#       formula_str <- paste(outcome, "~", paste(predictors, collapse = " + "))
#       return(as.formula(formula_str))
#     })
#     
#   # Iterate over best models
#     for (formula_obj in best_models) {
#       
#       # 10-fold cross-validation
#       ctrl <- trainControl(method = "cv", number = 5, savePredictions = TRUE)
#       fit <- train(formula_obj, data = data, method = "lm", trControl = ctrl, metric = "RMSE")
#       
#       # Extracting the outcome variable name
#       outcome_var <- as.character(formula_obj[[2]])
#       
#       # Extracting the coefficients from the final linear model
#       coefficients <- fit$finalModel$coefficients
#       
#       # Storing results
#       temp_df <- data.frame(
#         Outcome = outcome_var,
#         RMSE = fit$results$RMSE,
#         DF = df.residual(fit$finalModel),
#         R_2 = with(fit$finalModel, summary.lm(fit$finalModel)$r.squared),
#         matrix(NA, ncol = length(predict_cols), nrow = 1, dimnames = list(NULL, predict_cols)),
#         var_num = length(fit$finalModel$coefficients) - 1,
#         iteration = i
#       )
#       
#       # Fill in the coefficients for the predictors in this model
#       temp_df[1, names(coefficients)] <- coefficients
#       
#       # Add to the results data frames=
#       results <- rbind(results, temp_df)
#     }
#   }
# }
# 
# # Rename columns for clarity
# results <- results %>%
#   rename(
#     Conscientiousness = con_hex_score,
#     `Social desirability` = social_des_score,
#     Ambition = ambition_score,
#     `Self-control` = brief_self_control_score,
#     `Sensation-seeking` = bsss_overall,
#     `Future-perspective` = future_time_perspective_score,
#     Grit = grit_scale_score,
#     `Work ethic` = secular_measure_work_ethic_score,
#     `Intention-behavior gap` = model2_gap
#   )
# 
# # Save nested cross-validation results
# saveRDS(results, file = "data/model_selection_CV.rds")

# Load data from nested-cross-validation
results = readRDS(file = "data/model_selection_CV.rds")

# Select best models by lowest RMSE (1 for each iteration)
best_fits = results %>%
  group_by(Outcome, iteration) %>%
  filter(RMSE == min(RMSE)) %>%
  ungroup() %>%
  dplyr::select(-`(Intercept)`) # remove intercept column

# Rename outcomes for clarity
new_names = c(
  'DASS (reversed)',
  'Flourishing',
  'Harmony',
  'Stress (reversed)',
  'Self-esteem',
  'Quality of life',
  'Life satisfaction',
  'Subjective happiness'
)

# Repeat based on number of iterations
new_names = rep(new_names, iterations)
# Assign new names
best_fits$Outcome = new_names

# Plot hists of best model number of predictors for each outcome variable
ggplot(best_fits, aes(x = var_num)) +
  geom_histogram(binwidth = 1, fill = "dodgerblue", color = "white", alpha = 0.7) +
  facet_wrap(~ Outcome, scales = "free") +
  scale_x_continuous(breaks = function(x) seq(floor(min(x)), ceiling(max(x)), by = 1), limits = c(1,9)) +
  theme_minimal() +
  labs(title = "Best fitting model across iterations", x = "Parameters", y = "Frequency")
```

Figure \@ref(fig:model-selection-self-esteem) shows what this cross-fold validation process looked like for the Rosenberg Self-Esteem Scale (one of our eight well-being outcome measures) for a single iteration. Here the dots represent the average of the RMSE across the five folds, and we can observe that the lowest RMSE was acheived in the model with four independent variables. Looking back at Figure \@ref(fig:measure-fit-comparisons) we can see that this was the modal model, although there were a number of iterations for which different models achieved best fit (e.g. five and six parameter models).

```{r model-selection-self-esteem, echo=FALSE, fig.cap="Average RMSE (using 5-fold cross validation) for models with various numbers of independent variables predicting the self-esteem outcome.", message=FALSE, warning=FALSE}

# Create plotting data frame
df_plot = results[results$Outcome == 'rosenberg_SES_score' & results$iteration == 8, ]
df_plot$Predictors = 1:9

# Find the minimum RMSE
min_rmse <- min(df_plot$RMSE)
num_predictors = df_plot[df_plot$RMSE == min(df_plot$RMSE), ]$Predictors

# Plot
ggplot(df_plot, aes(x = Predictors, y = RMSE, color = (RMSE == min_rmse))) +
  geom_line(aes(group = 1, color='black')) +
  geom_point(size = 3) +
  scale_x_continuous(breaks = seq(min(as.numeric(df_plot$Predictors)), 
                                  max(as.numeric(df_plot$Predictors)), by = 1)) +
  scale_color_manual(values = c("TRUE" = "red", "FALSE" = "black")) +
  labs(x = "Predictors", y = "RMSE", color = "Min RMSE") +
  theme_minimal()
```

In Table \@ref(tab:model-selection-table) we can see which variables are included in this `r num_predictors`-predictor model for self-esteem. Similarly, for all other outcome measures we present the model of best fit along with the standardized beta coefficient, averaged across all models, and the 95% confidence interval of the beta coefficient. A grey background indicates that the variable is not included in the selected model of best fit.

```{r model-selection-table, echo=FALSE, message=FALSE, warning=FALSE}
best_fits_zeros = best_fits

best_fits_zeros[is.na(best_fits_zeros)] <- 0

# Summary by outcome
summary_df = best_fits_zeros %>%
  group_by(Outcome) %>%
  summarise(across(c(-iteration, -var_num), list(mean = mean, 
                                    lower = ~t.test(.)$conf.int[1], 
                                    upper = ~t.test(.)$conf.int[2])),
            .groups = "drop")

# Summary for the entire dataset
total_summary <- best_fits_zeros %>%
  summarise(
    Outcome = "Total",
    across(
      c(-iteration, -Outcome, -var_num),
      list(
        mean = ~mean(.x, na.rm = TRUE),
        lower = ~ifelse(length(.x) > 1, t.test(.x)$conf.int[1], NA),
        upper = ~ifelse(length(.x) > 1, t.test(.x)$conf.int[2], NA)
      )
    )
  )

# Bind the rows
final_df <- rbind(summary_df, total_summary)

# # Calculate means and put in extra row
# # Replace NA values with 0
# df_means = best_fits
# df_means[is.na(df_means)] <- 0
# 
# # Compute column means
# column_means <- apply(abs(df_means[, 2:13]), 2, mean)
# column_means = c(Outcome = 'Means', column_means)
# 
# # Add as bottom row on table
# df_means = rbind(df_means, column_means)
# 
# # Convert all columns to numeric except the first one
# df_means[] <- lapply(1:ncol(df_means), function(i) {
#   if (i != 1) {
#     return(as.numeric(df_means[[i]]))
#   } else {
#     return(df_means[[i]])
#   }
# })

## Order best fits data frame by abs value of mean coef
# Extract values from the last row for columns 2 to the end
means_df = final_df %>%
  dplyr::select(ends_with('_mean'))
last_row_values <- abs(unlist(means_df[nrow(means_df), 4:ncol(means_df)]))

# Get order indices
order_indices <- order(last_row_values, decreasing = TRUE)

# Need to account for the fact that each predictor has 3 columns
order_indices = order_indices * 3 # mean
order_indices_lower = order_indices + 1 # lower CI
order_indices_upper = order_indices + 2 # upper CI

# Interleave these three vectors
interleaved <- c(matrix(c(order_indices, order_indices_lower, order_indices_upper), ncol=length(order_indices), byrow=TRUE))
# Make ordered index start from 1...
interleaved = interleaved -2

# Rearrange columns based on order indices, keeping the first 10 columns unchanged
final_df <- final_df[, c(1:10, interleaved + 10)]

# Round all numeric columns for table
df_means_ordered_round <- data.frame(lapply(final_df, function(x) {
  if (is.numeric(x)) {
    return(round(x, 3))
  } else {
    return(x)
  }
}))

# List of variables (unique prefixes)
variables <- c(
  "Intention.behavior.gap",
  "Self.control",
  "Conscientiousness", 
  "Social.desirability",
  "Future.perspective",    
  "Work.ethic",
  "Sensation.seeking", 
  "Grit",
  "Ambition"
)

# Generate the combined columns in a new dataframe
table_df <- data.frame(matrix(ncol = 0, nrow = nrow(df_means_ordered_round)))

for (var in variables) {
  mean_col <- paste0(var, "_mean")
  lower_col <- paste0(var, "_lower")
  upper_col <- paste0(var, "_upper")
  
  new_col_name <- paste0(var, "_combined")
  
  table_df[[new_col_name]] <- sprintf("%.2f [%.2f, %.2f]", df_means_ordered_round[[mean_col]], df_means_ordered_round[[lower_col]], df_means_ordered_round[[upper_col]])
}

# Rename outcomes for clarity *HARD CODED*
new_col_names = c(
  "Intention-behavior gap",
  "Self-control",
  "Conscientiousness",
  "Social desirability",
  "Future-perspective",
  "Work ethic",
  "Sensation-seeking",
  "Grit",
  "Ambition"                         
)

names(table_df) = new_col_names

# Select best models overall
# Find the mean predictor number of the best model for each outcome
mean_param_count = best_fits %>%
  group_by(Outcome) %>%
  summarise(meanParams = mean(var_num),
            modeParams = names(sort(table(var_num), decreasing = TRUE)[1]),
            modeParams_n = sort(table(var_num), decreasing = TRUE)[1],
            )

# DF of best fitting models
result_df <- mean_param_count %>%
  rowwise() %>%
  transmute(match_found = list(
    best_fits[which(best_fits$Outcome == .data$Outcome & best_fits$var_num == .data$modeParams)[1], ]
  )) %>%
  ungroup() %>%
  unnest(cols = c(match_found))

# Add superscript '*' to cells in table_df where corresponding cell in result_df is NA
table_df[1:8,] <- table_df[1:8,] %>% mutate(across(
  everything(), 
  list(~ifelse(is.na(result_df[,5:13][[cur_column()]]), paste0(., "*"), .)),
  .names = "{col}"
))

# Prepare prepended columns
prepend_cols = result_df[,1:4]
# Add row
prepend_cols[9,] = NA
prepend_cols[9,1] = 'MEANS'
prepend_cols[9,2] = colMeans(prepend_cols[1:8,2])
prepend_cols[9,3] = colMeans(prepend_cols[1:8,3])
prepend_cols[9,4] = colMeans(prepend_cols[1:8,4])
# round
prepend_cols[c('RMSE', 'R_2')] = round(prepend_cols[c('RMSE', 'R_2')], 2)

# Join to table_df
table_df = bind_cols(prepend_cols, table_df)

#-------#
# Table #
#-------#

# # Function to apply styling based on * in string
# color_star <- function(x) {
#   extra_css <- "padding-left:0px; padding-right:0px; border-radius:0px;"
#   
#   ifelse(grepl("\\*", x), 
#          cell_spec(x, background = "black", color = 'white', extra_css = extra_css), 
#          x)
# }
# 
# # Apply the function to each column in the dataframe
# df_styled <- data.frame(lapply(table_df, color_star))
# 
# # Rename rounded df columns
# names(df_styled) = names(table_df)
# 
# # Create table
# outcome_fits_table = knitr::kable(
#   df_styled,
#   format = "html",
#   table.attr = "class='table'",
#   booktabs = TRUE,
#   escape = FALSE,
#   caption = "Best model fits with standardized beta weights"
#   ) %>%
#   column_spec(1, bold = TRUE)  # Make the first column bold
# 
# kable_styling(outcome_fits_table,
#               full_width = TRUE,
#               bootstrap_options = c("striped", "condensed"))

# Kable formatting
# https://haozhu233.github.io/kableExtra/awesome_table_in_html.html

table_df %>%
  kbl(caption = "Predictor column cells show beta coefficients with 95% confidence intervals. The coefficients are the mean of the coefficient value across all iterations of the nested cross-validation process. Coloring corresponds to the magnitude of the beta coefficient. The cells with a '*' (colored grey) were not included in the final best fitting model.") %>%
  kable_paper(full_width = F) %>%
  column_spec(5, color = 'white',
              background = spec_color(ifelse(str_detect(table_df$`Intention-behavior gap`, "\\*$"), NA,
                                             abs(as.numeric(str_extract(table_df$`Intention-behavior gap`, "^[^\\s]+")))),
                                      option = 'D', end = 1, scale_from = c(0, .5))) %>%
  column_spec(6, color = "white",
              background = spec_color(ifelse(str_detect(table_df$`Self-control`, "\\*$"), NA,
                                             abs(as.numeric(str_extract(table_df$`Self-control`, "^[^\\s]+")))),
                                      option = 'D', end = 1, scale_from = c(0, .5))) %>%
  column_spec(7, color = "white",
              background = spec_color(ifelse(str_detect(table_df$Conscientiousness, "\\*$"), NA,
                                             abs(as.numeric(str_extract(table_df$Conscientiousness, "^[^\\s]+")))),
                                      option = 'D', end = 1, scale_from = c(0, .5))) %>%
  column_spec(8, color = "white",
              background = spec_color(ifelse(str_detect(table_df$`Social desirability`, "\\*$"), NA,
                                             abs(as.numeric(str_extract(table_df$`Social desirability`, "^[^\\s]+")))),
                                      option = 'D', end = 1, scale_from = c(0, .5))) %>%
  column_spec(9, color = "white",
              background = spec_color(ifelse(str_detect(table_df$`Future-perspective`, "\\*$"), NA,
                                             abs(as.numeric(str_extract(table_df$`Future-perspective`, "^[^\\s]+")))),
                                      option = 'D', end = 1, scale_from = c(0, .5))) %>%
  column_spec(10, color = "white",
              background = spec_color(ifelse(str_detect(table_df$`Work ethic`, "\\*$"), NA,
                                             abs(as.numeric(str_extract(table_df$`Work ethic`, "^[^\\s]+")))),
                                      option = 'D', end = 1, scale_from = c(0, .5))) %>%
  column_spec(11, color = "white",
              background = spec_color(ifelse(str_detect(table_df$`Sensation-seeking`, "\\*$"), NA,
                                             abs(as.numeric(str_extract(table_df$`Sensation-seeking`, "^[^\\s]+")))),
                                      option = 'D', end = 1, scale_from = c(0, .5))) %>%
  column_spec(12, color = "white",
              background = spec_color(ifelse(str_detect(table_df$Grit, "\\*$"), NA,
                                             abs(as.numeric(str_extract(table_df$Grit, "^[^\\s]+")))),
                                      option = 'D', end = 1, scale_from = c(0, .5))) %>%
  column_spec(13, color = "white",
              background = spec_color(ifelse(str_detect(table_df$Ambition, "\\*$"), NA,
                                             abs(as.numeric(str_extract(table_df$Ambition, "^[^\\s]+")))),
                                      option = 'D', end = 1, scale_from = c(0, .5)))

```

Table \@ref(tab:model-selection-table) also shows that the weighted Intention-Behavior gap measure was the only variable that was included in every model for all of our eight well-being outcome measures. The bottom row of the table shows the average absolute magnitude of each feature's standardized beta weights in the best-fitting models. This average was calculated using $0$ for a feature's beta weight if it was not included in the best fitting model. Again, the Intention-Behavior gap appears to have the highest average standardized regression coefficient ($\beta$) of .31, although the difference between this value and that of Self-control ($\beta_{Avg} = .27$) is not significant ($t(9.4) = 1.01, p = .34$) according to a Welch two sample t-test. The difference between the Intention-Behavior gap measure and average magnitude of the Conscientiousness coefficient ($\beta_{Avg} = .15$), however, is highly significant ($t(10.2) = 4.58, p < .001$), as are the differences with the rest of the predictor variables.

### Combined Outcome Variables Prediction
An alternative approach is to combine the eight outcome variables into a single value, and then to test how predictive each of our designated moderators and the Intention-Behavior gap measure are of this combined outcome value. We perform this amalgamation of the eight well-being outcome variables using principal components analysis. As shown in the scree plot (Figure \@ref(fig:pca-outcomes-moderators)) there is a pronounced "elbow" at the first component, indicating that the bulk of the variance is explained by this single component on its own.

```{r pca-outcomes-moderators, fig.cap = "Scree plot of cumulative variance explained with each additional principal component.", echo=FALSE, message=FALSE, warning=FALSE}
# load data
df_pca = read_csv('/Users/djw/Documents/pCloud_synced/Academics/Projects/2020_thesis/thesis_experiments/3_experiments/3_1_gap_measure/3_1_2_processed_data/weighted_gap.csv')

# select prediction and outcome columns 
predict_cols = c(
  'con_hex_score',
  'social_des_score',
  'ambition_score',
  'brief_self_control_score',
  'bsss_overall',
  'future_time_perspective_score',
  'grit_scale_score',
  'secular_measure_work_ethic_score',
  'model2_gap' # weighted gap
)

outcome_cols = c(
  'DASS_overall_Reversed',
  'flourishing_score',
  'harmony_score',
  'perceived_stress_Reversed',
  'rosenberg_SES_score',
  'qol_score',
  'sat_life_score',
  'sub_happy_score'
)

# Select relevant columns
df_pca = df_pca[ ,c(predict_cols, outcome_cols)]
# Weighted gap is actually "success", not gap, so reverse it
df_pca$model2_gap = df_pca$model2_gap * -1

df_pca_clean <- df_pca[complete.cases(df_pca[, outcome_cols]), ]

# Perform PCA on the first 8 columns
pca_result <- prcomp(df_pca_clean[outcome_cols], center = TRUE, scale. = TRUE)

# Extract the first principal components
df_pca_clean$PC1 = pca_result$x[, 1]

# Variance explained by PC1 (%)
pc1_var = round(summary(pca_result)$importance[2,1], 2) * 100


## SCREE PLOT
# Proportion of variance explained
variance_explained <- (pca_result$sdev^2) / sum(pca_result$sdev^2)

# Cumulative variance explained
cumulative_variance <- cumsum(variance_explained)

# Create a data frame for plotting
plot_data <- data.frame(
  Component = 1:length(variance_explained),
  VarianceExplained = variance_explained
)

# # Scree plot
# ggplot(plot_data, aes(x = Component, y = VarianceExplained)) +
#   geom_point(size = 3) +
#   geom_line(aes(group = 1)) +
#   labs(title = "Scree Plot", 
#        x = "Principal Component", 
#        y = "Proportion of Variance Explained") +
#   theme_minimal()

## SCREE PLOT CUMULATIVE
# Proportion of variance explained
variance_explained <- (pca_result$sdev^2) / sum(pca_result$sdev^2)

# Cumulative variance explained
cumulative_variance <- cumsum(variance_explained)

# Create a data frame for plotting and add a point at (0,0)
plot_data <- data.frame(
  Component = 0:length(cumulative_variance),
  CumulativeVariance = c(0, cumulative_variance)
)

# Scree plot
ggplot(plot_data, aes(x = Component, y = CumulativeVariance)) +
  geom_point(size = 3) +
  geom_line(aes(group = 1)) +
  expand_limits(y = c(0, 1)) +  # Ensure y-axis starts at 0 and ends at 1
  labs(x = "Principal Component", 
       y = "Cumulative Variance Explained") +
  theme_minimal()
```

The first component successfully accounts for over half (`r pc1_var`%) of the total variance of the outcome variables. Given that this single components seemed like a meaningful combination of our eight outcome variables we looked at the correlations between this first component and our moderator variables and the weighted Intention-Behavior gap measure (Figure \@ref(fig:pca-outcomes-moderators-plot)).

```{r pca-outcomes-moderators-plot, fig.cap="Correlation plots of the Intention-Behavior gap measure and moderator variables with the first principle compoment of all outcome measures.", echo=FALSE, message=FALSE, warning=FALSE}
library(broom)
df_plot = df_pca_clean[c(predict_cols, 'PC1')]

# Rename columns for clarity
df_plot_rename <- df_plot %>%
  rename(
    Conscientiousness = con_hex_score,
    `Social desirability` = social_des_score,
    Ambition = ambition_score,
    `Self-control` = brief_self_control_score,
    `Sensation-seeking` = bsss_overall,
    `Future-perspective` = future_time_perspective_score,
    Grit = grit_scale_score,
    `Work ethic` = secular_measure_work_ethic_score,
    `Intention-behavior gap` = model2_gap
  )


# Convert to long format
df_long <- df_plot_rename %>%
  pivot_longer(cols = -PC1, names_to = "variable", values_to = "value")

# Compute correlation and p-value for each variable
cor_data <- df_long %>%
  group_by(variable) %>%
  do(tidy(cor.test(.$PC1, .$value))) %>%
  mutate(label = sprintf("r = %.2f\np = %.3f", estimate, p.value))

# Plot
ggplot(df_long, aes(x = value, y = PC1)) +
  geom_point() +
  geom_smooth(method = "lm", se = FALSE) +
  geom_label(data = cor_data, aes(label = label, x = Inf, y = -Inf), 
             hjust = "right", vjust = "bottom", inherit.aes = FALSE) +
  labs(x = "Value", y="PC1") +
  facet_wrap(~variable, scales = "free") +
  theme_minimal()
```

```{r pc1-measures-corr ICCs, echo=FALSE, message=FALSE}
# Intention Behavior Gap
pc1_ibg_corTest = cor.test(df_plot_rename$PC1, df_plot_rename$`Intention-behavior gap`)
pc1_ibg_cor = round(pc1_ibg_corTest$estimate, 2)
pc1_ibg_df = pc1_ibg_corTest$parameter
pc1_ibg_95l = round(pc1_ibg_corTest$conf.int[1], 2)
pc1_ibg_95u = round(pc1_ibg_corTest$conf.int[2], 2)

# Self Control
pc1_sc_corTest = cor.test(df_plot_rename$PC1, df_plot_rename$`Self-control`)
pc1_sc_95l = round(pc1_sc_corTest$conf.int[1], 2)
pc1_sc_95u = round(pc1_sc_corTest$conf.int[2], 2)

# Grit
pc1_g_corTest = cor.test(df_plot_rename$PC1, df_plot_rename$Grit)
pc1_g_95l = round(pc1_g_corTest$conf.int[1], 2)
pc1_g_95u = round(pc1_g_corTest$conf.int[2], 2)

# Social Desirability
pc1_sd_corTest = cor.test(df_plot_rename$PC1, df_plot_rename$`Social desirability`)
pc1_sd_95l = round(pc1_sd_corTest$conf.int[1], 2)
pc1_sd_95u = round(pc1_sd_corTest$conf.int[2], 2)

```

The strongest correlation was between the well-being outcomes' first principle component and the weighted Intention-Behavior gap measure (r(`r pc1_ibg_df`) = `r pc1_ibg_cor`, 95% CI [`r pc1_ibg_95l`, `r pc1_ibg_95u`]). However, it was similar in magnitude and had an overlapping 95% confidence interval, in absolute terms, with self-control (95% CI [`r pc1_sc_95l`, `r pc1_sc_95u`]), grit (95% CI [`r pc1_g_95l`, `r pc1_g_95u`]) and social desirability (95% CI [`r pc1_sd_95l`, `r pc1_sd_95u`], all $p$s $< .001$). 

We calculated the best fitting model (using 10-fold cross validation) of the well-being outcomes' first principle component (Table \@ref(tab:pca-outcomes-moderators-model)). We again found the Intention-Behavior gap measure and self-control to have strong effects ($|\beta| \geq 0.37$) on well-being with moderate effects ($0.1 \leq |\beta| < 0.3$) of conscientiousness, social desirability, work ethic and future time perspective measures. To assess the potential for multicollinearity among the predictor variables, variance inflation factors (VIFs) were computed. All VIF values were below the commonly used threshold of 5 (all $< 1.81$ ), suggesting that multicollinearity is not a concern in the current model. Interestingly, grit, which correlated strongly on its own with the outcomes' first principle component did not have a significant unique contribution to the final model.


```{r pca-outcomes-moderators-model, echo=FALSE, message=FALSE, warning=FALSE}
data = data.frame(scale(df_plot))

# Store all results
results <- data.frame(
  RMSE = numeric(0),
  DF = numeric(0),
  R_2 = numeric(0),
  setNames(lapply(predict_cols, function(x) numeric(0)), predict_cols)  # Columns for each predictor
)

data = na.omit(data)

outcome = 'PC1'
 
# for each outcome, use regsubsets to find the best models
best_subsets <- regsubsets(as.formula(paste(outcome, "~ .")), data = data, nvmax = ncol(data) - 1)

# Extract the best models based on coefficients
best_models <- lapply(1:(ncol(data) - 1), function(num_predictors) {
  coefs <- coef(best_subsets, id = num_predictors)
  predictors <- names(coefs)[-1]  # exclude intercept
  formula_str <- paste(outcome, "~", paste(predictors, collapse = " + "))
  return(as.formula(formula_str))
})

# Set seed  
set.seed(42)
# Iterate over best models
for (formula_obj in best_models) {
  
  # 10-fold cross-validation
  ctrl <- trainControl(method = "cv", number = 10, savePredictions = TRUE)
  fit <- train(formula_obj, data = data, method = "lm", trControl = ctrl, metric = "RMSE")
  
  
  # Extracting the coefficients from the final linear model
  coefficients <- fit$finalModel$coefficients
  
  # Storing results
  temp_df <- data.frame(
    RMSE = fit$results$RMSE,
    DF = df.residual(fit$finalModel),
    R_2 = with(fit$finalModel, summary.lm(fit$finalModel)$r.squared),
    matrix(NA, ncol = length(predict_cols), nrow = 1, dimnames = list(NULL, predict_cols))
  )
  
  # Fill in the coefficients for the predictors in this model
  temp_df[1, names(coefficients)] <- coefficients
  
  # Add to the results data frames=
  results <- rbind(results, temp_df)
}

# Rename columns for clarity
results <- results %>%
  rename(
    Conscientiousness = con_hex_score,
    `Social desirability` = social_des_score,
    Ambition = ambition_score,
    `Self-control` = brief_self_control_score,
    `Sensation-seeking` = bsss_overall,
    `Future-perspective` = future_time_perspective_score,
    Grit = grit_scale_score,
    `Work ethic` = secular_measure_work_ethic_score,
    `Intention-behavior gap` = model2_gap
  )

# Select best models by lowest RMSE (1 for each iteration)
best_fits = results %>%
  filter(RMSE == min(RMSE)) # %>%
  #select(-`(Intercept)`) # remove intercept column

# Extract values from the row for columns 3 to 12 and compute order
order_indices <- order(unlist(abs(best_fits[1, 4:12])), decreasing = TRUE)
pc_fit = best_fits[, c(1:3, (order_indices + 3))]

# Create table
outcome_fits_table = knitr::kable(
  round(pc_fit, 2),
  format = "html",
  table.attr = "class='table'",
  booktabs = TRUE,
  escape = FALSE,
  caption = "Best model fit to outcome measures first principal component (standardized beta weights)"
  )

kable_styling(outcome_fits_table,
              full_width = TRUE,
              bootstrap_options = c("striped", "condensed"))

# Test VIF
library(car)

m = lm(PC1 ~ model2_gap + brief_self_control_score + con_hex_score + social_des_score + secular_measure_work_ethic_score + future_time_perspective_score, data = data)
vif_m = vif(m)
```

## The Gap and Individual Potential
Given that it is possible to have an Intention-Behavior gap even while working as hard and efficiently as one possibly can (e.g. just set impossibly ambitious goals), we wanted to confirm our intuition that this theoretically possible situation is an edge case rather than a general explanation of the Intention-Behavior gap. To test this we added a question at the end of the measure asking subjects to indicate to what degree they felt like they were currently fulfilling their potential abilities (0-100%). The idea being that if you are currently fulfilling 100% of your potential then any gap you may have between intentions and behavior would best be solved by just setting more realistic goals, since you are already at ceiling in terms of goal accomplishment. 


```{r gap-versus-potential, warning=FALSE, message=FALSE, echo=FALSE}
df$meta_gap_feelings_1 = as.numeric(df$meta_gap_feelings_1)

# calculate correlation
cor_result <- cor.test(df$domain_gap, as.numeric(df$meta_potential_4), use="pairwise.complete.obs")

# plot
ggplot(df, aes(x = domain_gap, y = as.numeric(meta_potential_4))) +
  geom_point() +
  geom_smooth(method = "lm") +
  #geom_label(aes(x = 82, y = 5.5, label = paste("r =", round(cor_result$estimate, 2), ", p =", format(round(cor_result$p.value, 6), scientific = TRUE))), color = "white", fill = "orangered", fontface= 'bold', size=6) +
  geom_label(aes(x = 82, y = 95, label = paste("r =", round(cor_result$estimate, 2), ", p =", format(round(cor_result$p.value, 2), scientific = TRUE))), color = "black") +
  xlab("Gap Magnitude") +
  ylab("Potential Fulfilled") +
  theme_minimal() +
  scale_colour_brewer(type = "div", palette = "Viridis")

```

As expected (Figure \@ref(fig:gap-versus-potential)), there was a strong negative association between gap magnitude and self-reported fulfillment of one's personal potential (r(`r cor_result$parameter`) = `r round(cor_result$estimate, 2)`, p < .001). We interpret this as strong support for the idea that the Intention-Behavior gap is not solely driven by ambitious goals, but also, at least in part, by behavior that is un-aligned with accomplishing one's goals.


# Exploratory Analyses
We had many additional questions that we wanted to shed light on with the data collected for the Intention-Behavior gap measure. To start we look at whether there is interpretable structure contained within the reported goals. Do goal domains group together in ways that we would expect? If not what might that suggest? We also collected general information from subjects about their general tendencies when it came to setting and persuing their goals. We were interested to see in what ways these characteristics might be predictive of the Intention-Behavior gap. For example, while we might expect having a strong ability to prioritize goals might reduce your Intention-Behavior gap, is this intuition actually supported by the data? This also connects to another question we had regarding goal conflict. It is clear that goal pursuit is often a zero-sum game. If you are engaged in your goal of playing sports you cannot simultaneously be attending to your goal of cleaning the apartment. Given that time is a finite resource these types of tensions can be expected to often arise between different goals. However, there are also cases in which you could imagine goals actually symbiotically reinforcing each other. For example your goal to cook your own meals might actually help with your goal to follow a particular diet you have set for yourself. We look into a number of these questions in the [Goal Conflict] section. Given our fairly modest test-retest values for our measure (see [Test-Retest Reliability]) we were curious whether some peolple might simple be more variable than others when it came to their Intention-Behavior gap. We tested to see if variability in daily reports of subjects' Intention-Behavior gap was associated with higher variability in their t1 and t2 gap measures. Finally, we looked briefly at a couple of different ways in which gender differences in goal domains manifested.

## Exploratory Factor Analysis

### Goal Domain Structure
Given that we have people reporting the specific life domains in which they have goals we can look to see whether there might be interesting structure in the data where certain goal domains might group together. The groupings could suggest that a set of domains have common characteristics. For example, if a subject has a goal in domain 'A', will they be likely to also have a goal in domain 'B'? To do this we create a correlation table of bi-serial correlations between each pair of goal domains. We can then use hierarchical agglomerative clustering to create groupings.

```{r correlation-goal-domains-goal, echo=FALSE, message=FALSE, warning=FALSE}
# If you have a goal in domain A do you tend to have a goal in domain B?
library(corrplot)
library(factoextra)
library(NbClust)
library(cluster)
library(dendextend)

#------#
# Data #
#------#

# Select domain goal responses
domain_goals = df %>%
  dplyr::select(starts_with('ib_domain_goal')) %>%
  mutate(across(where(is.character), as.numeric))

# Remove "ib_domain_goal_" from the column names
names(domain_goals) <- sub("^ib_domain_goal_", "", names(domain_goals))

# Remove rows that have only NA values
domain_goals <- domain_goals[apply(domain_goals, 1, function(row) any(!is.na(row))), ]

# Scale
# Not necessary for binary variable

# Create correlation matrix
cor_matrix <- cor(domain_goals, use = "pairwise.complete.obs")

# Compute the distance matrix
dist_matrix <- as.dist(1 - abs(cor_matrix))

# Cluster
hc = hclust(dist_matrix)

#--------------#
# Correlations #
#--------------#
plot_cor = cor_matrix
diag(plot_cor) <- 0

# Sig test
res1 <- cor.mtest(domain_goals, conf.level = .95)

# Plot matrix
corrplot(plot_cor, order = 'hclust', tl.col = 'black', diag = F, p.mat = res1$p, insig = 'blank', cl.ratio = 0.2,  col.lim=c(-0.6, 0.6), is.corr = F, addrect = 8, tl.cex = .8, col = COL2('RdYlBu'))

#-------------#
# Dendrograms #
#-------------#

df_labels <- data.frame(
  x = c(1.5, 4.5, 9, 15, 19.5, 23, 28, 32.5),
  y = c(.1, .2, .1, .2, .1, .2, .1, .2),
  text = c("active leisure", "passive leisure", "growth", "self admin", "health", "relationships", "future", "if there's time")
)

fviz_dend(hc, k = 8,                 # Cut in eight groups
          cex = 0.6,                 # label size
          #horiz = TRUE,
          #k_colors = c("#ff6961", "#ffb480", "#f8f38d", "#42d6a4", "#08cad1", "#59adf6", "#9d94ff", "#c780e8"),
          k_colors = c("lancet"),
          rect = TRUE,
          rect_border = "lancet", 
          rect_fill = TRUE,
          lower_rect = 0,
          type = 'rectangle',
          color_labels_by_k = TRUE,  # color labels by groups
          ggtheme = theme_dendro()     # Change theme
          ) + 
  scale_y_continuous(limits = c(-.43, 1)) +
  geom_label(aes(x=df_labels$x, y=df_labels$y, label=df_labels$text), vjust=0.5, size = 3, label.padding = unit(0.25, "lines")) 
  

fviz_dend(hc, k = 8,                 # Cut in eight groups
          cex = .8,                 # label size
          #horiz = TRUE,
          #k_colors = c("#ff6961", "#ffb480", "#f8f38d", "#42d6a4", "#08cad1", "#59adf6", "#9d94ff", "#c780e8"),
          k_colors = c("lancet"),
          rect = TRUE,
          rect_border = "lancet", 
          rect_fill = TRUE,
          lower_rect = 0,
          type = 'phylogenic',
          color_labels_by_k = TRUE,  # color labels by groups
          ggtheme = theme_dendro()     # Change theme
          ) +
  labs(title = "Unrooted Dendrogram")

# assign cluster to domain 
clust = cutree(hc, k = 8)
# 
# # visualize
# fviz_cluster(list(data = cor_matrix, cluster = clust))  ## from ‘factoextra’ package 

#------------#
# Clustering #
#------------#

# set.seed(42)
# # Gap Stat
# gap.stat <- clusGap(domain_goals, FUNcluster = kmeans, K.max = 15, verbose=FALSE)
# fviz_gap_stat(gap.stat)

# Test for optimal number of clusters for domain correlations
# Elbow method
fviz_nbclust(cor_matrix, kmeans, method = "wss") +
  labs(subtitle = "Elbow method")

# Silhouette method
fviz_nbclust(cor_matrix, kmeans, method = "silhouette")+
  labs(subtitle = "Silhouette method")
# 
# # NbClust - K-means
# nbClust = NbClust(data = domain_goals, min.nc = 2, max.nc = 15, method = 'kmeans')
# nbClust = as.data.frame(nbClust$Best.nc)
# 
# # Plot Hist
# # Convert the first row to a vector
# row_values <- unlist(nbClust[1, ])
# 
# # Plot
# ggplot(data.frame(Value=row_values), aes(x=Value)) +
#   geom_histogram(binwidth=1, color="black", fill="lightblue", position=position_dodge()) +
#   stat_count(aes(label=after_stat(count)), geom="text", vjust=-0.5) +
#   labs(x = "Clusters", y = "Frequency", title = "NbClust w/ kmeans method") +
#   scale_x_continuous(breaks = seq(min(row_values), max(row_values), by=1)) +
#   theme_minimal()
# 
# # NbClust - ward D
# nbClust = NbClust(data = domain_goals, min.nc = 2, max.nc = 15, method = 'ward.D')
# nbClust = as.data.frame(nbClust$Best.nc)
# 
# # Plot Hist
# # Convert the first row to a vector
# row_values <- unlist(nbClust[1, ])
# 
# # Plot
# ggplot(data.frame(Value=row_values), aes(x=Value)) +
#   geom_histogram(binwidth=1, color="black", fill="lightblue", position=position_dodge()) +
#   stat_count(aes(label=after_stat(count)), geom="text", vjust=-0.5) +
#   labs(x = "Clusters", y = "Frequency", title = "NbClust w/ ward.D method") +
#   scale_x_continuous(breaks = seq(min(row_values), max(row_values), by=1)) +
#   theme_minimal()
# 
# 
# ####
# 
# # Compute distance and perform hierarchical clustering
# dist_mat <- dist(domain_goals)
# hclust_result <- hclust(dist_mat)
# 
# # Cut dendrogram to get 3 clusters
# clusters <- cutree(hclust_result, k=8)
# 
# # Perform PCA
# pca_result <- prcomp(domain_goals)
# 
# data = as.data.frame(pca_result$x[,1:2])
# data$clusters = clusters
# data$clusters = as.factor(data$clusters)
# 
# ggplot(data, aes(x=PC1, y=PC2)) + 
#   geom_point(aes(color=clusters)) +  # coloring by PC1 value just as an example
#   theme_minimal() +
#   labs(title="PCA Plot", x="First Principal Component", y="Second Principal Component")
# 
# fviz_pca_ind(pca_result,
#              label = "none", # hide individual labels
#              habillage = clusters, # color by groups
#              #palette = c("#00AFBB", "#E7B800", "#FC4E07"),
#              addEllipses = TRUE # Concentration ellipses
#              )
# 
# fviz_pca_ind(pca_result,
#              label = "none", # hide individual labels
#              habillage = clusters, # color by groups
#              #palette = c("#00AFBB", "#E7B800", "#FC4E07"),
#              addEllipses = TRUE # Concentration ellipses
#              )
```

Using this methodology we decided on eight groups that seemed relatively interpretable: active and passive leisure, personal growth, self-administration, health, relationships, future, and "if there's time".

We can also use the presence or absence of goals in each domain as a means of grouping subjects. We use multiple correspondence analysis (MCA, close relation of PCA) since we are dealing with categorical data (had a goal in a domain or did not). 

```{r cluster-subjects-goal-domains, echo=FALSE, message=FALSE, warning=FALSE}
# If you have a goal in domain A do you tend to have a goal in domain B?

library(FactoMineR)

#------#
# Data #
#------#

# Select domain goal responses
domain_goals = df %>%
  dplyr::select(starts_with('ib_domain_goal')) %>%
  mutate(across(where(is.character), as.numeric))

# Remove "ib_domain_goal_" from the column names
names(domain_goals) <- sub("^ib_domain_goal_", "", names(domain_goals))

# Remove rows that have only NA values
domain_goals <- domain_goals[apply(domain_goals, 1, function(row) any(!is.na(row))), ]

data = as.data.frame(lapply(domain_goals, as.factor))
  
res.mca <- MCA(data, graph = F)

fviz_screeplot(res.mca, addlabels = TRUE, ylim = c(0, 28))

fviz_mca_var(res.mca, col.var = "cos2",
             gradient.cols = c("#00AFBB", "#E7B800", "#FC4E07"), 
             repel = TRUE, # Avoid text overlapping
             ggtheme = theme_minimal())

fviz_mca_biplot(res.mca, col.var = "cos2",
                gradient.cols = c("#00AFBB", "#E7B800", "#FC4E07"),
                repel = TRUE, # Avoid text overlapping (slow if many point)
                ggtheme = theme_minimal())

# fviz_cos2(res.mca, choice = "var", axes = 1:2)
# 
# # Contributions of rows to dimension 1
# fviz_contrib(res.mca, choice = "var", axes = 1, top = 15)
# # Contributions of rows to dimension 2
# fviz_contrib(res.mca, choice = "var", axes = 2, top = 15)

# Characteristics of quadrants
domain_goals = df %>%
  dplyr::select(starts_with('ib_domain_goal'), domain_gap, grades_avg, sub_happy_score) %>%
  mutate(across(where(is.character), as.numeric))

# Remove rows that have only NA values
domain_goals <- domain_goals[apply(domain_goals %>% select(starts_with("ib_domain_goal")), 1, function(row) any(!is.na(row))), ]

domain_goals$dim_1 = res.mca$ind$coord[,1]
domain_goals$dim_2 = res.mca$ind$coord[,2]
domain_goals$quadrant = 0
domain_goals[domain_goals$dim_1 < 0 & domain_goals$dim_2 > 0,]$quadrant = 1
domain_goals[domain_goals$dim_1 > 0 & domain_goals$dim_2 > 0,]$quadrant = 2
domain_goals[domain_goals$dim_1 > 0 & domain_goals$dim_2 < 0,]$quadrant = 3
domain_goals[domain_goals$dim_1 < 0 & domain_goals$dim_2 < 0,]$quadrant = 4

domain_goals %>%
  group_by(quadrant) %>%
  summarise(mean_gap = mean(domain_gap, na.rm=T),
            mean_grades = mean(grades_avg, na.rm = T),
            mean_happy = mean(sub_happy_score, na.rm = T),
            n = n() #,
            # sd_gap = sd(domain_gap, na.rm = TRUE),
            # se_gap = sd_gap / sqrt(n),
            # ci_lower = mean_gap - 1.96 * se_gap,
            # ci_upper = mean_gap + 1.96 * se_gap
            )
```

We did not find any significant differences between subjects in the four different quadrants, so this may not be a particularly meaningful grouping.

### Goal Domain Importance

We can do a similar analysis of goal domains based on importance, rather than presence/absence of a goal. For example, if domain A is of high importance does this correlate with domain B also being highly important?

```{r correlation-goal-domains-import, echo=FALSE, message=FALSE, warning=FALSE}
# If you have a goal in domain A do you tend to have a goal in domain B?
library(corrplot)
library(factoextra)
library(NbClust)
library(cluster)
library(dendextend)

#------#
# Data #
#------#

# Select domain goal responses
domain_goals = df %>%
  dplyr::select(starts_with('ib_domain_import')) %>%
  mutate(across(where(is.character), as.numeric))

# Remove "ib_domain_import_" from the column names
names(domain_goals) <- sub("^ib_domain_import_", "", names(domain_goals))

# Remove rows that have only NA values
domain_goals <- domain_goals[apply(domain_goals, 1, function(row) any(!is.na(row))), ]

# Replace remaining NAs with 0s
domain_goals[is.na(domain_goals)] = 0

# Scale
domain_goals = scale(domain_goals)

# Create correlation matrix
cor_matrix <- cor(domain_goals, use = "pairwise.complete.obs")

# Compute the distance matrix
dist_matrix <- as.dist(1 - abs(cor_matrix))

# Cluster
hc = hclust(dist_matrix)

#--------------#
# Correlations #
#--------------#
plot_cor = cor_matrix
diag(plot_cor) <- .65

# Sig test
res1 <- cor.mtest(domain_goals, conf.level = .95)

# Plot matrix
corrplot(plot_cor, order = 'hclust', tl.col = 'black', diag = F, p.mat = res1$p, insig = 'blank', cl.ratio = 0.2,  col.lim=c(-0.65, 0.65), is.corr = F, addrect = 9, tl.cex = .8, col = COL2('RdYlBu'))

#-------------#
# Dendrograms #
#-------------#

df_labels <- data.frame(
  x = c(2, 5, 9, 14.5, 20.5, 26.5, 32),
  y = c(.1, .2, .1, .2, .1, .2, .1),
  text = c("personal\n growth", "scheduling", "community", "life admin", "future", "relationships", "leisure")
)

fviz_dend(hc, k = 9,                 # Cut in eight groups
          cex = 0.6,                 # label size
          #horiz = TRUE,
          #k_colors = c("#ff6961", "#ffb480", "#f8f38d", "#42d6a4", "#08cad1", "#59adf6", "#9d94ff", "#c780e8"),
          k_colors = c("lancet"),
          rect = TRUE,
          rect_border = "lancet", 
          rect_fill = TRUE,
          lower_rect = 0,
          type = 'rectangle',
          color_labels_by_k = TRUE,  # color labels by groups
          ggtheme = theme_dendro()     # Change theme
          ) + 
  scale_y_continuous(limits = c(-.43, 1)) +
  geom_label(aes(x=df_labels$x, y=df_labels$y, label=df_labels$text), vjust=0.5, size = 3, label.padding = unit(0.25, "lines")) 
  

fviz_dend(hc, k = 8,                 # Cut in eight groups
          cex = .8,                 # label size
          #horiz = TRUE,
          #k_colors = c("#ff6961", "#ffb480", "#f8f38d", "#42d6a4", "#08cad1", "#59adf6", "#9d94ff", "#c780e8"),
          k_colors = c("lancet"),
          rect = TRUE,
          rect_border = "lancet", 
          rect_fill = TRUE,
          lower_rect = 0,
          type = 'phylogenic',
          color_labels_by_k = TRUE,  # color labels by groups
          ggtheme = theme_dendro()     # Change theme
          ) +
  labs(title = "Unrooted Dendrogram")

#------------#
# Clustering #
#------------#

# set.seed(42)
# # Gap Stat
# gap.stat <- clusGap(domain_goals, FUNcluster = kmeans, K.max = 15)
# fviz_gap_stat(gap.stat)

# Test for optimal number of clusters for domain correlations
# Elbow method
fviz_nbclust(cor_matrix, kmeans, method = "wss") +
  labs(subtitle = "Elbow method")

# Silhouette method
fviz_nbclust(cor_matrix, kmeans, method = "silhouette")+
  labs(subtitle = "Silhouette method")
# 
# # NbClust - K-means
# nbClust = NbClust(data = domain_goals, min.nc = 2, max.nc = 15, method = 'kmeans')
# nbClust = as.data.frame(nbClust$Best.nc)
# 
# # Plot Hist
# # Convert the first row to a vector
# row_values <- unlist(nbClust[1, ])
# 
# # Plot
# ggplot(data.frame(Value=row_values), aes(x=Value)) +
#   geom_histogram(binwidth=1, color="black", fill="lightblue", position=position_dodge()) +
#   stat_count(aes(label=after_stat(count)), geom="text", vjust=-0.5) +
#   labs(x = "Clusters", y = "Frequency", title = "NbClust w/ kmeans method") +
#   scale_x_continuous(breaks = seq(min(row_values), max(row_values), by=1)) +
#   theme_minimal()
# 
# # NbClust - ward D
# nbClust = NbClust(data = domain_goals, min.nc = 2, max.nc = 15, method = 'ward.D')
# nbClust = as.data.frame(nbClust$Best.nc)
# 
# # Plot Hist
# # Convert the first row to a vector
# row_values <- unlist(nbClust[1, ])
# 
# # Plot
# ggplot(data.frame(Value=row_values), aes(x=Value)) +
#   geom_histogram(binwidth=1, color="black", fill="lightblue", position=position_dodge()) +
#   stat_count(aes(label=after_stat(count)), geom="text", vjust=-0.5) +
#   labs(x = "Clusters", y = "Frequency", title = "NbClust w/ ward.D method") +
#   scale_x_continuous(breaks = seq(min(row_values), max(row_values), by=1)) +
#   theme_minimal()

# # Plot clusters 6
# final <- kmeans(cor_matrix, 6, nstart = 30)
# fviz_cluster(final, data = cor_matrix) + theme_minimal() + ggtitle("k = 6")
# 
# # Plot clusters 2
# final <- kmeans(dist_matrix, 2, nstart = 30)
# fviz_cluster(final, data = dist_matrix) + theme_minimal() + ggtitle("k = 2")
```

In this case we had a different grouping than for goal presence/absence. For example, instead of playing sports being grouped with video games, it is now clustered with "hobby" and "learning", a grouping we have labeled "personal growth". Work/School and Alcohol/Drug also form their own solo clusters in this grouping.

### Goal Domain Success

Finally, we conducted a similar analysis this time looking at domain success (the flip side of the gap). If you achieve success in domain A are you likely to also achieve success in domain B?

```{r correlation-goal-domains-success, echo=FALSE, message=FALSE, warning=FALSE}
#------#
# Data #
#------#

# Select domain goal responses
domain_goals = df %>%
  dplyr::select(starts_with('ib_domain_success')) %>%
  mutate(across(where(is.character), as.numeric))

# Remove "ib_domain_success_" from the column names
names(domain_goals) <- sub("^ib_domain_success_", "", names(domain_goals))

# Remove rows that have only NA values
domain_goals <- domain_goals[apply(domain_goals, 1, function(row) any(!is.na(row))), ]

# Scale
domain_goals = scale(domain_goals)

# Create correlation matrix
cor_matrix <- cor(domain_goals, use = "pairwise.complete.obs")

# Compute the distance matrix
dist_matrix <- as.dist(1 - abs(cor_matrix))

# Cluster
hc = hclust(dist_matrix)

#--------------#
# Correlations #
#--------------#
plot_cor = cor_matrix
diag(plot_cor) <- 0

# Sig test
res1 <- cor.mtest(domain_goals, conf.level = .95)

# Plot matrix
corrplot(plot_cor, order = 'hclust', tl.col = 'black', diag = F, p.mat = res1$p, insig = 'blank', cl.ratio = 0.2,  col.lim=c(-0.8, 0.8), is.corr = F, addrect = 9, tl.cex = .8, col = COL2('RdYlBu'))

#-------------#
# Dendrograms #
#-------------#

df_labels <- data.frame(
  x = c(2, 5, 9, 14.5, 20.5, 26.5, 32),
  y = c(.1, .2, .1, .2, .1, .2, .1),
  text = c("personal\n growth", "scheduling", "community", "life admin", "future", "relationships", "leisure")
)

fviz_dend(hc, k = 9,                 # Cut in eight groups
          cex = 0.6,                 # label size
          #horiz = TRUE,
          #k_colors = c("#ff6961", "#ffb480", "#f8f38d", "#42d6a4", "#08cad1", "#59adf6", "#9d94ff", "#c780e8"),
          k_colors = c("lancet"),
          rect = TRUE,
          rect_border = "lancet", 
          rect_fill = TRUE,
          lower_rect = 0,
          type = 'rectangle',
          color_labels_by_k = TRUE,  # color labels by groups
          ggtheme = theme_dendro()     # Change theme
          ) + 
  scale_y_continuous(limits = c(-.43, 1)) +
  geom_label(aes(x=df_labels$x, y=df_labels$y, label=df_labels$text), vjust=0.5, size = 3, label.padding = unit(0.25, "lines")) 
  

fviz_dend(hc, k = 8,                 # Cut in eight groups
          cex = .8,                 # label size
          #horiz = TRUE,
          #k_colors = c("#ff6961", "#ffb480", "#f8f38d", "#42d6a4", "#08cad1", "#59adf6", "#9d94ff", "#c780e8"),
          k_colors = c("lancet"),
          rect = TRUE,
          rect_border = "lancet", 
          rect_fill = TRUE,
          lower_rect = 0,
          type = 'phylogenic',
          color_labels_by_k = TRUE,  # color labels by groups
          ggtheme = theme_dendro()     # Change theme
          ) +
  labs(title = "Unrooted Dendrogram")

#------------#
# Clustering #
#------------#

# Test for optimal number of clusters for domain correlations
# Elbow method
fviz_nbclust(cor_matrix, kmeans, method = "wss") +
  labs(subtitle = "Elbow method")

# Silhouette method
fviz_nbclust(cor_matrix, kmeans, method = "silhouette")+
  labs(subtitle = "Silhouette method")

# # Plot
# ggplot(data.frame(Value=row_values), aes(x=Value)) +
#   geom_histogram(binwidth=1, color="black", fill="lightblue", position=position_dodge()) +
#   stat_count(aes(label=after_stat(count)), geom="text", vjust=-0.5) +
#   labs(x = "Clusters", y = "Frequency", title = "NbClust w/ kmeans method") +
#   scale_x_continuous(breaks = seq(min(row_values), max(row_values), by=1)) +
#   theme_minimal()
# 
# # NbClust - ward D
# nbClust = NbClust(data = domain_goals, min.nc = 2, max.nc = 15, method = 'ward.D')
# nbClust = as.data.frame(nbClust$Best.nc)
# 
# # Plot Hist
# # Convert the first row to a vector
# row_values <- unlist(nbClust[1, ])
# 
# # Plot
# ggplot(data.frame(Value=row_values), aes(x=Value)) +
#   geom_histogram(binwidth=1, color="black", fill="lightblue", position=position_dodge()) +
#   stat_count(aes(label=after_stat(count)), geom="text", vjust=-0.5) +
#   labs(x = "Clusters", y = "Frequency", title = "NbClust w/ ward.D method") +
#   scale_x_continuous(breaks = seq(min(row_values), max(row_values), by=1)) +
#   theme_minimal()

# # Plot clusters 6
# final <- kmeans(cor_matrix, 6, nstart = 30)
# fviz_cluster(final, data = cor_matrix) + theme_minimal() + ggtitle("k = 6")
# 
# # Plot clusters 2
# final <- kmeans(dist_matrix, 2, nstart = 30)
# fviz_cluster(final, data = dist_matrix) + theme_minimal() + ggtitle("k = 2")
```

## Goal Characteristics and the Intention-Behavior Gap

### Goal Pursuit Characteristics
We asked people a number of questions about their general goal setting style to try and gain some insight into which strategies or approaches might be more associated with the Intention-Behavior gap. These included:
- How detailed are your goals?
- How variable are your goals on a day to day basis?
- How many goals do you tend to have?
- How good are you at selecting a specific goal to pursue at any given time?
- How easy is it for you to focus on one specific goal?
- Do you tend to plan for contingencies with your goals?

See the plot below to observe how these correlate with the (unweighted) Intention-Behavior gap. 

```{r general-goal-style, warning=FALSE, message=FALSE, echo=FALSE}
data = dplyr::select(df, dplyr::starts_with('plan_'), domain_gap)

data = data %>%
  mutate_if(is.character, as.numeric)

chart.Correlation(data, histogram = TRUE, pch = 25)

m <- lm(domain_gap ~ plan_goal_select_1 + plan_goal_focus_1 + plan_goalnum_1 +
          plan_contingencies_1 + plan_variation_1 + plan_detailed_1,
        data = data)
summary(m)

summary(lm(domain_gap ~ plan_goal_select_1 + plan_goal_focus_1 +
          plan_contingencies_1,
        data = data))
```
If we include all of these characteristics in a model with the Intention-Behavior gap as the dependent variable, we observe that only the ability to prioritize goals for pursuit, the ability to focus on a single goal at a time, and the tendency to plan for contingencies remain significant predictors of the gap (ps < .05).

### Number of goals

While we were not able to check on each of these goal setting style attributes in our measure data, we were able to at least test whether the number of goals was, in fact, not significantly related to the gap given that we had reports of both quantities for our participants. This was indeed the case with a non-significant relationship (r(286) = .06, p = .31).

```{r goalCount-gap-correlation, warning=FALSE, message=FALSE, echo=FALSE}

x = dplyr::select(df, contains('ib_domain_success'))

# Calculate the number of non-NA domain goal values for the domains
non_na_counts <- data.frame(counts = rowSums(!is.na(x)))
# Add gap
non_na_counts$gap = df$domain_gap
# Remove rows with missing data
non_na_counts = non_na_counts[non_na_counts$counts>0,]

# Calculate correlation coefficient and p-value
cor_result <- cor.test(non_na_counts$counts, non_na_counts$gap)

#plot
ggplot(non_na_counts, aes(x = counts, y = gap)) +
  geom_point(size = 3, color = "skyblue3") +
  geom_smooth(method = "lm", se = FALSE, color = "red3", size = 1) +
  theme_minimal() + 
  geom_label(aes(x = max(counts), y = max(gap), label = paste("r =", round(cor_result$estimate, 2), ", p =", format(round(cor_result$p.value, 2), scientific = FALSE))),
            hjust = 1, vjust = 1, color = "black")

```

### Goal importance
Does having on average more important goals mean you are more likely to accomplish them?

```{r goalImportAvg-gap-correlation, warning=FALSE, message=FALSE, echo=FALSE}

x = dplyr::select(df, contains('ib_domain_import'))

# Convert to numeric
x = as.data.frame(lapply(x, as.numeric))

# Calculate the mean of non-NA domain goal values for each subject
non_na_counts <- data.frame(import_means = rowMeans(x, na.rm = TRUE))
# Add gap
non_na_counts$gap = df$domain_gap
# Remove rows with missing data
non_na_counts = non_na_counts[!is.na(non_na_counts$import_means),]

# Calculate correlation coefficient and p-value
cor_result <- cor.test(non_na_counts$import_means, non_na_counts$gap)

#plot
ggplot(non_na_counts, aes(x = import_means, y = gap)) +
  geom_point(size = 3, color = "skyblue3") +
  geom_smooth(method = "lm", se = FALSE, color = "red3", size = 1) +
  geom_label(aes(x = max(import_means), y = max(gap), label = paste("r =", round(cor_result$estimate, 2), ", p =", format(round(cor_result$p.value, 2), scientific = FALSE))),
            hjust = 1, vjust = 1, color = "black") +
  labs(x = 'Avg. domain importance', y = 'IB gap') +
  theme_minimal()

```


## Domain Importance for Outcomes
Using the first principle component of our eight well-being outcome measures, do certain goal domains have a higher association with well-being than others?

```{r pca-outcomes-domains, echo=FALSE, message=FALSE, warning=FALSE}
# load data
df_pca = read_csv('/Users/djw/Documents/pCloud_synced/Academics/Projects/2020_thesis/thesis_experiments/3_experiments/3_1_gap_measure/3_1_2_processed_data/weighted_gap.csv')

# select prediction and outcome columns 
predict_cols = c(
  'con_hex_score',
  'social_des_score',
  'ambition_score',
  'brief_self_control_score',
  'bsss_overall',
  'future_time_perspective_score',
  'grit_scale_score',
  'secular_measure_work_ethic_score',
  'model2_gap' # weighted gap
)

outcome_cols = c(
  'DASS_overall_Reversed',
  'flourishing_score',
  'harmony_score',
  'perceived_stress_Reversed',
  'rosenberg_SES_score',
  'qol_score',
  'sat_life_score',
  'sub_happy_score'
)

# Select relevant columns
df_pca = df_pca[ ,c(predict_cols, outcome_cols)]
# Weighted gap is actually "success", not gap, so reverse it
df_pca$model2_gap = df_pca$model2_gap * -1

# Add in domain success
x = dplyr::select(df, contains('ib_domain_success'))

# Convert to numeric
x = as.data.frame(lapply(x, as.numeric))

# Add gap
x$gap = df$domain_gap
# Remove rows with missing data
x = x[!is.na(x$gap),]
# remove gap
x$gap <- NULL

# Add domain cols to pca df
data = bind_cols(x, df_pca)
  
data <- data[complete.cases(data[, outcome_cols]), ]

# Perform PCA on the first 8 columns
pca_result <- prcomp(df_pca_clean[outcome_cols], center = TRUE, scale. = TRUE)

# Extract the first principal components
data$PC1 = pca_result$x[, 1]

# Calculate correlations
# Identify columns that start with the string 'ib_domain_success'
cols_to_correlate <- grep("^ib_domain_success", names(data), value = TRUE)

# Compute correlations with PC1 using pairwise complete method
correlations <- sapply(data[, cols_to_correlate], function(col) {
  cor(data$PC1, col, use = "pairwise.complete.obs")
})

# Convert to a named vector for clarity
correlations <- setNames(correlations, cols_to_correlate)

# Convert the correlations vector to a data frame for ggplot
cor_df <- data.frame(
  Variable = names(correlations),
  Correlation = correlations
)

# Remove extra text in Variable name
cor_df$Variable <- gsub("^ib_domain_success_", "", cor_df$Variable)

# Sort correlations from largest to smallest
cor_df_sorted <- cor_df %>% arrange(desc(Correlation))

# Convert the 'Variable' column to a factor with the order from the sorted dataframe
cor_df_sorted$Variable <- factor(cor_df_sorted$Variable, levels = cor_df_sorted$Variable)

# Plot the sorted correlations
ggplot(cor_df_sorted, aes(x = Variable, y = Correlation, fill = Correlation)) +
  geom_bar(stat = "identity") +
  scale_fill_viridis() +
  coord_flip() + 
  theme_minimal() +
  labs(title = "Correlation of outcomes' PC1 with domain success",
       y = "Correlation coefficient",
       x = "Variable")
```

## Goal Conflict
Given that it is often the case that goal pursuit is a zero-sum scenario in which time spent pursuing goal A is taking away from time available to pursue goal B, it is clear that conflict between and among goals can arise.

We attempted to quantify this conflict for our subjects by taking their top seven goal domains, and then for each of these domains specifying whether their goal in the selected domain promoted or conflicted with their goals in each of the other six domains (or was independent). 

### Goal Conflict and the Intention-Behavior Gap
We first tested our basic intuition that higher average goal-conflict level for a subject would predict a larger Intention-Behavior Gap measure.

```{r goalConflict-gap, warning=FALSE, message=FALSE, echo=FALSE}

#------------#
# Data Munge #
#------------#
library(interactions)
library(stargazer)

x = dplyr::select(df, contains('goal_interact'))

# Convert to numeric
x = as.data.frame(lapply(x, as.numeric))

# Calculate the mean of non-NA domain goal values for each subject
conflict_means <- data.frame(conflict_means = rowMeans(x, na.rm = TRUE))

# Add gap
conflict_means$gap = df$domain_gap
# Add goal selection and goal prioritization ability
conflict_means$goal_prioritization = as.numeric(df$plan_goal_select_1)
conflict_means$goal_focus = as.numeric(df$plan_goal_focus_1)

# Add Outcomes data for PCA
outcome_cols = c(
  'DASS_overall',
  'flourishing_score',
  'harmony_score',
  'perceived_stress_score',
  'rosenberg_SES_score',
  'qol_score',
  'sat_life_score',
  'sub_happy_score'
)

# Select relevant columns
df_outcomes = df[ ,c(outcome_cols)]

# Reverse negative well being outcomes
df_outcomes$DASS_overall_Reversed = 3 - df_outcomes$DASS_overall
df_outcomes$perceived_stress_Reversed = 6 - df_outcomes$perceived_stress_score

df_outcomes = df_outcomes %>%
  select(-c(DASS_overall, perceived_stress_score))

# Update outcome cols
outcome_cols = c(
  'DASS_overall_Reversed',
  'flourishing_score',
  'harmony_score',
  'perceived_stress_Reversed',
  'rosenberg_SES_score',
  'qol_score',
  'sat_life_score',
  'sub_happy_score'
)

# Add domain cols to pca df
data = bind_cols(conflict_means, df_outcomes)
  
data <- data[complete.cases(data[, outcome_cols]), ]

# Perform PCA on the first 8 columns
pca_result <- prcomp(data[outcome_cols], center = TRUE, scale. = TRUE)

# Extract the first principal components
data$PC1 = pca_result$x[, 1]

# Remove rows with missing data
data = data[!is.na(data$conflict_means),]

#-----------------#
# CONFLICT VS GAP #
#-----------------#
# Calculate correlation coefficient and p-value
cor_result_con <- cor.test(data$conflict_means, data$gap)

#plot
ggplot(data, aes(x = conflict_means, y = gap)) +
  geom_point(size = 2) +
  geom_smooth(method = "lm", color = "skyblue", size = 1) +
  geom_label(aes(x = max(conflict_means), y = max(gap), label = paste("r =", round(cor_result_con$estimate, 2), ", p =", format(round(cor_result_con$p.value, 3), scientific = FALSE))),
            hjust = 1, vjust = 1, color = "black") +
  labs(x = 'Avg. goal conflict', y = 'IB gap', title = 'Goal Conflict vs. Intention-Behavior Gap') +
  theme_minimal()
```

We found this to be the case with a positive correlation of r(246) = .25 (p < .001). Given that we had self report information on how well subjects could prioritize and focus on goals, we were also intersted to test whether these goal strategies might moderate the relationship between goal conflict and the intention behavior gap.

```{r stargazer-conflict-focus-gap, warning=FALSE, message=FALSE, echo=FALSE}
# Stargazer to visualize results
base_m <- lm(gap ~ conflict_means + goal_focus, data)
interact_m <- lm(gap ~ conflict_means * goal_focus, data)

stargazer(base_m, interact_m,
          type="text", 
          column.labels = c("Main Effects", "Interaction"),
          report=("vc*p"),
          star.cutoffs = c(0.05, 0.01, 0.001),
          intercept.bottom = FALSE, 
          single.row=FALSE,     
          notes.append = FALSE, 
          header=FALSE) 

#-----------------------------#
# CONFLICT INTERACTION VS GAP #
#-----------------------------#
# conflict x prioritization
data$interact_conflict_focus = data$conflict_means * data$goal_focus

# Calculate correlation coefficient and p-value
cor_result_conXf <- cor.test(data$interact_conflict_focus, data$gap)

# #plot
# ggplot(data, aes(x = interact_conflict_focus, y = gap)) +
#   geom_point(size = 2) +
#   geom_smooth(method = "lm", color = "skyblue", size = 1) +
#   geom_label(aes(x = max(interact_conflict_focus), y = max(gap), label = paste("r =", round(cor_result_conXf$estimate, 2), ", p =", format(round(cor_result_conXf$p.value, 2), scientific = FALSE))),
#             hjust = 1, vjust = 1, color = "black") +
#   labs(x = 'Avg. goal conflict x goal focus', y = 'IB gap', title = 'Goal Conflict x Goal Focus Ability vs. Intention-Behavior Gap') +
#   theme_minimal()

m <- lm(gap ~ conflict_means * goal_focus, data)

interact_plot(m, "conflict_means", "goal_focus", plot.points = TRUE, interval = T,
              rug = T, vary.lty = F, colors = 'Dark2', facet.modx = F,
              main.title = 'Interaction: goal focus x goal conflict'
              )
```
This was, in fact, what we found. If we included an interaction between a subject's ability to focus on a specific goal and their average goal conflict level, then this relationship between the magnitude of conflict and magnitude of gap is reduced. We tested this also for goal prioritization but did not find the same relationship.

```{r conflict-prioritization, warning=FALSE, message=FALSE, echo=FALSE}

base_m <- lm(gap ~ conflict_means + goal_prioritization, data)
interact_m <- lm(gap ~ conflict_means * goal_prioritization, data)

stargazer(base_m, interact_m,
          type="text", 
          column.labels = c("Main Effects", "Interaction"),
          report=("vc*p"),
          star.cutoffs = c(0.05, 0.01, 0.001),
          intercept.bottom = FALSE, 
          single.row=FALSE,     
          notes.append = FALSE, 
          header=FALSE) 

# conflict x prioritization
data$interact_conflict_priority = data$conflict_means * data$goal_prioritization

# Calculate correlation coefficient and p-value
cor_result_conXp <- cor.test(data$interact_conflict_priority, data$gap)

# #plot
# ggplot(data, aes(x = interact_conflict_priority, y = gap)) +
#   geom_point(size = 2) +
#   geom_smooth(method = "lm", color = "skyblue", size = 1) +
#   geom_label(aes(x = max(interact_conflict_priority), y = max(gap), label = paste("r =", round(cor_result_conXp$estimate, 2), ", p =", format(round(cor_result_conXp$p.value, 2), scientific = FALSE))),
#             hjust = 1, vjust = 1, color = "black") +
#   labs(x = 'Avg. goal conflict x goal prioritization', y = 'IB gap', title = 'Goal Conflict x Goal Prioritization Ability vs. Intention-Behavior Gap') +
#   theme_minimal()

m <- lm(gap ~ conflict_means * goal_prioritization, data)

interact_plot(m, "conflict_means", "goal_prioritization", plot.points = TRUE, interval = T,
              rug = T, vary.lty = F, colors = 'Dark2', facet.modx = F,
              main.title = 'Interaction: goal prioritization x goal conflict'
              )
```

In this case it appeared that there was no interaction at all, with goal prioritization and goal conflict having independent effects on the gap.

### Goal Conflict and Well-being
We were also interested in whether there would be a negative relationship between average goal conflict and well-being. Our intuition was that having greater goal conflict in life might lead to a reduction in well-being (as quantified by the first principal component of our eight well-being measures).

```{r goalConflict-wellBeing, warning=FALSE, message=FALSE, echo=FALSE}
# CONFLICT VS WELL-BEING

# Calculate correlation coefficient and p-value
cor_result <- cor.test(data$conflict_means, data$PC1)

#plot
ggplot(data, aes(x = conflict_means, y = PC1)) +
  geom_point(size = 2) +
  geom_smooth(method = "lm", color = "skyblue", size = 1) +
  geom_label(aes(x = max(conflict_means), y = max(PC1), label = paste("r =", round(cor_result$estimate, 2), ", p =", format(round(cor_result$p.value, 2), scientific = FALSE))),
            hjust = 1, vjust = 1, color = "black") +
  labs(x = 'Avg. goal conflict', y = 'Well-Being', title = "Goal Conflict vs. Well-Being") +
  theme_minimal()

```

While the relationship was negative, it was not significant (r(`r cor_result$parameter`) = `r round(cor_result$estimate, 2)`, p = `r round(cor_result$p.value, 2)`).

### Average Goal Conflict by Domain
Finally, we looked at the average level of conflict by domain. Perhaps unsurprisingly watching television (streaming) and social media were the two domains that were highest in conflict with all other domains.

```{r average-conflict-by-domain, warning=FALSE, message=FALSE, echo=FALSE}

x = dplyr::select(df, contains('goal_interact'))

# Convert to numeric
x = as.data.frame(lapply(x, as.numeric))

# Results DF
results <- data.frame(
  domain = integer(),
  mean = numeric(),
  sd = numeric(),
  count = integer()
)

# Loop over the numbers
for (i in 1:34) {
  cols <- grep(paste0("_", i, "$"), names(x), value = TRUE)
  
  # Remove columns starting with Xi
  cols <- setdiff(cols, grep(paste0("^X", i), names(x), value = TRUE))
  
  if (length(cols) > 0) {
    all_values <- unlist(x[, cols, drop = FALSE])
    current_mean <- mean(all_values, na.rm = TRUE)
    current_sd <- sd(all_values, na.rm = TRUE)
    current_count <- sum(!is.na(all_values))
    
    results <- rbind(results, c(i, current_mean, current_sd, current_count))
  }
}

# Rename cols
names(results) = c('domain', 'mean', 'sd', 'count')

# Rename domains
domain_names = c("Diet",
                 "Exercise",
                 "MentalPersonal_Health",
                 "Medical_Health",
                 "Sleep",
                 "Alcohol_drug",
                 "Online",
                 "Phone",
                 "Video games",
                 "Reading_leisure",
                 "SocialMedia",
                 "Sports_playing",
                 "TV_Streaming",
                 "Family",
                 "Friends",
                 "Partner",
                 "Social_life",
                 "Hobby",
                 "Housework",
                 "Cooking",
                 "Work_School",
                 "Environment",
                 "Culture",
                 "Learning",
                 "Self-Improvement",
                 "Volunteering",
                 "Community involvement",
                 "Admin",
                 "Future_Planning",
                 "Finances",
                 "Time_Management",
                 "Punctuality",
                 "Personal_Values",
                 "Other"
  )

results$domain <- domain_names

# Sort
results <- results[order(results$mean), ]
results$domain <- factor(results$domain, levels = results$domain)

# Calculate SE
results$se <- results$sd / sqrt(results$count)

ggplot(results, aes(x = domain, y = mean, fill = mean)) +
  geom_bar(stat = "identity", show.legend = FALSE) +
  geom_pointrange(aes(x=domain, ymin=mean-se, ymax=mean+se), size=.08, show.legend = FALSE) +
  scale_fill_viridis() +
  coord_flip() + 
  theme_minimal() +
  labs(title = "Average conflict of domain with all other domains",
       y = "Conflict",
       x = "Domain")

```

## Trait vs State Stability
Given that we have modest test-retest reliability it is clear that subjects' self-reported Intention-Behavior gap often differed between the two timepoints where measurements were collected. We were interested to see if the magnitude of a subject's change might be associated with their self-reported gap stability in the daily gap state measurements we collected. Our intuition was that someone who had higher day to day variability, as assessed via the standard deviation of their score, might tend to show larger magnitude changes in their trait level measure from collection timepoint one to two.

```{r trait-vs-state-variability, warning=FALSE, message=FALSE, echo=FALSE}
# import daily gaps
daily_gaps = read_csv('/Users/djw/Documents/pCloud_synced/Academics/Projects/2020_thesis/thesis_experiments/3_experiments/3_3_experience_sampling/3_3_2_processed_data/run_2/run2_selfReport.csv')

daily_gaps = daily_gaps[c('ParticipantIdentifier', 'DAILY_past24_gap', 'DAILY_goal1_report', 'DAILY_goal2_report')]
# convert success to gap
daily_gaps$DAILY_goal1_report = 100 - daily_gaps$DAILY_goal1_report
daily_gaps$DAILY_goal2_report = 100 - daily_gaps$DAILY_goal2_report

daily_gaps$combined_gap = ((3/4) * daily_gaps$DAILY_past24_gap) + ((1/8) * daily_gaps$DAILY_goal1_report) + ((1/8) * daily_gaps$DAILY_goal2_report)

daily_gaps_agg = daily_gaps %>% 
    group_by(ParticipantIdentifier) %>% 
    summarise(mean_daily_gap = mean(DAILY_past24_gap, na.rm = T),
              mean_daily_gap_combo = mean(combined_gap, na.rm = T),
              sd_daily_gap_combo = sd(combined_gap, na.rm = T),
              n = n()
              )

# summary stats of daily data observations collection frequency
daily_n_m = describe(daily_gaps_agg$n)$mean
daily_n_sd = describe(daily_gaps_agg$n)$sd

# merge daily gaps with test-retest df
trt_daily = merge(test_retest_all_gap, daily_gaps_agg, by = 'ParticipantIdentifier')
# create mean of onboarding offboarding measures
trt_daily$mean_measure = (trt_daily$gap_t1 + trt_daily$gap_t2) / 2
# create absolute diff of onboarding offboarding measures
trt_daily$abs_diff_measure = abs(trt_daily$gap_t1 - trt_daily$gap_t2)

cor_result <- cor.test(trt_daily$abs_diff_measure, trt_daily$sd_daily_gap_combo)

ggplot(trt_daily, aes(x = abs_diff_measure, y = sd_daily_gap_combo)) +
  geom_point(size = 2, color = "skyblue3") +
  geom_smooth(method = "lm", se = TRUE, color = "red3", size = 1) +
  geom_label(aes(x = max(abs_diff_measure), y = max(sd_daily_gap_combo), label = paste("r =", round(cor_result$estimate, 2), ", p =", format(round(cor_result$p.value, 2), scientific = FALSE))),
            hjust = 1, vjust = 1, color = "black") +
  labs(x = 'Measure Difference (abs(T1-T2))', y = 'Daily Gap Variability (SD)', title = "Trait vs. State Stability") +
  theme_minimal()
```

We correlated the magnitude of the Intention-Behavior gap difference between timepoint one and two, and the state-level (daily) Intention-Behavior gap measure variability (SD). We did not find a significant correlation ($r(76) = -.06, p - .6$).


## Gender Differences
We also noticed some interesting and thought-provoking trends when we separated subjects be gender. First, the frequency with which different domain goals were selected varied substantially, though not for all domains. The domains for which males had the largest difference in terms of the frequency with which they selected goals as compared to females included playing sports, video games, exercise and sleep, whereas for females the domains in which they were more frequently reporting goals as compared to males included culture, environment, reading for leisure and family. 

``` {r gendered_goals, warning=FALSE, message=FALSE, echo=FALSE}
x = dplyr::select(df, contains('ib_domain_success'), gender)
# Count the non NA values in each column

non_na_counts <- colSums(!is.na(x[x$gender == '1',]))
dom_count_m <- data.frame(Domain = names(non_na_counts), Frequency = non_na_counts)

non_na_counts <- colSums(!is.na(x[x$gender == '2',]))
dom_count_f <- data.frame(Domain = names(non_na_counts), Frequency = non_na_counts)

# clean up names
specified_string = 'ib_domain_success_'
dom_count_f$Domain = sub(paste0("^", specified_string), "", dom_count_f$Domain)
dom_count_m$Domain = sub(paste0("^", specified_string), "", dom_count_m$Domain)

# convert to percentage
dom_count_f$Frequency = (dom_count_f$Frequency / dom_count_f[dom_count_f$Domain == 'gender',]$Frequency) * 100
dom_count_m$Frequency = (dom_count_m$Frequency / dom_count_m[dom_count_m$Domain == 'gender',]$Frequency) * 100

# add gender column
dom_count_f$gender = 'Female'
dom_count_m$gender = 'Male'

# remove gender row
dom_count_f = dom_count_f[dom_count_f$Domain!='gender',]
dom_count_m = dom_count_m[dom_count_m$Domain!='gender',]

# factorize
dom_count_f$Domain = as.factor(dom_count_f$Domain)
dom_count_f$gender = as.factor(dom_count_f$gender)
dom_count_m$Domain = as.factor(dom_count_m$Domain)
dom_count_m$gender = as.factor(dom_count_m$gender)

# diff
dom_count_m$diff = dom_count_m$Frequency - dom_count_f$Frequency
dom_count_f$diff = dom_count_m$Frequency - dom_count_f$Frequency
# reorder
dom_count_m = dom_count_m[order(dom_count_m$diff), ]
dom_count_f = dom_count_f[order(dom_count_f$diff), ]
# factor
dom_count_m$Domain = factor(dom_count_m$Domain, levels = dom_count_m$Domain)
dom_count_f$Domain = factor(dom_count_f$Domain, levels = dom_count_m$Domain)

# create single df for plot
mf = bind_rows(dom_count_f, dom_count_m)
mf$Domain = as.factor(mf$Domain)
mf$Domain = factor(mf$Domain, levels = dom_count_m$Domain)

p <- ggplot(mf)+
  
 geom_segment(data = dom_count_m,
              aes(x = Frequency, y = Domain,
                  yend = dom_count_f$Domain, xend = dom_count_f$Frequency), #use the $ operator to fetch data from our "Females" tibble
              color = "#aeb6bf",
              linewidth = 4.5, #Note that I sized the segment to fit the points
              alpha = .5) +
  
  geom_point(aes(x = Frequency, y = Domain, color = gender), size = 4, show.legend = TRUE)+
  theme_minimal() +
  ggtitle("Female vs. Male Goal Frequency") +
  ylab("")

p
```
We also found a notable trend in which males tended to overestimate their future academic success much more than females. 

```{r gender-grade-goals-actual, warning=FALSE, message=FALSE, echo=FALSE}
###############################
# goal vs actual male/female. #
###############################

# Plot v2
mf = df %>%
  group_by(gender) %>%
  summarise(mean_goal = mean(gradeGoalAvg, na.rm = T),
            mean_predict = mean(gradePredictAvg, na.rm = T),
            mean_grade = mean(grades_avg, na.rm = T),
            se_goal = sd(gradeGoalAvg, na.rm = T) / sqrt(sum(!is.na(df$gradeGoalAvg))),
            se_predict = sd(gradePredictAvg, na.rm = T) / sqrt(sum(!is.na(df$gradePredictAvg))),
            se_grade = sd(grades_avg, na.rm = T) / sqrt(sum(!is.na(df$grades_avg))),
            n = n()
            ) %>%
  as_tibble() %>%
  t() %>%
  as.data.frame()

names(mf) = c('male', 'female', 'other', 'na')

data4t = mf
data4t = mutate_all(data4t, function(x) as.numeric(as.character(x)))

mf = mf[2:7, 1:2]
mf = mutate_all(mf, function(x) as.numeric(as.character(x)))

mf$category = c('goal', 'predicted', 'actual', 'goal', 'predicted', 'actual')

# Split the dataframe based on "mean_" and "se_" prefixes
df_mean <- mf %>% filter(grepl("mean_", rownames(mf)))
df_se <- mf %>% filter(grepl("se_", rownames(mf)))

# Melt each dataframe
df_mean_long <- df_mean %>% pivot_longer(cols = c(male, female), names_to = "gender", values_to = "grade")
df_se_long <- df_se %>% pivot_longer(cols = c(male, female), names_to = "gender", values_to = "se")

# Combine them together
df_final <- df_mean_long %>%
  left_join(df_se_long, by = c("category", "gender"))

df_final$category = as.factor(df_final$category)
df_final$category = factor(df_final$category, levels = c("goal", "predicted", "actual"))

# Line Plot
ggplot(df_final, aes(x=category, y=grade, color=gender, group=gender)) +
  geom_line(size=1) +
  geom_point(size=3) +
  geom_errorbar(aes(ymin = grade - se, ymax = grade + se), 
                width = 0.25) +
  labs(y="Value", x="Condition") +
  scale_y_continuous(breaks = seq(70,86, by = 2)) +
  theme_minimal()

# T vals: PREDICT
predict_t = t.test(df[df$gender==1,]$gradePredictAvg, df[df$gender==2,]$gradePredictAvg)
sd_m_predict = round(sd(df[df$gender==1,]$gradePredictAv, na.rm = T), 2)
sd_f_predict = round(sd(df[df$gender==2,]$gradePredictAv, na.rm = T), 2)
actual_t = t.test(df[df$gender==1,]$grades_avg, df[df$gender==2,]$grades_avg)
```
While males and females did not significantly differ in terms of reporting their 'goal' grades, males 'predicted' grades (what they thought they would actually get) tended to be higher (M = `r round(predict_t$estimate[1], 1)`, SD = `r sd_m_predict`) than females 
(M = `r round(predict_t$estimate[2], 1)`, SD = `r sd_f_predict`), though not significantly (p = `r round(predict_t$p.value, 2)`). However, when it came to actual grades (as collected from the registrar) things flipped, with males grades now significantly lower than females (t(71.2) = -2.6, p = .01).
